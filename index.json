[{"authors":null,"categories":null,"content":"I like big, policy-relevant social science projects. I’ve spent much of my career leading technical/research teams in industry, while managing to occasionally publish and participate in the public sphere. Full bio below.\n","date":1549324800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1567641600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I like big, policy-relevant social science projects. I’ve spent much of my career leading technical/research teams in industry, while managing to occasionally publish and participate in the public sphere. Full bio below.","tags":null,"title":"Sol Messing","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://solmessing.netlify.app/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Minali Aggarwal","Jennifer Allen","Alexander Coppock","Dan Frankowski","Solomon Messing","Kelly Zhang","James Barnes","Andrew Beasley","Harry Hantman","Sylvan Zheng"],"categories":null,"content":" Supplimentary materials at end of manuscript Replication materials Media coverage: Nature ","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"809a0f9cc24fd9e35181b25a11b9e4b5","permalink":"https://solmessing.netlify.app/publication/aggarwal20232/","publishdate":"2023-01-12T00:00:00Z","relpermalink":"/publication/aggarwal20232/","section":"publication","summary":"We present the results of a massive, $8.9 million campaign-wide field experiment, conducted among 2 million moderate and low-information ``persuadable'' voters in five battleground states during the 2020 US Presidential election. Treatment group subjects were exposed to an eight-month-long advertising program delivered via social media, designed to persuade people to vote against Donald Trump and for Joe Biden. On average, the program neither increased or decreased turnout. We find evidence of differential turnout effects by modeled level of Trump support: the campaign increased voting among Biden leaners by 0.4 percentage points (SE: 0.2pp) and decreased voting among Trump leaners by 0.3 percentage points (SE: 0.3pp), for a difference-in-CATES of 0.7 points t(1035571) = -2.09, p = 0.036, DIC = 0.7 points, 95% CI = [-0.014, -0.00]). An important but exploratory finding is that the strongest differential effects appear in early voting data, which may inform future work on early campaigning in a post-COVID electoral environment. Our results indicate that differential mobilization effects of even large digital advertising campaigns in presidential elections are likely to be modest.","tags":null,"title":"A 2 million-person, campaign-wide field experiment shows how digital advertising affects voter turnout","type":"publication"},{"authors":["Sol Messing"],"categories":[],"content":"It’s becoming clear that the 2020 polls underestimated Trump’s support by anywhere from a 4-8 point margin depending on your accounting–a significantly worse miss than in 2016, when state polls were off but the national polls did relatively well.\nIn fact, this year we were better off using projections based on past vote history in each state to predict how things would go in battleground states, as I’ll show below.\nBut I also want to start to ask questions about what happened this time around. The polling from 2018 looked encouraging, convincing many pollsters that the post-2016 reckoning had fixed many issues called out in the 2016 AAPOR report on election polling. After 2018, FiveThirtyEight wrote that the “Polls are Alright”.\nBut the second Miami-Dade reported results from the 2020 election, we knew something was probably wrong with the 2020 polls.\nHere\u0026#39;s another chart of polls vs. returns that splits the data by how much the polls underestimated Trump. One place where the polls most underestimated Trump is Wisconsin (off by -9 points). Note returns are not yet verified and states are still finalizing their counts. pic.twitter.com/iM8mjqoAuK\n— Stefan Wojcik (@stefanjwojcik) November 9, 2020 As Stefan notes (we worked together at Pew Research Center’s Data Labs), the error seems slightly lower in key battleground states, though the polls missed big in WI, perhaps in part due to its horrifically bad voter file data.\nUnlike 2016, both state and national polls appeared to underestimate Trump’s support, as this early (Nov 7) analysis from Tom Wood shows:\nCurrent as to the afternoon on the 7th, and with Senate results too. pic.twitter.com/EGZGarRPNj\n— Tom Wood (@thomasjwood) November 7, 2020 Polling versus past votes Perhaps what surprised me the most about polling this time around was when I went to evaluate some election projections I put together in April that we used internally at Acronym to help evaluate where we might want to spend. I pulled in the NYTimes polling averages and compared them with the latest state-level presidential results from the AP. I then did the same for the April projections. Turns out the projections were significantly more accurate than the polling averages:\nWe used these projections, and other extant data (including the fact that there are two Senate races in play), when making what turned out to be a very lucky decision to start spending money in Georgia. We were one of the biggest and earliest spenders in that race.\nWhat are these projections? I simply took the last two state-level Presidential and U.S. House election totals, estimated each state’s “trajectory,” and added that to each state’s Democratic margin from the previous cycle.\n(Note that I also weighted 60-40 toward the Presidential results, and slightly regularized both the latest margin and the trajectory toward zero.)\nInforming this approach is work from Yair Ghitza describing what went wrong in 2016, which suggested polarization and other state-level trends would continue, in addition to national trends or “uniform swing.” This paper from @SimonJackman also deserves a big hat tip https://t.co/CTTpYDPwl2\n— Sol Messing (@SolomonMg) November 8, 2020 I should note that this may only have worked because of something peculiar about this election cycle–I haven’t gone an back-tested this approach or anything like that.\nSeems I was not the only one who noticed this kind of pattern:\nA similar observation from @gelliottmorris https://t.co/XSUAhGBZfb\n— Sol Messing (@SolomonMg) November 8, 2020 What went wrong: The Usual Suspects Humble-brag aside, it’s worth asking what might have gone wrong with polling in 2020?\nThe 2016 AAPOR report on election polling provides some guidance for how we might start to examine issues with the 2020 polls.\nUndecided voters: Undecideds broke toward Trump late in the election in 2016–polls found as many as 13 percent of voters were undecided on election day or planned to vote for a third party. According to Poynter, there were half as many of these voters in 2020, so this is unlikely to be as big a factor as in 2016.\nLow education non-response \u0026amp; adjustment: In 2016, individuals lower levels of education were much less likely to answer polls but still voted, and broke for Trump. The national polls adjusted for this but state level polls did not, which is partially why forecasting models that rely on state-level polls missed so hard.\nWhile many state-level pollsters did this in 2020, Pew Research Center still found problems with state level polling this time around, for example failing to adjust for race and education simultaneously–non-college whites are far more likely to support Trump than non-college non-whites.\nWhat’s more, pollsters adjusted only for college/non-college, which may not have been enough. They might need to use more fine grained adjustment–accounting for whether respondents have a high school degree and a college degree. Also error/missing data when people complete education in a …","date":1604793600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604793600,"objectID":"790c784c696b7c214cfb5c0ab22ebed6","permalink":"https://solmessing.netlify.app/post/what-the-polls-got-wrong-in-2020/","publishdate":"2020-11-08T00:00:00Z","relpermalink":"/post/what-the-polls-got-wrong-in-2020/","section":"post","summary":"It’s becoming clear that the 2020 polls underestimated Trump’s support by anywhere from a 4-8 point margin depending on your accounting–a significantly worse miss than in 2016, when state polls were off but the national polls did relatively well.","tags":[],"title":"Past vote data outperformed the polls. How did it go so wrong?","type":"post"},{"authors":["Sol Messing"],"categories":[],"content":"According to the latest polling research, Trump’s chances of hanging on to power beyond 2020 look pretty dismal. Nate Cohn published an impressive battleground poll from New York Times/Sienna showing Biden ahead of Trump by at least six points in pivotal states. The Economist’s forecast, powered by Elliott Morris and Andrew Gelman, is suggesting Biden is likely to get 64% of electoral college votes, and that if the election were held 100 times Biden would win 90 times to Trump’s 10.\nAt this point I would like to remind you of that feeling you felt on election night 2016. When a month earlier, CNN’s ‘Poll of Polls’ had Clinton up by 9 points and two prominent forecasters put Clinton’s chances at 99%. Remember that?\nI could probably stop there, but I’m not going to because although we’ve fixed some of the issues from 2016, we have COVID-19. And COVID will mess with our election in ways very likely to hurt Democrats, and I know of no pollster factoring this into their method or likely voter model.\nAfter 2016, Sean Westwood, Yphtach Lelkes and I began a multi-year research project (recently published in the Journal of Politics) and found that when you have high confidence that one candidate will win, you’re less likely to vote. The fact that everyone thought Clinton would win in 2016 shaped Comey’s decision to release his infamous letter that some believe cost Clinton the election, changed the way campaigns operated, and likely lowered Democratic turnout.\nIn addition to showing this in an experiment, one pattern that clearly pops out in the data we analyzed (ANES timeseries) is that people who think the leading candidate will win by quite a bit report voting at about a 3% lower rate. That’s in line with other research showing that early exit polls indicating one candidate is likely to win decrease turnout, and are more likely to affect Democrats. Yet this is by no means an upper bound—one study found more decisive exit polling depressed turnout by 11 points.\nWhile it’s if anything a noisy indicator of the influence Clinton’s ostensible lead may have had on Democrats compared with Republicans, the proportion of Democrats who thought Clinton would ‘win by quite a bit’ was much higher in 2016 than for Republicans, and much higher than it’d been in many years.\nTo be clear, I no longer occupy the role of dispassionate observer–I’m actively working in politics at the moment.\nSo while I like seeing Biden up, let me explain exactly why the margins we’re seeing could be a polling mirage.\nCOVID-19 Are pollsters accounting for the likely decline in urban turnout due to COVID-19? Not if they are assuming typical levels of turnout across urban and rural areas.\nMake no mistake, COVID-19 is already affecting the political process—look at voter registration. As many colleagues who regularly deal with registration data have warned me, the usual rush of new voter registrations, often from young voters, have “fallen off a cliff.” Registration numbers started stronger than ever as the new year began, but as 538 notes, fell to unprecedented levels in March as pandemic social distancing measures took effect.\nSo it’s already hurting Democrats in terms of new registrations, but what might all this mean on election day? At first blush, it may be tempting to say to yourself, “COVID is affecting old people more than the young, and they break conservative so the left is probably fine,” before feeling slightly ashamed that you’re thinking about strategic considerations before the loss of life and sadness this statement implies.\nThink a little deeper and you’ll likely realize that so far COVID-19 has affected left-leaning people in left-leaning places—non-White voters in urban areas far more than their suburban/rural counterparts. Even the recent surge in cases in sunbelt states is hitting urban and non-White regions hardest.\nWhat’s more, conservatives seem to be far more likely to be willing risk going out and about than liberals. A Pew study shows Republicans are far more likely to support lifting COVID restrictions quickly than Democrats.\nWith a deadly pandemic raging, will urban and non-urban voters go to the polls at the usual rates?\nPost-pandemic primary voting has meant a vast reduction in the number of polling places and a big increase in mail-in-ballots. We’re seeing this in post-pandemic primaries like this Tuesday’s in Kentucky, New York, and Virginia.\nIn New York’s primary, there were reports of missing mail in ballots. Kentucky also saw reports of long lines that disportionately hit Black neighborhoods, in a primary that will determine the Democrat who runs against Senate Majority Leader Mitch McConnell.\nWhat at first looks like maybe a silver lining is the surge in voting by mail-in ballot. And while Trump sees mail-in ballots as a threat to his re-election, the evidence is far from clear that widespread voting by mail would hurt his chances.\nOn the contrary, Stanford’s Andy Hall estimates that universal vote by mail should …","date":1592611200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592611200,"objectID":"7b0afb24f5d96ef5976ea76f9a574f64","permalink":"https://solmessing.netlify.app/post/trumps-chances-are-better-than-they-look/","publishdate":"2020-06-20T00:00:00Z","relpermalink":"/post/trumps-chances-are-better-than-they-look/","section":"post","summary":"According to the latest polling research, Trump’s chances of hanging on to power beyond 2020 look pretty dismal. Nate Cohn published an impressive battleground poll from New York Times/Sienna showing Biden ahead of Trump by at least six points in pivotal states.","tags":[],"title":"Trump's chances are better than they look","type":"post"},{"authors":[],"categories":[],"content":"Inspired by Donald Trump’s surprise victory over Hillary Clinton in the 2016 general election, Sean Westwood, Yphtach Lelkes and I set out to interrogate the question of whether elecion forecasts—particularly probablistic forecasts—might create a sense of inevitability, and ultimately lead people to stay home on election day.\nClinton herself was quoted in New York Magazine after the election:\nI had people literally seeking absolution… ‘I’m so sorry I didn’t vote. I didn’t think you needed me.’ I don’t know how we’ll ever calculate how many people thought it was in the bag, because the percentages kept being thrown at people — ‘Oh, she has an 88 percent chance to win!’\nIs it plausible that forecasting could have affected the election?\nFor this phenomena to affect an election, it must:\nbe visible in the media so it reaches potential voters, depress turnout, and affect one side more than the other. In the case of 2016, that means affecting Clinton’s supporters (and/or Clinton campaigners) more than Trump’s. We found evidence for all of the above. First, witness the rise of forecasts since 2008, when FiveThirtyEight first came on the scene:\nWhat’s more, there is good evidence that one side will be more affected. Our research (see results below) suggests that candidate who is ahead in the polls is more affected by probablistic forecasts. In 2016, that was Hillary.\nAnd irrespective of 2016, it’s outlets with a left-leaning audience that publish and cover election forecasts. The websites that present their poll aggregation results in terms of probabilities have left-leaning (negative) social media audiences—only realclearpolitics.com, which doesn’t emphasize win-probabilities, has a conservative audience:\nThese data come from the average self-reported ideology of people who share links to various sites hosting poll-aggregators on Facebook, data that come from this paper’s replication materials.\nWhen you look at the balance of coverage of probabilistic forecasts on major television broadcasts, there is more coverage on MSNBC, which has a more liberal audience.\nHow much influence do forecasters really have?\nIt’s increadibly difficult to tease out when one media outlet is influencing another. However, a freak event in 2018 allows us to get some traction on this question, and suggests that FiveThirtyEight’s 2018 coverage was highly influential.\nAfter FiveThirtyEight’s real-time forecast suddenely moved the the GOP’s odds of taking the House from single digits to about 60% at around 8:15PM, PredictIt’s odds on the GOP rose above 50-50, \u0026amp; U.S. government bond yields rose 2-4 basis points. FiveThirtyEight then altered it’s prediction system and the markets calmed down.\nThis spike seems to have occurred because a number of big, Republican-dominated districts started reporting returns before those that went toward Democrats and because it was making inferences from partial vote counts:\nThis was first reported by Colby Smith \u0026amp; Brian Greeley of FT.com. They report that because markets expected to see more inflation under a Republican House (high spending, low taxes) the U.S. Bond yield rose.\nWas this just a correlation? Possibly, but there was pretty much nothing else happening in the U.S., and it was like 1 am in Europe, as pointed out in the FT.com piece above.\nJosh Tucker suggested that 538 might be driving prediction markets back in 2012 in a Monkey Cage blogpost.\nOur research on forecasting and perception\nOur research shows that probablistic election forecasts make a race look less competitive. Participants in a national probability survey-experiment were substantially more certain that one candidate would win a hypothetical race after seeing a probablistic forecast than after seeing the equivalent vote share estimate and margin of error. This is a big effect—those are confidence intervals not standard errors, with p-values below $$10^{-11}$$.\nWhy do people do this?\nMore research is needed here but we do have some leads. First, small differences in the election metric most familiar to the public—vote share estimates—generally correspond to very large differences in the probability of a candidate’s chance of victory.\nAndy Gelman referenced this in passing in a 2012 blogpost questioning the decimal precision (0.1 percent) that 538 used to communicate its forecast on its website:\nThat’s right: a change in 0.1 of win probability corresponds to a 0.004 percentage point share of the two-party vote. I can’t see that it can possibly make sense to imagine an election forecast with that level of precision…\nSecond, people sometimes confuse probabilistic forecasts with vote share projections, and incorrectly conclude that a candidate is projected to say win 85% percent of the vote, rather than to having an 85% chance of winning the election. About 1 in 10 peope did this in our experiment.\nAs Joshua Benton pointed out in a tweet, TalkingPointsMemo.com made this very mistake:\nFinally, people tend to think in qualitative terms about …","date":1589860109,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589860109,"objectID":"8476d790ce80588cccb6543c5dbdb394","permalink":"https://solmessing.netlify.app/project/projecting_confidence/","publishdate":"2020-05-18T22:48:29-05:00","relpermalink":"/project/projecting_confidence/","section":"project","summary":"How the Probabilistic Horse Race Demobilizes the Public","tags":[],"title":"Projecting Confidence","type":"project"},{"authors":[],"categories":[],"content":"On January 17, 2020 my team at Facebook launched one of the largest social science data sets ever constructed. It’s meant to facilitate research on misinformation from across the web, shared and spread on Facebook.\nFull details on the release here.\nWe also released the URL santization framework, which I implemented (and which my SWE colleagues refactored).\nWhat makes this data release unprecedented is that it contains exposure data describing external links that billions of users saw and read while using the site.\nThe data set goes beyond URL-level data, breaking down exposure and interactions by month, country, age, gender, and in the U.S., political page affinity (see Barbera et al 2015).\nThe data contain two tables: (1) a “URL attributes” table describing the 38 million URLs in the data set, including how many times users tagged those posts as containing misinformation, harassment, etc. and (2) a “breakdown” table, which aggregates counts of actions taken on urls, broken out by user demographics and URL attributes.\nThe technical documentation reflects more work than most papers I’ve written: . This list of authors reflects the scale of this massive team effort, and that’s before you include increadibly helpful advice we got from a number of computer scientists in the academy listed in the acknowledgements.\nPerhaps most importantly, this release provides guarantees about anonymity in an incredibly rigorous way–action-level differential privacy, while preserving more underlying signal in the data.\n","date":158976e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":158976e4,"objectID":"b14799ea11298f03030a016bc10937b1","permalink":"https://solmessing.netlify.app/project/condor_data_release/","publishdate":"2020-05-18T00:00:00Z","relpermalink":"/project/condor_data_release/","section":"project","summary":"Largest ever social science data set, released under differential privacy","tags":[],"title":"Facebook Condor URLs Data Release","type":"project"},{"authors":[],"categories":[],"content":"The Impression of Influence: Legislator Communication, Representation, and Democratic Accountability Princeton University Press, 2015. With Justin Grimmer and Sean Westwood\nMedia: Mischiefs of Faction. ","date":1589673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589673600,"objectID":"f0402bccd43759784e08f521443e4da3","permalink":"https://solmessing.netlify.app/project/impression_of_influence/","publishdate":"2020-05-17T00:00:00Z","relpermalink":"/project/impression_of_influence/","section":"project","summary":"A book on how Members of Congress use Credit Claiming   ","tags":[],"title":"Impression of Influence","type":"project"},{"authors":["Sol Messing"],"categories":[],"content":"Do you remember the night of Nov 8, 2016? I was glued to election coverage and obsessively checking probabilistic forecasts, wondering whether Clinton might do so well that she’d win in places like my home state of Arizona. Although FiveThirtyEight had Clinton’s chances at beating Trump at around 70%, most other forecasters had her at around 90%.\nWhen she lost, many on both sides of the aisle were shocked. My co-authors and I wondered if America’s seeming confidence in a Clinton victory wasn’t driven in part by increasing coverage of probabilistic forecasts. And, if a Clinton victory looked inevitable, what did that do to turnout?\nWe weren’t alone. Clinton herself was quoted in New York Magazine after the election:\nI had people literally seeking absolution… ‘I’m so sorry I didn’t vote. I didn’t think you needed me.’ I don’t know how we’ll ever calculate how many people thought it was in the bag, because the percentages kept being thrown at people — ‘Oh, she has an 88 percent chance to win!’\nEnter our recent blog post and paper released on SSRN, “Projecting confidence: How the probabilistic horse race confuses and de-mobilizes the public,” by Sean Westwood, Solomon Messing, and Yphtach Lelkes. While our work cannot definitively say whether probabilistic forecasts played a decisive role in the 2016 election, it does indeed show that compared to more conventional vote share projections, probabilistic forecasts can confuse people, can give people more confidence that the candidate depicted as being ahead will win, may decrease turnout, and that liberals in the U.S. are more likely to encounter them. We appreciate the media attention to this work, including coverage by the Washington Post, New York Magazine, and the Political Wire. What’s more, FiveThirtyEight devoted much of their Feb. 12 Politics Podcast to a spirited, and at points critical discussion of our work. We are open to criticism and will respond to some of the questions raised in this post. Below, we’ll show that the evidence in our study and in other research is not inconsistent with our headline, as the hosts suggest—we’ll detail the evidence that probabilistic forecasts confuse people, irrespective of their technical accuracy. We’ll also discuss where we agree with the podcast hosts. Furthermore, we’ll discuss a few topics which, judging from the hosts discussion, may not have come through clearly enough in our paper. We’ll reiterate what this work contributes to social science—how the paper adds to our understanding of how people think about probabilistic forecasts and how they may decrease voting, particularly for the leading candidate’s supporters and among liberals in the U.S. We’ll then walk readers through the way we mapped vote share projections to probabilities in the study. Finally we’ll discuss why this work matters, and conclude by pointing out future research we’d like to see in this area.\nWhat’s new here?\nThe research contains a number findings that are new to social science:\nPresenting forecasted win-probabilities gives potential voters the impression that one candidate will win more decisively, compared with vote share projections (Study 1). Higher win probabilities, but not vote share estimates, decrease voting in the face of the trade-offs embedded in our election simulation (Study 2). This helps confirm the findings in Study 1 and adds to the evidence from past research that people vote at lower rates when they perceive an election to be uncompetitive. In 2016, probabilistic forecasts were covered more extensively than in the past and tended to be covered by outlets with more liberal audiences. Where we agree\nIf what you care about is conveying an accurate sense of whether one candidate will win, probabilistic forecasts do this slightly better than vote share. And, they seem to give people an edge on accuracy when interpreting the vote share if your candidate is behind. Of course, people can be confused and still end up being accurate, as we’ll discuss below.\nWe also agree that people often do not accurately judge the likelihood of victory after seeing a vote share projection. That makes sense because, as the study shows, people appear to largely ignore the margin of error, which they’d need to map between vote share estimates and win probabilities.\nWe also agree that a lot of past work shows that people stay home when they think an election isn’t close. What we’re adding to that body of work is evidence that compared with vote share projections, probabilistic forecasts give people the impression that one candidate will win more decisively, and may thus more powerfully affect turnout.\nDoes the evidence in our study contradict our headline?\nOur headline isn’t about accuracy, it’s about confusion. And the evidence from this research and past work taken as a whole suggests that probabilistic forecasts confuse people — something that came up at the end of segment — even if the result sometimes is technically higher accuracy.\n1. …","date":1587859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587859200,"objectID":"f4366daf83642214038e5de247bbc2a2","permalink":"https://solmessing.netlify.app/post/response-to-fivethirtyeights-podcast-about-our-paper-projecting-confidence/","publishdate":"2020-04-26T00:00:00Z","relpermalink":"/post/response-to-fivethirtyeights-podcast-about-our-paper-projecting-confidence/","section":"post","summary":"Do you remember the night of Nov 8, 2016? I was glued to election coverage and obsessively checking probabilistic forecasts, wondering whether Clinton might do so well that she’d win in places like my home state of Arizona.","tags":[],"title":"Why Election Forecasting Matters","type":"post"},{"authors":["Sol Messing"],"categories":[],"content":" My last post railed against the bad visualizations that people often use to plot quantitive data by groups, and pitted pie charts, bar charts and dot plots against each other for two visualization tasks. Dot plots came out on top. I argued that this is because humans are good at the cognitive task of comparing position along a common scale, compared to making judgements about length, area, shading, direction, angle, volume, curvature, etc.—a finding credited to Cleveland and McGill. I enjoyed writing it and people seemed to like it, so I’m continuing my visualization series with the scatterplot.\nScatterplots A scatterplot is a two-dimensional plane on which we record the intersection of two measurements for a set of case items–usually two quantitative variables. Just as humans are good at comparing position along a common scale in one dimension, our visual capabilities allow us to make fast, accurate judgements and recognize patterns when presented with a series of dots in two dimensions. This makes the scatterplot a valuable tool for data analysts both when exploring data and when communicating results to others.\nIn this post—part 1—I’ll demonstrate various uses for scatterplots and outline some strategies to help make sure key patterns are not obscured by the scale or qualitative group-level differences in the data (e.g., the relationship between test scores and income differs for men and women). The motivation in this post is to come up with a model of diamond prices that you can use to help make sure you don’t get ripped off, specified based on insight from exploratory scatterplots combined with (somewhat) informed speculation. In part 2, I’ll discuss the use of panels aka facets aka small multiples to shed additional light on key patterns in the data, and local regression (loess) to examine central tendencies in the data. There are far fewer bad examples of this kind of visualization in the wild than the 3D barplots and pie charts mocked in my last post, though I was still able to find this lovely scatterplot + trend-line.\nScatterplots and the Cartesian coordinate system The scatterplot has a richer history than the visualizations I wrote about in my last post. The scatterplot’s face forms a two-dimensional Cartesian coordinate system, and DeCartes’ invention/discovery of this eponymous plane in around 1657 represents one of the most fundamental developments in science. The Cartesian plane unites measurement, algebra, and geometry, depicting the relationship between variables (or functions) visually. Prior to the Cartesian plane, mathematics was divided into algebra and geometry, and the unification of the two made many new developments possible. Of course, this includes modern map-making—cartography, but the Cartesian plane was also an important step in the development of calculus, without which very little of our modern would would be possible.\nThe scatterplot is a powerful tool to help understand the relationship between variables, and especially if that relationship is non-linear. Say you want to get a sense of whether you’re paying the right price when shopping for a diamond. You can use data on the price and characteristics of many diamonds to help figure out whether the price advertised for any given diamond is reasonable, and you can use scatterplots to help figure out how to model that data in a sensible way. Consider the important relationship between the price of a diamond and its carat weight (which corresponds to its size):\nA few things pop out right away. We can see a non-linear relationship, and we can also see that the dispersion (variance) of the relationship also increases as carat size increases. With just a quick look at a scatterplot of the data, we’ve learned two important things about the functional relationship between price and carat size. And, we also therefore learned that running a linear model on this data as-is would be a bad idea.\nDiamonds If you’ve ever used R, you’ve probably seen references to the diamonds data set that ships with Hadley Wickham’s ggplot2. It records the carat size and the price of more than 50 thousand diamonds, from http://www.diamondse.info/ collected in in 2008, and if you’re in the market for a diamond, exploring this data set can help you understand what’s in store and at what price point. This is particularly useful because each diamond is unique in a way that isn’t true of most manufactured products we are used to buying—you can’t just plug a model number and look up the price on Amazon. And even an expert cannot cannot incorporate as much information about price as a picture of the entire market informed by data (though there’s no substitute for qualitative expertise to make sure your diamond is what the retailer claims).\nBut even if you’re not looking to buy a diamond, the socioeconomic and political history of the diamond industry is fascinating. Diamonds birthed the mining industry in South Africa, which is now by far the largest and most …","date":1580601600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580601600,"objectID":"280f6c7dfb3af4294f0a954eabc201ba","permalink":"https://solmessing.netlify.app/post/visualization-series-scatterplot-understanding-the-diamond-market/","publishdate":"2020-02-02T00:00:00Z","relpermalink":"/post/visualization-series-scatterplot-understanding-the-diamond-market/","section":"post","summary":"Second post in my data visualization series on scatterplots and regression analysis","tags":[],"title":"Know your data - Pricing diamonds using scatterplots and predictive models","type":"post"},{"authors":["Solomon Messing","Christina DeGregorio","Bennett Hillenbrand","Gary King","Saurav Mahanti","Chaya Nayak","Nate Persily"],"categories":null,"content":" Largest ever social science data set released, protected using action-level differential privacy. See also Social Science One’s annoucement. Media coverage: Science, Nature, Poynter, Wired, Financial Times, Tech Crunch, The Verge. ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"aa0019151390f46b5fa17fb066ba25f4","permalink":"https://solmessing.netlify.app/publication/messing-2020/","publishdate":"2020-06-02T02:04:47.467918Z","relpermalink":"/publication/messing-2020/","section":"publication","summary":"One of the largest social science data sets ever constructed, meant to facilitate research on misinformation from across the web, shared and spread on Facebook. It contains exposure data describing external links that billions of users saw and read while using the site. The data set goes beyond URL-level data, breaking down exposure and interactions by month, country, age, gender, and in the U.S., political page affinity (see Barbera et al 2015).   ","tags":null,"title":"Facebook Privacy-Protected Full URLs Data Set","type":"publication"},{"authors":["Daniel Kifer","Solomon Messing","Aaron Roth","Abhradeep Thakurta","Danfeng Zhang"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"8c7294b70c5b70c0f008671c7bfdca5a","permalink":"https://solmessing.netlify.app/publication/kifer-2020-guidelines/","publishdate":"2020-06-02T02:04:47.467086Z","relpermalink":"/publication/kifer-2020-guidelines/","section":"publication","summary":"Differential privacy is an information theoretic constraint on algorithms and code. It provides quantification of privacy leakage and formal privacy guarantees that are currently considered the gold standard in privacy protections. In this paper we provide an initial set of \"best practices\" for developing differentially private platforms, techniques for unit testing that are specific to differential privacy, guidelines for checking if differential privacy is being applied correctly in an application, and recommendations for parameter settings. The genesis of this paper was an initiative by Facebook and Social Science One to provide social science researchers with programmatic access to a URL-shares dataset. In order to maximize the utility of the data for research while protecting privacy, researchers should access the data through an interactive platform that supports differential privacy. The intention of this paper is to provide guidelines and recommendations that can generally be re-used in a wide variety of systems. For this reason, no specific platforms will be named, except for systems whose details and theory appear in academic papers.","tags":null,"title":"Guidelines for Implementing and Auditing Differentially Private Systems","type":"publication"},{"authors":["Sol Messing"],"categories":[],"content":"from IPython.core.display import Image Image(\u0026#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png\u0026#39;) print(\u0026#34;Welcome to Academic!\u0026#34;) Welcome to Academic! Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata (front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post\u0026#39;s title date: 2019-09-01 # Put any other Academic metadata here... --- Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post’s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=. Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://solmessing.netlify.app/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://solmessing.netlify.app/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Sean Westwood","Solomon Messing","Yphtach Lelkes"],"categories":null,"content":" Supplimentary materials Cited by FiveThirthyEight’s Politics Podcast as influential in decision to change forecast presentation. Media coverage: Washington Post, New York Magazine, Political Wire. ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"2ee4f07a9fc739fed229a2d87e54bfec","permalink":"https://solmessing.netlify.app/publication/wlm-2019-projecting/","publishdate":"2020-06-02T02:04:47.465593Z","relpermalink":"/publication/wlm-2019-projecting/","section":"publication","summary":"Recent years have seen a dramatic change in horserace coverage of elections in the U.S.---shifting focus from late-breaking poll numbers to sophisticated meta-analytic forecasts that emphasize candidates' chance of victory. Could this shift in the political information environment affect election outcomes? We use experiments to show that forecasting increases certainty about an election's outcome, confuses many, and decreases turnout. Furthermore, we show that election forecasting has become prominent in the media, particularly in outlets with liberal audiences, and show that such coverage tends to more strongly affect the candidate who is ahead---raising questions about whether they contributed to Trump's victory over Clinton in 2016. We bring empirical evidence to this question, using ANES data to show that Democrats and Independents expressed unusual confidence in a decisive 2016 election outcome---and that the same measure of confidence is associated with lower reported turnout.","tags":null,"title":"Projecting confidence: How the probabilistic horse race confuses and demobilizes the public","type":"publication"},{"authors":["Fanny Chapelin","Aman Khurana","Mohammad Moneeb","Florette K Gray Hazard","Chun Fai Ray Chan","Hossein Nejadnik","Dita Gratzinger","Solomon Messing","Jason Erdmann","Amitabh Gaur"," others"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"7ffc5c7b4880707290b23b30678b540d","permalink":"https://solmessing.netlify.app/publication/chapelin-2019-tumor/","publishdate":"2020-06-02T02:04:47.466292Z","relpermalink":"/publication/chapelin-2019-tumor/","section":"publication","summary":"Purpose: While imaging matrix-associated stem cell transplants aimed for cartilage repair in a rodent arthritis model, we noticed that some transplants formed locally destructive tumors. The purpose of this study was to determine the cause for this tumor formation in order to avoid this complication for future transplants.  Procedures: Adipose-derived stem cells (ADSC) isolated from subcutaneous adipose tissue were implanted into 24 osteochondral defects of the distal femur in ten athymic rats and two immunocompetent control rats. All transplants underwent serial magnetic resonance imaging (MRI) up to 6 weeks post-transplantation to monitor joint defect repair. Nine transplants showed an increasing size over time that caused local bone destruction (group 1), while 11 transplants in athymic rats (group 2) and 4 transplants in immunocompetent rats did not. We compared the ADSC implant size and growth rate on MR images, macroscopic features, histopathologic features, surface markers, and karyotypes of these presumed neoplastic transplants with non-neoplastic ADSC transplants.  Results: Implants in group 1 showed a significantly increased two-dimensional area at week 2 (p = 0.0092), 4 (p = 0.003), and 6 (p = 0.0205) compared to week 0, as determined by MRI. Histopathological correlations confirmed neoplastic features in group 1 with significantly increased size, cellularity, mitoses, and cytological atypia compared to group 2. Six transplants in group 1 were identified as malignant chondrosarcomas and three transplants as fibromyxoid sarcomas. Transplants in group 2 and immunocompetent controls exhibited normal cartilage features. Both groups showed a normal ADSC phenotype; however, neoplastic ADSC demonstrated a mixed population of diploid and tetraploid cells without genetic imbalance.  Conclusions: ADSC transplants can form tumors in vivo. Preventive actions to avoid in vivo tumor formations may include karyotyping of culture-expanded ADSC before transplantation. In addition, serial imaging of ADSC transplants in vivo may enable early detection of abnormally proliferating cell transplants. ","tags":null,"title":"Tumor formation of adult stem cell transplants in rodent arthritic joints","type":"publication"},{"authors":["Sol Messing"],"categories":[],"content":"Regression models are a cornerstone of modern social science. They’re at the heart of efforts to estimate causal relationships between variables in a multivariate environment and are the basic building blocks of many machine learning models. Yet social scientists can run into a lot of situations where regression models break.\nFamed social psychologist Richard Nisbett recently argued that regression analysis is so misused and misunderstood that analyses based on multiple regression “are often somewhere between meaningless and quite damaging.” (He was mainly talking about cases in which researchers publish correlational results that are covered in the media as causal statements about the world.)\nBelow, I’ll walk through some of the potential pitfalls you might encounter when you fire up your favorite statistical software package and run regressions. Specifically, I’ll be using simulation in R as an educational tool to help you better understand the ways in which regressions can break.\nUsing simulations to unpack regression\nThe idea of using R simulations to help understand regression models was inspired by Ben Ogorek’s post on regression confounders and collider bias.\nThe great thing about using simulation in this way is that you control the world that generates your data. The code I’ll introduce below represents the true data-generating process,since I’m using R’s random number generators to simulate the data. In real life, of course, we only have the data we observe, and we don’t really know how the data-generating process works unless we have a solid theory (like Newtonian physics or evolution) where the system of relevant variables and causal relationships is well understood and to which there is really no analogous phenomenon in social science.\nWhat I’ll do here is create a dataset based on two random standard normal variables by simulating them using the rnorm() function, which draws random values from a normal distribution with mean 0 and standard deviation 1, unless you specify otherwise. I’ll create a functional relationship between y and x such that a 1 unit increase in x will be associated with a .4 unit increase in y.\n# make the code reproducible by setting a random number seed set.seed(100) # When everything works: N \u0026lt;- 1000 x \u0026lt;- rnorm(N) y \u0026lt;- .4 * x + rnorm(N) hist(x) hist(y) # Now estimate our model: summary(lm(y ~ x)) Call: lm(formula = y ~ x) Residuals: Min 1Q Median 3Q Max -3.0348 -0.7013 0.0085 0.6212 3.1688 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 0.003921 0.031039 0.126 0.899 x 0.413415 0.030129 13.722 \u0026lt;2e-16 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.9814 on 998 degrees of freedom Multiple R-squared: 0.1587,\tAdjusted R-squared: 0.1579 F-statistic: 188.3 on 1 and 998 DF, p-value: \u0026lt; 2.2e-16 # Plot it library(ggplot2) qplot(x, y) + geom_smooth(method=\u0026#39;lm\u0026#39;) + theme_bw() + ggtitle(\u0026#34;The Perfect Regression\u0026#34;) Notice that the model estimates the functional relationship between x and y that I simulated quite well. The plot looks like this:\nWhat about omitted variables? Our machinery actually still works if there is another factor causing y, as long as it is uncorrelated with x.\nThe dreaded omitted variable bias\nOmitted variable bias (OVB) is much feared, and judging by the top internet search results, not well understood. Some top sources say it occurs when “an important” variable is missing or when a variable that “is correlated” with both x and y is missing. I even found a university econometrics course that defined OVB this way.\nBut neither of those definitions are quite right. OVB occurs when a variable that causes y is missing from the model (and is correlated with x). Let’s call that variable w. Because w is in play when we consider the causal relationship between x and y, it’s often referred to as “endogenous” or a “confounding variable.”\nThe example below first demonstrates that w, our confounding variable, will bias our results if we fail to include it in our model. The next two examples are essentially a re-telling of the post I mentioned above on collider bias, but emphasizing slightly different points.\nw \u0026lt;- rnorm(N) x \u0026lt;- .5 * w + rnorm(N) y \u0026lt;- .4 * x + .3 * w + rnorm(N) m1 \u0026lt;- lm(y ~ x) summary (m1) # Omitted variable bias Call: lm(formula = y ~ x) Residuals: Min 1Q Median 3Q Max -3.2190 -0.7025 0.0314 0.7120 3.1158 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 0.01126 0.03310 0.34 0.734 x 0.50179 0.03049 16.46 \u0026lt;2e-16 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 1.046 on 998 degrees of freedom Multiple R-squared: 0.2135,\tAdjusted R-squared: 0.2127 F-statistic: 270.9 on 1 and 998 DF, p-value: \u0026lt; 2.2e-16 There it is: classic omitted variable bias. We only observed x, and the influence of the omitted variable w was attributed to x in our model. If you re-rerun the regression with w in the model, you no longer get biased estimates.\nm2 \u0026lt;- lm(y ~ x + w) …","date":1528848e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"a167f7f26122792b24d60fbe134ef5ee","permalink":"https://solmessing.netlify.app/post/how-to-break-regression/","publishdate":"2018-06-13T00:00:00Z","relpermalink":"/post/how-to-break-regression/","section":"post","summary":"Regression models are a cornerstone of modern social science. They’re at the heart of efforts to estimate causal relationships between variables in a multivariate environment and are the basic building blocks of many machine learning models.","tags":[],"title":"How to break regression","type":"post"},{"authors":["Sol Messing"],"categories":[],"content":"This post presents a replication of Messing et al. (2016, study 2), which showed that exposure to darker images of Barack Obama increased stereotype activation, as indicated by the tendency to finish incomplete word prompts---such as “W E L _ _ _ _”---in stereotype-consistent ways (“WELFARE”).\nOverall, the replication shows that darker images of even counter-stereotypical exemplars like Barack Obama can increase stereotype activation, but that the strength of the effect is weaker than conveyed in the original study. A reanalysis of the original study conducted in the course of this replication effort unearthed a number of problems that, when corrected, yield estimates of the effect that are consistent with those documented in the replication. This reanalysis also follows.\nI\u0026#39;m posting this to\ndisseminate a corrected version of the original study; show how I found those problems with the original study in the course of conducting this replication; circulate these generally confirmatory findings, along with a pooled analysis revealing a stronger effect among conservatives; and provide a demonstration of how replication almost always enhances our knowledge about the original research, which I hope may encourage others to invest the time and money in such efforts. First some context.\nThe original study that formed the basis of the manuscript shows that more negative campaign ads in 2008 were also more likely to contain darker images of President Obama. In 2009 when I started this work, I was most proud of the method to collect data on skin complexion outlined in study 1. I included another study, what\u0026#39;s now study 3, which shows that 2012 ANES survey-takers were more likely to respond negatively to Chinese characters after being presented with darker images of Obama (this is called the Affect Misattribution Procedure (AMP)). But the AMP was not a true experiment and a reviewer was concerned that Study 3 did not provide sufficiently rigorous, causal evidence that darker images alone can cause negative affect. So I conducted an experiment that would establish a causal link between darker images of Obama and something I thought was even more important---stereotype activation. There were strong reasons to expect this effect based on past lab studies showing links between darker skin and negative stereotypes about Blacks, and past observational studies showing far more negative socioeconomic outcomes across the board among darker versus lighter skinned Black Americans. We found an effect and published the three studies.\nThis replication effort was prompted by a post-publication reanalysis and critique, which raised questions about potential weaknesses in the original analysis. My aim in replicating the study was to bring new data to the discussion and make sure we hadn’t polluted the literature with a false discovery.\nThe main objection was the way we formed our stereotype consistency index. The items assessing stereotype consistency comprised 11 words with missing blank spaces (e.g., L A _ _). Each fragment had as one possible solution a stereotype-related completion. The complete list follows: L A _ _ (LAZY): C R _ _ _ (CRIME); _ _ O R (POOR); R _ _ (RAP); WEL _ _ _ _ (WELFARE); _ _ C E (RACE); D _ _ _ Y (DIRTY); B R _ _ _ _ _ (BROTHER); _ _ A C K (BLACK); M I _ _ _ _ _ _ (MINORITY); D R _ _ (DRUG).\nThe author pointed out that there were many potential ways to analyze the original data---he claimed over 16 thousand. Yet very few of these are consistent with generally accepted research practices. We\u0026#39;ve known, arguably since the 16th century, that combining several measures reduces measurement error and hence variance in estimation. This is particularly important in social science, and especially for this particular study---it would be unwise to attempt to use a single word completion or an arbitrary subset thereof to measure a complex, noisy construct like stereotype activation as measured via a word completion game. Rather, taking the average or constructing an index based on clustering several measures should be expected to result in far less measurement error, which is what we did.\nStill, I am sympathetic to concerns about the garden of forking paths, which is part of the motivation for this replication.\nIn the original study, I formed this index based on what I judged to be the most unambiguously negative word-completions (lazy, dirty, poor), consistent with past work suggesting that darker complexion activates the most negative stereotypes about Blacks. I calculated that these were the three variables that also maximized interclass correlation (ICC). As a robustness check, I also computed a measure that maximized alpha reliability (AR). This measure contained more items, and also seemed to include stereotype-consistent word completions that were on balance negative---lazy, dirty, poor, crime, black, and welfare. I should have but did not report results based on a simple average of these items, which was not …","date":1508112e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508112e3,"objectID":"3be66141785515b63c646ac8b7c8fa93","permalink":"https://solmessing.netlify.app/post/replication_of_bias_in_the_flesh/","publishdate":"2017-10-16T00:00:00Z","relpermalink":"/post/replication_of_bias_in_the_flesh/","section":"post","summary":"Replication of Study 2 in \"Bias in the Flesh: Skin Complexion and Stereotype Consistency in Political Campaigns\"","tags":[],"title":"Replication of 'Bias in the Flesh'","type":"post"},{"authors":["Justin Grimmer","Solomon Messing","Sean J Westwood"],"categories":null,"content":"- [Replication materials](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/BQMLQW). - [Software implementation](https://github.com/SolomonMg/HetSL) (under development) ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"73a96689e3afd7f02aca578fdc14fedb","permalink":"https://solmessing.netlify.app/publication/grimmer-2017-estimating/","publishdate":"2020-06-02T02:04:47.460136Z","relpermalink":"/publication/grimmer-2017-estimating/","section":"publication","summary":"Randomized experiments are increasingly used to study political phenomena because they can credibly estimate the average effect of a treatment on a population of interest. But political scientists are often interested in how effects vary across subpopulations---heterogeneous treatment effects---and how differences in the content of the treatment affects responses---the response to heterogeneous treatments. Several new methods have been introduced to estimate heterogeneous effects, but it is difficult to know if a method will perform well for a particular data set. Rather than using only one method, we show how an ensemble of methods---weighted averages of estimates from individual models increasingly used in machine learning---accurately measure heterogeneous effects. Building on a large literature on ensemble methods, we show how the weighting of methods can contribute to accurate estimation of heterogeneous treatment effects and demonstrate how pooling models lead to superior performance to individual methods across diverse problems. We apply the ensemble method to two experiments, illuminating how the ensemble method for heterogeneous treatment effects facilitates exploratory analysis of treatment effects. ","tags":null,"title":"Estimating heterogeneous treatment effects and the effects of heterogeneous treatments with ensemble methods","type":"publication"},{"authors":["Patrick van Kessel, Adam G Hughes, Nick Judd, Rachel Blum, Brian Broderick Solomon Messing"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"d72021c34d2d19b7f54c5d3c3d35f01e","permalink":"https://solmessing.netlify.app/publication/solomon-2017-partisan/","publishdate":"2020-06-02T02:04:47.46401Z","relpermalink":"/publication/solomon-2017-partisan/","section":"publication","summary":"Partisan criticism generates most engagement in social media; most liberal and conservative legislators, party leadership more likely to `go negative'","tags":null,"title":"Partisan Conflict and Congressional Outreach","type":"publication"},{"authors":["Solomon Messing","Maria Jabon","Ethan Plaut"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"60026b3d5e7fe991ef729e9cb082798e","permalink":"https://solmessing.netlify.app/publication/messing-2016-bias/","publishdate":"2020-06-02T02:04:47.448293Z","relpermalink":"/publication/messing-2016-bias/","section":"publication","summary":"There is strong evidence linking skin complexion to negative stereotypes and adverse real-world outcomes. We extend these findings to political ad campaigns, in which skin complexion can be easily manipulated in ways that are difficult to detect. Devising a method to measure how dark a candidate appears in an image, this paper examines how complexion varied with ad content during the 2008 presidential election campaign (study 1). Findings show that darker images were more frequent in negative ads---especially those linking Obama to crime---which aired more frequently as Election Day approached. We then conduct an experiment to document how these darker images can activate stereotypes, and show that a subtle darkness manipulation is sufficient to activate the most negative stereotypes about Blacks---even when the candidate is a famous counter-stereotypical exemplar---Barack Obama (study 2). Further evidence of an evaluative penalty for darker skin comes from an observational study measuring affective responses to depictions of Obama with varying skin complexion, presented via the Affect Misattribution Procedure in the 2008 American National Election Study (study 3). This study demonstrates that darker images are used in a way that complements ad content, and shows that doing so can negatively affect how individuals evaluate candidates and think about politics.","tags":null,"title":"Bias in the flesh: Skin complexion and stereotype consistency in political campaigns","type":"publication"},{"authors":["Eytan Bakshy","Sol Messing"],"categories":[],"content":"Earlier this month, we published an early access version of our paper in ScienceExpress (Bakshy et al. 2015), “Exposure to ideologically diverse news and opinion on Facebook.” The paper constitutes the first attempt to quantify the extent to which ideologically cross-cutting hard news and opinion is shared by friends, appears in algorithmically ranked News Feeds, and is actually consumed (i.e., click through to read).\nWe are grateful for the widespread interest this paper, which grew out of two threads of related research that we began nearly five years ago: Eytan and Lada\u0026#39;s work on the role of social networks in information diffusion (Bakshy et al. 2012) and Sean and Solomon\u0026#39;s work on selective exposure in social media (Messing and Westwood 2012).\nWhile Science papers are explicitly prohibited from suggesting future directions for research, we would like to shed additional light on our study and raise a few questions that we would be excited to see addressed in future work.\nTradeoffs when Selecting a Population\nThere were tradeoffs when deciding on who to include in this study. While we could have examined all U.S. adults on Facebook, we focused on people who identify as liberals or conservatives and encounter hard news, opinion, and other political content in social media regularly. We did so because many important questions around “echo chambers” and “filter bubbles\u0026#34;on Facebook relate to this subpopulation, and we used self-reported ideological preferences to define it.\nUsing self-reported ideological preferences in online profiles is not the only a way to measure ideology or define the population of interest. Yet, people who publicly identify as liberals or conservatives in their Facebook profiles are an interesting and important subpopulation worthy of study for many reasons. As Hopkins and King 2010 have pointed out, studying the expression and behavior of those who are politically engaged online is of interest to political scientists studying activists (Verba, Schlozman, and Brady 1995), the media (Drezner and Farrell 2004), public opinion (Gamson 1992), social networks (Adamic and Glance 2005; Huckfeldt and Sprague 1995), and elite influence (Grindle 2005; Hindman, Tsioutsiouliklis, and Johnson 2003; Zaller 1992).\nThis subpopulation has limitations and is not the only population of interest. The data are not appropriate for those who seek estimates of the entire U.S. public, people without strong opinions, or people not on Facebook (at least not without additional extrapolation, re-weighting, additional evidence, etc.). While our data could plausibly also provide good estimates of the population of people who are ideologically active and have clear preferences, we are not claiming that\u0026#39;s necessarily the case---that remains to be determined in future work.\nWe\u0026#39;d like to help other researchers looking to study other populations understand more about the population we\u0026#39;ve defined. An important question in this regard is what proportion of active U.S. adults actually report an identifiable left/right/center ideology in their profile. That number is 25%, or 10.1 million people.\nIt\u0026#39;s also informative to examine the proportion of those users who provide identifiable profile affiliations conditional on demographics and Facebook usage:\nAge Percent reporting ideological affiliation 18-24 21.60% 25-44 28.50% 45-64 24.30% 65+ 21.40% Gender Percent reporting ideological affiliation Female 21.90% Male 30.60% Login Days Percent reporting ideological affiliation 105-140 18.90% 140-185 26.70% Clearly those who report an ideology in their profile tend to be more active on Facebook. They are also more likely to be men, which is consistent with the well-documented gender gap in American politics (Box-Steffensmeier 2004).\nIt\u0026#39;s possible that these individuals differ from other Facebook users in other ways. It seems plausible to expect these people to have higher levels of political interest, a stronger sense of political ideology and political identity, and to be more likely to be active in politics than most others on Facebook. It\u0026#39;s also possible that these individuals are more extroverted than the average user, especially in the somewhat taboo domain of politics. These possibilities also strike us as interesting questions for study in future work.\nHow to Measure Ideology\nWe hope others will replicate this work using other populations and ways of measuring ideology, which will provide a broader view of exposure to political media. Data on ideology could be collected by, for example, surveying users, imputing ideology based on user behavior, or joining data to the voter file. Each of these methods have advantages and potential challenges.\nUsing surveys in future work would allow researchers to collect data on ideology in a way that can facilitate comparisons with much of the extant literature in political science, and allow researchers to sample from a less politically engaged population. Of course, this could be tricky …","date":1429833600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1429833600,"objectID":"0f268982a57d085d401a76516f7bd169","permalink":"https://solmessing.netlify.app/post/exposure-to-ideologically-diverse-response/","publishdate":"2015-04-24T00:00:00Z","relpermalink":"/post/exposure-to-ideologically-diverse-response/","section":"post","summary":"Earlier this month, we published an early access version of our paper in ScienceExpress (Bakshy et al. 2015), “Exposure to ideologically diverse news and opinion on Facebook.” The paper constitutes the first attempt to quantify the extent to which ideologically cross-cutting hard news and opinion is shared by friends, appears in algorithmically ranked News Feeds, and is actually consumed (i.","tags":[],"title":"Ideologically diverse news, an agenda for future research","type":"post"},{"authors":["Eytan Bakshy","Solomon Messing","Lada A Adamic"],"categories":null,"content":" Review by David Lazer Supplementary materials Replication materials Media: New York Times, the Washington Post, the BBC, CBS, Huffington Post, Ars Technica, Wired UK, The Verge. ","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"dfefb9c37ee61844bf84fb56d900fc37","permalink":"https://solmessing.netlify.app/publication/bakshy-2015-exposure/","publishdate":"2020-06-02T02:04:47.463109Z","relpermalink":"/publication/bakshy-2015-exposure/","section":"publication","summary":"Exposure to news, opinion, and civic information increasingly occurs through social media. How do these online networks influence exposure to perspectives that cut across ideological lines? Using deidentified data, we examined how 10.1 million U.S. Facebook users interact with socially shared news. We directly measured ideological homophily in friend networks and examined the extent to which heterogeneous friends could potentially expose individuals to cross-cutting content. We then quantified the extent to which individuals encounter comparatively more or less diverse content while interacting via Facebook's algorithmically ranked News Feed and further studied users' choices to click through to ideologically discordant content. Compared with algorithmic ranking, individuals' choices played a stronger role in limiting exposure to cross-cutting content.","tags":null,"title":"Exposure to ideologically diverse news and opinion on Facebook","type":"publication"},{"authors":["Robert Bond","Solomon Messing"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"51912c6b9fbd8ba45a049856aa55aa80","permalink":"https://solmessing.netlify.app/publication/bond-2015-quantifying/","publishdate":"2020-06-02T02:04:47.462374Z","relpermalink":"/publication/bond-2015-quantifying/","section":"publication","summary":"We demonstrate that social media data represent a useful resource for testing models of legislative and individual-level political behavior and attitudes. First, we develop a model to estimate the ideology of politicians and their supporters using social media data on individual citizens' endorsements of political figures. Our measure allows us to place politicians and more than 6 million citizens who are active in social media on the same metric. We validate the ideological estimates that result from the scaling process by showing they correlate highly with existing measures of ideology from Congress, and with individual-level self-reported political views. Finally, we use these measures to study the relationship between ideology and age, social relationships and ideology, and the relationship between friend ideology and turnout.","tags":null,"title":"Quantifying social media's political space: Estimating ideology from publicly revealed preferences on Facebook","type":"publication"},{"authors":["Sol Messing"],"categories":[],"content":"Yesterday a few of us on Facebook’s Data Science Team released a blogpost showing how candidates are campaigning on Facebook in the 2014 U.S. midterm elections. It was picked up in the Washington Post, in which Reid Wilson calls us \u0026#34;data wizards.\u0026#34; Outstanding.\nI used Hadly Wickham\u0026#39;s ggplot2 for every visualization in the post except a map that Arjun Wilkins produced using D3, and for the first time I used stacked bar charts. Now as I\u0026#39;ve stated previously, one should generally avoid bar charts, and especially stacked bar charts, except in a few specific circumstances.\nBut let\u0026#39;s talk about when not to use stacked bar charts first---I had the pleasure of chatting with Kaiser Fung of JunkCharts fame the other day, and I think what makes his site so compelling is the mix of schadenfreude and Fremdscham that makes taking apart someone else\u0026#39;s mistake such an effective teaching strategy and such a memorable read. I also appreciate the subtle nod to junk art.\nHere\u0026#39;s a typical, terrible stacked bar chart, which I found on http://www.storytellingwithdata.com/ and originally published on a Wall Street Journal blogpost. It shows the share of the personal computing device market by operating system, over time. The problem with using a stacked bar chart is that there are only two common baselines for comparison (the top and bottom of the plotting area), but we are interested in the relative share for more than two OS brands. The post is really concerned with Microsoft, so one solution would be to plot Microsoft versus the rest, or perhaps Microsoft on top versus Apple on the bottom with \u0026#34;Other\u0026#34; in the middle. Then we\u0026#39;d be able to compare the over time market share for Apple and Microsoft. As the author points out, an over time trend can also be visualized with line plots.\nBy far the worst offender I found in my 5 minute Google search was from junkcharts and originally published on Vox. These cumulative sum plots are so bad I was surprised to see them still up. The first problem is that the plots represent an attempt to convey way too much information---either plot total sales or pick a few key brands that are most interesting and plot them on a multi-line chart or set of faceted time series plots. The only brand for which you can quickly get a sense of sales over time is the Chevy Volt because it\u0026#39;s on the baseline. I\u0026#39;m sure the authors wanted to also convey the proportion of sales each year, but if you want to do that just plot the relative sales. Of course, the order in which the bars appear on the plot has no organizing principle, and you need to constantly move your eyes back and forth from the legend to the plot when trying to make sense of this monstrosity.\nAs Kaiser notes in his post, less is often more. Here\u0026#39;s his redux, which uses lines and aggregates by both quarter and brand, resulting in a far superior visualization:\nSo when *should* you use a stacked bar chart? Here are a two scenarios with examples, inspired by work with Eytan Bakshy and conversations with Ta Chiraphadhanakul and John Myles White.\n1. You care about comparing the proportion of two things, in this case the share of posts by Democrats and Republicans, along a variety of dimensions. In this case those dimensions consist of keyword (dictionary-based) categories (above) and LDA topics (below). When these are sorted by relative proportion, the reader gains insight into which campaign strategies and issues are used more by Republican or Democratic candidates.\nYou care about comparing proportions along an ordinal, additive variable such as 5-point party identification, along a set of dimensions. I provide an example from a forthcoming paper below (I\u0026#39;ll re-insert the axis labels once it\u0026#39;s published). Notice that it draws the reader toward two sets of comparisons across dimensions -- one for strong democrats and republicans, the other for the set of *all* Democrats and *all* Republicans. Of course, R code to produce these plots follows:\n# Uncomment these lines and install if necessary: #install.packages(\u0026#39;ggplot2\u0026#39;) #install.packages(\u0026#39;dplyr\u0026#39;) #install.packages(\u0026#39;scales\u0026#39;) library(ggplot2) library(dplyr) library(scales) # We start with the raw number of posts for each party for # each candidate. Then we compute the total by party and # category. catsByParty %\u0026gt;% group_by(party, all_cats) %\u0026gt;% summarise(tot = summ(posts)) # Next, compute the proportion by party for each category # using dplyr::mutate catsByParty \u0026lt;- catsByParty %\u0026gt;% group_by(all_cats) %\u0026gt;% mutate(prop = tot/sum(tot)) # Now compute the difference by category and order the # categories by that difference: catsByParty \u0026lt;- catsByParty %\u0026gt;% group_by(all_cats) %\u0026gt;% mutate(pdiff = diff(prop)) catsByParty$all_cats \u0026lt;- reorder(catsByParty$all_cats, -catsByParty$pdiff) # And plot: ggplot(catsByParty, aes(x=all_cats, y=prop, fill=party)) + scale_y_continuous(labels = percent_format()) + geom_bar(stat=\u0026#39;identity\u0026#39;) + geom_hline(yintercept=.5, linetype = \u0026#39;dashed\u0026#39;) + coord_flip() + theme_bw() + …","date":1412985600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446336e3,"objectID":"5d94250278351913449ae273f52e0d18","permalink":"https://solmessing.netlify.app/post/when-to-use-stacked-barcharts/","publishdate":"2014-10-11T00:00:00Z","relpermalink":"/post/when-to-use-stacked-barcharts/","section":"post","summary":"Yesterday a few of us on Facebook’s Data Science Team released a blogpost showing how candidates are campaigning on Facebook in the 2014 U.S. midterm elections. It was picked up in the Washington Post, in which Reid Wilson calls us \"data wizards.","tags":[],"title":"When to Use Stacked Barcharts?","type":"post"},{"authors":["Solomon Messing","Sean J Westwood"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"7e2c39141cb5dd16b90abe4c28649059","permalink":"https://solmessing.netlify.app/publication/messing-2014-selective/","publishdate":"2020-06-02T02:04:47.454746Z","relpermalink":"/publication/messing-2014-selective/","section":"publication","summary":"Much of the literature on polarization and selective exposure presumes that the internet exacerbates the fragmentation of the media and the citizenry. Yet this ignores how the widespread use of social media changes news consumption. Social media provide readers a choice of stories from different sources that come recommended from politically heterogeneous individuals, in a context that emphasizes social value over partisan affiliation. Building on existing models of news selectivity to emphasize information utility, we hypothesize that social media's distinctive feature, social endorsements, trigger several decision heuristics that suggest utility. In two experiments, we demonstrate that stronger social endorsements increase the probability that people select content and that their presence reduces partisan selective exposure to levels indistinguishable from chance.","tags":null,"title":"Selective exposure in the age of social media: Endorsements trump partisan source affiliation when selecting news online","type":"publication"},{"authors":["Justin Grimmer","Sean J. Westwood","Solomon Messing"],"categories":null,"content":" Media: Mischiefs of Faction. ","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"5d2a8978877a76495e40d836d45689cf","permalink":"https://solmessing.netlify.app/publication/gri-mes-wes-14/","publishdate":"2020-06-02T02:04:47.469664Z","relpermalink":"/publication/gri-mes-wes-14/","section":"publication","summary":"Constituents often fail to hold their representatives accountable for federal spending decisions―even though those very choices have a pervasive influence on American life. Why does this happen? Breaking new ground in the study of representation, The Impression of Influence demonstrates how legislators skillfully inform constituents with strategic communication and how this facilitates or undermines accountability. Using a massive collection of Congressional texts and innovative experiments and methods, the book shows how legislators create an impression of influence through credit claiming messages.  Anticipating constituents' reactions, legislators claim credit for programs that elicit a positive response, making constituents believe their legislator is effectively representing their district. This spurs legislators to create and defend projects popular with their constituents. Yet legislators claim credit for much more―they announce projects long before they begin, deceptively imply they deserve credit for expenditures they had little role in securing, and boast about minuscule projects. Unfortunately, legislators get away with seeking credit broadly because constituents evaluate the actions that are reported, rather than the size of the expenditures.  The Impression of Influence raises critical questions about how citizens hold their political representatives accountable and when deception is allowable in a democracy.","tags":null,"title":"The Impression of Influence: Legislator Communication, Representation, and Democratic Accountability","type":"publication"},{"authors":["Shanto Iyengar","Simon Jackman","Solomon Messing","Nicholas Valentino","Toril Aalberg","Raymond Duch","Kyu S Hahn","Stuart Soroka","Allison Harell","Tetsuro Kobayashi"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"2c65df7390dcb0c75e09462a9dbbe837","permalink":"https://solmessing.netlify.app/publication/iyengar-2013-attitudes/","publishdate":"2020-06-02T02:04:47.461571Z","relpermalink":"/publication/iyengar-2013-attitudes/","section":"publication","summary":"This paper demonstrates that citizens in seven advanced industrialized democracies generally oppose more open immigration policies, but stand ready to admit individual immigrants. Using an experimental design, we demonstrate the applicability of the ``person-positivity bias'' to immigration and investigate the effects of economic and cultural ``deservingness'' on evaluations of individual immigrants. Our results show that immigrants from professional backgrounds elicit higher levels of support than unskilled workers. The bias against unskilled workers is enlarged among immigrants accompanied by families. In comparison with occupational status and the number of family dependents, the target immigrant's cultural attributes---as measured by Middle Eastern nationality and Afrocentric appearance---prove relatively inconsequential as criteria for evaluating immigrants.","tags":null,"title":"Do attitudes about immigration predict willingness to admit individual immigrants? A cross-national test of the person-positivity bias","type":"publication"},{"authors":["Qiaoyun Shi","Laura J Pisani","Yauk K Lee","Solomon Messing","Celina Ansari","Srabani Bhaumik","Lisa Lowery","Brian D Lee","Dan E Meyer","Heike E Daldrup-Link"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"3b02c1428b9d7eb21419f6aea57c1661","permalink":"https://solmessing.netlify.app/publication/shi-2013-evaluation/","publishdate":"2020-06-02T02:04:47.456258Z","relpermalink":"/publication/shi-2013-evaluation/","section":"publication","summary":"Tumor-associated macrophages (TAM) maintain a chronic inflammation in cancers, which is associated with tumor aggressiveness and poor prognosis. The purpose of this study was to: (1) evaluate the pharmacokinetics and tolerability of the novel ultrasmall superparamagnetic iron oxide nanoparticle (USPIO) compound GEH121333; (2) assess whether GEH121333 can serve as a MR imaging biomarker for TAM; and (3) compare tumor MR enhancement profiles between GEH121333 and ferumoxytol. Blood half-lives of GEH121333 and ferumoxytol were measured by relaxometry (n = 4 each). Tolerance was assessed in healthy rats injected with high dose GEH121333, vehicle or saline (n = 4 each). Animals were monitored for 7 days regarding body weight, complete blood counts and serum chemistry, followed by histological evaluation of visceral organs. MR imaging was performed on mice harboring MMTV-PyMT-derived breast adenocarcinomas using a 7 T scanner before and up to 72 h post-injection (p.i.) of GEH121333 (n = 10) or ferumoxytol (n = 9). Tumor R1, R2* relaxation rates were compared between different experimental groups and time points, using a linear mixed effects model with a random effect for each animal. MR data were correlated with histopathology. GEH121333 showed a longer circulation half-life than ferumoxytol. Intravenous GEH121333 did not produce significant adverse effects in rats. All tumors demonstrated significant enhancement on T1, T2 and T2*-weighted images at 1, 24, 48 and 72 h p.i. GEH121333 generated stronger tumor T2* enhancement than ferumoxytol. Histological analysis verified intracellular compartmentalization of GEH121333 by TAM at 24, 48 and 72 h p.i. MR imaging with GEH121333 nanoparticles represents a novel biomarker for TAM assessment. This new USPIO MR contrast agent provides a longer blood half-life and better TAM enhancement compared with the iron supplement ferumoxytol. ","tags":null,"title":"Evaluation of the novel USPIO GEH121333 for MR imaging of cancer immune responses","type":"publication"},{"authors":["Aman Khurana","Hossein Nejadnik","Fanny Chapelin","Olga Lenkov","Rakhee Gawande","Sungmin Lee","Sandeep N Gupta","Nooshin Aflakian","Nikita Derugin","Solomon Messing"," others"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"19fc4256a404029410428d5fbb9e5732","permalink":"https://solmessing.netlify.app/publication/khurana-2013-ferumoxytol/","publishdate":"2020-06-02T02:04:47.457093Z","relpermalink":"/publication/khurana-2013-ferumoxytol/","section":"publication","summary":"Aim To develop a clinically applicable MRI technique for tracking stem cells in matrix-associated stem-cell implants, using the US FDA-approved iron supplement ferumoxytol. Materials \u0026 methods Ferumoxytol-labeling of adipose-derived stem cells (ADSCs) was optimized in vitro. A total of 11 rats with osteochondral defects of both femurs were implanted with ferumoxytol- or ferumoxides-labeled or unlabeled ADSCs, and underwent MRI up to 4 weeks post matrix-associated stem-cell implant. The signal-to-noise ratio of different matrix-associated stem-cell implant was compared with t-tests and correlated with histopathology. Results An incubation concentration of 500 µg iron/ml ferumoxytol and 10 µg/ml protamine sulfate led to significant cellular iron uptake, T2 signal effects and unimpaired ADSC viability. In vivo, ferumoxytol-and ferumoxides-labeled ADSCs demonstrated significantly lower signal-to-noise ratio values compared with unlabeled controls (p \u003c 0.01). Histopathology confirmed engraftment of labeled ADSCs, with slow dilution of the iron label over time. Conclusion Ferumoxytol can be used for in vivo tracking of stem cells with MRI. ","tags":null,"title":"Ferumoxytol: a new, clinically applicable label for stem-cell tracking in arthritic joints with MRI","type":"publication"},{"authors":["Solomon Messing"],"categories":null,"content":"https://www.dropbox.com/s/n0x4iepyj9pzwiw/CH7brief.pdf?raw=true\n","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"f2785b86c361b56c54719cba9be71ffd","permalink":"https://solmessing.netlify.app/publication/messing-2013-friends/","publishdate":"2020-06-02T02:04:47.464876Z","relpermalink":"/publication/messing-2013-friends/","section":"publication","summary":"https://www.dropbox.com/s/n0x4iepyj9pzwiw/CH7brief.pdf?raw=true","tags":null,"title":"Friends that Matter: How Social News Shapes Political Knowledge, Attitudes, and Behavior","type":"publication"},{"authors":["Aman Khurana","Fanny Chapelin","Graham Beck","Olga D Lenkov","Jessica Donig","Hossein Nejadnik","Solomon Messing","Nikita Derugin","Ray Chun-Fai Chan","Amitabh Gaur"," others"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"020aeabe16a9cfe66314a5ff6ce4bc4f","permalink":"https://solmessing.netlify.app/publication/khurana-2013-iron/","publishdate":"2020-06-02T02:04:47.460841Z","relpermalink":"/publication/khurana-2013-iron/","section":"publication","summary":"In vivo--labeled MSCs demonstrated significantly higher ferumoxytol uptake compared with ex vivo--labeled cells. With electron microscopy, iron oxide nanoparticles were localized in secondary lysosomes. In vivo--labeled cells demonstrated significant T2 shortening effects in vitro and in vivo when they were compared with unlabeled control cells (T2 in vivo, 15.4 vs 24.4 msec; P \u003c .05) and could be tracked in osteochondral defects for 4 weeks. Histologic examination confirmed the presence of iron in labeled transplants and defect remodeling. Conclusion: Intravenous ferumoxytol can be used to effectively label MSCs in vivo and can be used for tracking of stem cell transplants with MR imaging. This method eliminates risks of contamination and biologic alteration of MSCs associated with ex vivo--labeling procedures.","tags":null,"title":"Iron administration before stem cell harvest enables MR imaging tracking after transplantation","type":"publication"},{"authors":["Rakhee S Gawande","Gabriel Gonzalez","Solomon Messing","Aman Khurana","Heike E Daldrup-Link"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"e977e8a49de19194f8129472a8c6f88c","permalink":"https://solmessing.netlify.app/publication/gawande-2013-role/","publishdate":"2020-06-02T02:04:47.458138Z","relpermalink":"/publication/gawande-2013-role/","section":"publication","summary":"We retrospectively analyzed DWI scans of 68 consecutive children with 39 benign and 34 malignant abdominal masses. To calculate the apparent diffusion coefficient (ADC) maps and ADC values, we used 1.5-T sequences at TR/TE/b-value of 5,250--7,500/54--64/b = 0, 500 and 3-T sequences at 3,500--4,000/66--73/b = 0, 500, 800. ADC values were compared between benign and malignant and between data derived at 1.5 tesla (T) and at 3 tesla magnetic field strength, using the Mann-Whitney-Wilcoxon test, ANOVA and a receiver operating curve (ROC) analysis.  Results There was no significant difference in ADC values obtained at 1.5 T and 3 T (P = 0.962). Mean ADC values (× 10−3 mm2/s) were 1.07 for solid malignant tumors, 1.6 for solid benign tumors, 2.9 for necrotic portions of malignant tumors and 3.1 for cystic benign lesions. The differences between malignant and benign solid tumors were statistically significant (P = 0.000025). ROC analysis revealed an optimal cut-off ADC value for differentiating malignant and benign solid tumors as 1.29 with excellent inter-observer reliability (alpha score 0.88).  Conclusion DWI scans and ADC values can contribute to distinguishing between benign and malignant pediatric abdominal tumors. ","tags":null,"title":"Role of diffusion-weighted imaging in differentiating benign and malignant pediatric abdominal tumors","type":"publication"},{"authors":[],"categories":[],"content":"After my post on making dotplots with concise code using plyr and ggplot, I got an email from my dad who practices immigration law and runs a website with a variety of immigration resources and tools. He pointed out that the post was written for folks who already know that they want to make dot plots, and who already know about bootstrapped standard errors. That’s not many people.\nIn an attempt to appeal to a broader audience, I’m starting a series in which I’ll outline the key principles I use when developing a visualization. In this post, I’ll articulate these principles, which combine some of Tuft’s aesthetic guidelines with Cleveland’s scientific approach to visualization, which is based on the psychological processes involved in making sense of visualizations, and has been rigorously tested via randomized controlled experiments. Based on these principles, I’ll argue that dotplots and scatterplots are better than other types of plots (especially pie charts) in most situations. In later posts, I’ll demonstrate another innovation whose widespread use I’ll credit to Cleveland and Tufte: the use of multiple panels (aka small multiples, trellis graphics, facets, generalized draftsman’s displays, multivar charts) to clearly convey the same information embedded in more complex and difficult to read visualizations, including multiple line plots and mosaic plots. In future posts I’ll also emphasize why it is important to provide some indication of the noise present in the underlying data using error bars or bands. Along the way, I’ll put you to the test–I’ll present some visualizations of the same data using different visualization techniques and ask you to try to get as much information as you can in 2 seconds from each type of visualization.\nA good visualization conveys key information to those who may have trouble interpreting numbers and/or statistics, which can make your findings accessible to a wider audience (more on this below). Visualizations also give your audience a break from lexical processing, which is especially useful when you are presenting your findings–people can listen to you and process the findings from a well-designed visual at the same time, but most people have trouble listening while reading your PowerPoint bullet points. Visualizations also convey key information embedded in massive amounts of data, which can aid your own exploratory analysis of data, no matter how massive.\nYet most visualizations are flawed, drawn using elements that make it unnecessarily difficult for the human visual system to make sense of things. I see a lot of these visualizations attending research presentations, screening incoming draft manuscripts as the assistant editor for Political Communication, and as a consumer of media info-graphics (CNN is especially bad, have a look at this monstrosity). Kevin Fox has an especially compelling visual speaking to this here. A big part of the problem is that Microsoft makes it easy to draw flashy but ultimately confusing visualizations in Excel. If you are too busy to read this post in full, follow this short list of guidelines and you’ll be on your way to producing elegant visualizations that impose a minimal cognitive burden on your audience:\nNever represent something in 2 or worse yet 3 dimensions if it can be represented in one—NEVER use pie charts, 3-D pie charts, stacked bar charts, or 3-D bar charts.\nRemove as much chart junk as possible–unnecessary gridlines, shading, borders, etc.\nGive your audience a sense of the noise present in your data–draw error bars or confidence bands if you are plotting estimates.\nIf you want to plot multiple types of groups on a single outcome (the visual analog of cross-tabulations/marginals), use multi-paneled plots. These can also help if overploting looks too cluttered.\nAvoid mosaic plots. Instead use paneled histograms.\nDitch the legend if you can (you almost always can).\nThe rest of the content in this series emphasizes why it makes sense to follow these guidelines. In this post I’ll look at the first point in detail and touch on the sixth. These two guidelines are most relevant when you want to look at a quantitative variable (e.g., earnings, vote-share, temperature, etc.) across different qualitative groupings (e.g., industry segment, candidate, party, racial group, season, etc.). This is one of the most common visualization tasks in business, media, and social science, and for this task people often use pie charts and/or bar charts, and occasionally dot plots.\nThe science of graphical perception\nWhen most people think about visualization, they think first of Edward Tufte. Tufte emphasizes integrity to the data, showing relationships between phenomena, and above all else aesthetic minimalism. I appreciate his ruthless crusade against chart junk and pie charts (nice quote from Data without Borders). We share an affinity for multipanel plotting approaches, which he calls “small multiples,” (thanks to Rebecca Weiss for pointing …","date":1330819200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488672e3,"objectID":"ada9e4ddb3a338e86de34d9370723250","permalink":"https://solmessing.netlify.app/post/visualization-series-insight-from-cleveland-and-tufte-on-plotting-numeric-data-by-groups/","publishdate":"2012-03-04T00:00:00Z","relpermalink":"/post/visualization-series-insight-from-cleveland-and-tufte-on-plotting-numeric-data-by-groups/","section":"post","summary":"After my post on making dotplots with concise code using plyr and ggplot, I got an email from my dad who practices immigration law and runs a website with a variety of immigration resources and tools.","tags":[],"title":"Insight From Cleveland And Tufte On Plotting Numeric Data By Groups","type":"post"},{"authors":["Sol Messing"],"categories":[],"content":"Data can often be usefully conceptualized in terms affiliations between people (or other key data entities). It might be useful analyze common group membership, common purchasing decisions, or common patterns of behavior. This post introduces bipartite/affiliation network data and provides R code to help you process and visualize this kind of data. I recently updated this for use with larger data sets, though I put it together a while back.\nPreliminaries Much of the material here is covered in the more comprehensive “Social Network Analysis Labs in R and SoNIA,” on which I collaborated with Dan McFarland, Sean Westwood and Mike Nowak. For a great online introduction to social network analysis see the online book Introduction to Social Network Methods by Robert Hanneman and Mark Riddle.\nBipartite/Affiliation Network Data A network can consist of different ‘classes’ of nodes. For example, a two-mode network might consist of people (the first mode) and groups in which they are members (the second mode). Another very common example of two-mode network data consists of users on a particular website who communicate in the same forum thread. Here’s a short example of this kind of data. Run this in R for yourself - just copy an paste into the command line or into a script and it will generate a dataframe that we can use for illustrative purposes:\ndf \u0026lt;- data.frame( person = c(\u0026#39;Sam\u0026#39;,\u0026#39;Sam\u0026#39;,\u0026#39;Sam\u0026#39;,\u0026#39;Greg\u0026#39;,\u0026#39;Tom\u0026#39;,\u0026#39;Tom\u0026#39;,\u0026#39;Tom\u0026#39;,\u0026#39;Mary\u0026#39;,\u0026#39;Mary\u0026#39;), group = c(\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;d\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;d\u0026#39;), stringsAsFactors = F) df person group 1 Sam a 2 Sam b 3 Sam c 4 Greg a 5 Tom b 6 Tom c 7 Tom d 8 Mary b 9 Mary d Fast, efficient two-mode to one-mode conversion in R Suppose we wish to analyze or visualize how the people are connected directly - that is, what if we want the network of people where a tie between two people is present if they are both members of the same group? We need to perform a two-mode to one-mode conversion.\nTo convert a two-mode incidence matrix to a one-mode adjacency matrix, one can simply multiply an incidence matrix by its transpose, which sum the common 1’s between rows. Recall that matrix multiplication entails multiplying the k-th entry of a row in the first matrix by the k-th entry of a column in the second matrix, then summing, such that the ij-th row-column entry in resulting matrix represents the dot-product of the i-th row of the first matrix and the j-th column of the second. In mathematical notation:\n$$ AB = \\left [ \\begin{array}{cc} a \u0026amp; b \\\\\\ c \u0026amp; d \\end{array} \\right ] \\left [ \\begin{array}{cc} e \u0026amp; f \\\\\\ g \u0026amp; h \\end{array} \\right ] = \\left [ \\begin{array}{cc} ae+bg \u0026amp; af+bh \\\\\\ ce+dg \u0026amp; cf+dh \\end{array} \\right ] $$\nNotice further that multiplying a matrix by its transpose yields the following:\n$$ \\begin{align} AA’ = \\left[ \\begin{array}{cc} a \u0026amp; b \\\\\\ c \u0026amp; d \\end{array} \\right] \\left[ \\begin{array}{cc} a \u0026amp; c \\\\\\ b \u0026amp; d \\end{array} \\right] = \\left[ \\begin{array}{cc} aa+bb \u0026amp; ac+bd \\\\\\ ca+db \u0026amp; cc+dd \\end{array} \\right] \\end{align} $$\nBecause our incidence matrix consists of 0’s and 1’s, the off-diagonal entries represent the total number of common columns, which is exactly what we wanted. We’ll use the %*% operator to tell R to do exactly this. Let’s take a look at a small example using toy data of people and groups to which they belong. We’ll coerce the data to an incidence matrix, then multiply the incidence matrix by its transpose to get the number of common groups between people.\nThis is easy to do using the matrix algebra functions included in R. But first, you need to restructure your (edgelist) network data as an incidence matrix. An incidence will record a 1 for row-column combinations where a tie is present and 0 otherwise. One easy way to do this in R is to use the table function and then coerce the table object to a matrix object:\nm \u0026lt;- table( df ) M \u0026lt;- as.matrix( m ) If you are using the network or sna packages, a network object be coerced via as.matrix(your-network); with the igraph package use get.adjacency(your-network).\nThis is great, but what about if we are working with a really large data set? Network data is almost always sparse—there are far more pairwise combinations of potential connections than actual observed connections. Hence, we’d actually prefer to keep the underlying data structured in edgelist format, but we’d also like access to R’s matrix algebra functionality.\nWe can get the best of both worlds using the Matrix library to construct a sparse triplet representation of a matrix. But we’d also like to avoid building the entire incidence matrix and just feed Matrix our edgelist directly, a point that came up in a recent conversation I had with Sean Taylor. We feed Matrix our ‘person’ column to index ‘i’ (rows in the new incidence matrix), our ‘group’ column to index j (columns in the new incidence matrix), and we repeat ‘1’ for the length of the edgelist to denote an incidence.\nlibrary(\u0026#39;Matrix\u0026#39;) A \u0026lt;- spMatrix(nrow=length(unique(df$person)), ncol=length(unique(df$group)), i = …","date":1330819200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589241600,"objectID":"d65b13c16e4dbae2ab430dcb587674fd","permalink":"https://solmessing.netlify.app/post/working-with-bipartite-affiliation-network-data-in-r/","publishdate":"2012-03-04T00:00:00Z","relpermalink":"/post/working-with-bipartite-affiliation-network-data-in-r/","section":"post","summary":"Data can often be usefully conceptualized in terms affiliations between people (or other key data entities). It might be useful analyze common group membership, common purchasing decisions, or common patterns of behavior.","tags":[],"title":"Working with Bipartite/Affiliation Network Data in R","type":"post"},{"authors":["Rakhee S Gawande","Aman Khurana","Solomon Messing","Dong Zhang","Rosalinda T Castañeda","Robert E Goldsby","Randall A Hawkins","Heike E Daldrup-Link"],"categories":null,"content":"","date":1325376e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376e3,"objectID":"25f28c69be85af54f682a4a6a62c7dbd","permalink":"https://solmessing.netlify.app/publication/gawande-2012-differentiation/","publishdate":"2020-06-02T02:04:47.452805Z","relpermalink":"/publication/gawande-2012-differentiation/","section":"publication","summary":"","tags":null,"title":"Differentiation of Normal Thymus from Anterior Mediastinal Lymphoma and Lymphoma Recurrence at Pediatric PET/CT","type":"publication"},{"authors":["Rakhee S Gawande","Spencer Behr","Solomon Messing","Robert E Goldsby","Randall A Hawkins","Heike E Daldrup-Link"],"categories":null,"content":"","date":1325376e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376e3,"objectID":"5a19add7da3fbf0f14499fc92ed65b79","permalink":"https://solmessing.netlify.app/publication/gawande-2012-fdg/","publishdate":"2020-06-02T02:04:47.459277Z","relpermalink":"/publication/gawande-2012-fdg/","section":"publication","summary":"","tags":null,"title":"FDG PET/CT for the Evaluation of Normal Thymus, Lymphoma Recurrence, and Mediastinal Lymphoma in Pediatric Patients Response","type":"publication"},{"authors":["Justin Grimmer","Solomon Messing","Sean J. Westwood"],"categories":null,"content":" Supplementary Information ","date":1325376e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376e3,"objectID":"416a548a5447ed74020dacde412c4853","permalink":"https://solmessing.netlify.app/publication/grimmer-how-2012/","publishdate":"2020-06-02T02:04:47.468728Z","relpermalink":"/publication/grimmer-how-2012/","section":"publication","summary":"Particularistic spending, a large literature argues, builds support for incumbents. This literature equates money spent in the district with the credit constituents allocate. Yet, constituents lack the necessary information and motivation to allocate credit in this way. We use extensive observational and experimental evidence to show how legislators' credit claiming messages---and not just money spent in the district---affect how constituents allocate credit. Legislators use credit claiming messages to influence the expenditures they receive credit for and to affect how closely they are associated with spending in the district. Constituents are responsive to credit claiming messages---they build more support than other nonpartisan messages. But contrary to expectations from other studies, constituents are more responsive to the total number of messages sent rather than the amount claimed. Our results have broad implications for political representation, the personal vote, and the study of U.S. Congressional elections. ","tags":null,"title":"How Words and Money Cultivate a Personal Vote: The Effect of Legislator Credit Claiming on Constituent Credit Allocation","type":"publication"},{"authors":["Aman Khurana","Hossein Nejadnik","Rakhee Gawande","Guiting Lin","Sungmin Lee","Solomon Messing","Rosalinda Castaneda","Nikita Derugin","Laura Pisani","Tom F Lue"," others"],"categories":null,"content":"","date":1325376e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376e3,"objectID":"786f2c7b23d09fe2c571bf4952ac25de","permalink":"https://solmessing.netlify.app/publication/khurana-2012-intravenous/","publishdate":"2020-06-02T02:04:47.453844Z","relpermalink":"/publication/khurana-2012-intravenous/","section":"publication","summary":"","tags":null,"title":"Intravenous ferumoxytol allows noninvasive MR imaging monitoring of macrophage migration into stem cell transplants","type":"publication"},{"authors":["Solomon Messing","Cameron Marlow","Eytan Bakshy"],"categories":null,"content":"","date":1325376e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376e3,"objectID":"95bb892c20393a8ef472a05f1015ead8","permalink":"https://solmessing.netlify.app/publication/messing-2012-ts/","publishdate":"2020-06-02T02:04:47.470668Z","relpermalink":"/publication/messing-2012-ts/","section":"publication","summary":"","tags":null,"title":"The 2012 Election Day Through the Facebook Lens","type":"publication"},{"authors":["Toril Aalberg","Shanto Iyengar","Solomon Messing"],"categories":null,"content":"","date":1325376e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376e3,"objectID":"57ed5bd2b494f1e506e83c36712449eb","permalink":"https://solmessing.netlify.app/publication/aalberg-2012-deserving/","publishdate":"2020-06-02T02:04:47.451855Z","relpermalink":"/publication/aalberg-2012-deserving/","section":"publication","summary":"","tags":null,"title":"Who is a `deserving'immigrant? An experimental study of Norwegian attitudes","type":"publication"},{"authors":["Solomon Messing"],"categories":null,"content":"","date":129384e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":129384e4,"objectID":"0576dc9e0f87910bd26d4c6e59f06c0d","permalink":"https://solmessing.netlify.app/publication/messing-2011-measuring/","publishdate":"2020-06-02T02:04:47.450255Z","relpermalink":"/publication/messing-2011-measuring/","section":"publication","summary":"","tags":null,"title":"Measuring issue salience: Using supervised machine learning to generate data from free responses to the 'most important problem' question","type":"publication"},{"authors":["Letitia Lew","Truc Nguyen","Solomon Messing","Sean Westwood"],"categories":null,"content":"","date":129384e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":129384e4,"objectID":"fae5010841580654746a3dacd80a79c2","permalink":"https://solmessing.netlify.app/publication/lew-2011-course/","publishdate":"2020-06-02T02:04:47.4509Z","relpermalink":"/publication/lew-2011-course/","section":"publication","summary":"","tags":null,"title":"Of course I wouldn't do that in real life: advancing the arguments for increasing realism in HCI experiments","type":"publication"},{"authors":["Shanto Iyengar","Kyu S Hahn","Solomon Messing","Jeremy N Bailenson"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"b9250434eff1e09336ecd546de2c2f4f","permalink":"https://solmessing.netlify.app/publication/iyengar-2008-explicit/","publishdate":"2020-06-02T02:04:47.449595Z","relpermalink":"/publication/iyengar-2008-explicit/","section":"publication","summary":"Skin color is an explicit racial cue. Although there is strong evidence linking darker skin complexion to the activation of racial stereotypes and adverse societal outcomes, little is known about the extent to which this effect is in play during political campaigns. If white voters make use of this skin complexion cue, we would expect exposure to darker images of a minority candidate to result in a ``dark-skin penalty'' at the ballot box. We investigate the impact of skin complexion on support for Barack Obama at two different stages of the 2008 campaign: Study 1 occurred during the primary campaign and Study 2 during the closing stages of the general election. Our findings suggest that when citizens are still learning about a minority candidate's personal background, subtle changes in skin complexion can have an effect on evaluations of that candidate and that citizens with higher levels of implicit racial bias are less likely to prefer a darker-skinned minority candidate. ","tags":null,"title":"Do explicit racial cues influence candidate preference? The case of skin complexion in the 2008 campaign","type":"publication"},{"authors":null,"categories":null,"content":" Best terminal (iterm 2) setup I’ve found yet Fix annoying error messages in RStudio StatET, an R plugin for Eclipse. Why not just RStudio? StatET has the best object browser available if you’re doing anything with JSON or lists of lists, and will not crash when R crashes, unlike RStudio. It also plays well with cloud instances. Though, yes, I find myself using RStudio on new machines because setting up StatET can be a challenge. Get started using Hugo - Academic. Quickly snap windows side-by-side with shortcut keys on Mac using Rectangle. How to setup shotcut keys in gmail. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d78b1b528b4c87f00c6fea7f67c9ae99","permalink":"https://solmessing.netlify.app/home/techsetup/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home/techsetup/","section":"home","summary":"Best terminal (iterm 2) setup I’ve found yet Fix annoying error messages in RStudio StatET, an R plugin for Eclipse. Why not just RStudio? StatET has the best object browser available if you’re doing anything with JSON or lists of lists, and will not crash when R crashes, unlike RStudio.","tags":null,"title":"Tech Stack","type":"home"},{"authors":null,"categories":null,"content":" Best terminal (iterm 2) setup I’ve found yet Fix annoying error messages in RStudio StatET, an R plugin for Eclipse. Why not just RStudio? StatET has the best object browser available if you’re doing anything with JSON or lists of lists, and will not crash when R crashes, unlike RStudio. It also plays well with cloud instances. Though, yes, I find myself using RStudio on new machines because setting up StatET can be a challenge. Get started using Hugo - Academic. Quickly snap windows side-by-side with shortcut keys on Mac using Rectangle. How to setup shotcut keys in gmail. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d9ed7a09b8c4f6666f7fa92d4d10d00b","permalink":"https://solmessing.netlify.app/techsetup/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/techsetup/","section":"","summary":"Best terminal (iterm 2) setup I’ve found yet Fix annoying error messages in RStudio StatET, an R plugin for Eclipse. Why not just RStudio? StatET has the best object browser available if you’re doing anything with JSON or lists of lists, and will not crash when R crashes, unlike RStudio.","tags":null,"title":"Tech Stack","type":"page"}]