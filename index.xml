<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Sol Messing</title><link>https://solmessing.netlify.app/</link><atom:link href="https://solmessing.netlify.app/index.xml" rel="self" type="application/rss+xml"/><description>Sol Messing</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate><image><url>https://solmessing.netlify.app/media/icon_huedd5de82286bc1d0a1509b535f624c76_25974_512x512_fill_lanczos_center_3.png</url><title>Sol Messing</title><link>https://solmessing.netlify.app/</link></image><item><title>Example Talk</title><link>https://solmessing.netlify.app/talk/example-talk/</link><pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/talk/example-talk/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click on the &lt;strong>Slides&lt;/strong> button above to view the built-in slides feature.
&lt;/div>
&lt;/div>
&lt;p>Slides can be added in a few ways:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Create&lt;/strong> slides using Wowchemy&amp;rsquo;s &lt;a href="https://wowchemy.com/docs/managing-content/#create-slides" target="_blank" rel="noopener">&lt;em>Slides&lt;/em>&lt;/a> feature and link using &lt;code>slides&lt;/code> parameter in the front matter of the talk file&lt;/li>
&lt;li>&lt;strong>Upload&lt;/strong> an existing slide deck to &lt;code>static/&lt;/code> and link using &lt;code>url_slides&lt;/code> parameter in the front matter of the talk file&lt;/li>
&lt;li>&lt;strong>Embed&lt;/strong> your slides (e.g. Google Slides) or presentation video on this page using &lt;a href="https://wowchemy.com/docs/writing-markdown-latex/" target="_blank" rel="noopener">shortcodes&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>Further event details, including &lt;a href="https://wowchemy.com/docs/writing-markdown-latex/" target="_blank" rel="noopener">page elements&lt;/a> such as image galleries, can be added to the body of this page.&lt;/p></description></item><item><title>A 2 million-person, campaign-wide field experiment shows how digital advertising affects voter turnout</title><link>https://solmessing.netlify.app/publication/aggarwal20232/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/aggarwal20232/</guid><description>&lt;ul>
&lt;li>Supplimentary materials at end of manuscript&lt;/li>
&lt;li>&lt;a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/YMKVA1" target="_blank" rel="noopener">Replication materials&lt;/a>&lt;/li>
&lt;li>Media coverage: &lt;a href="https://www.nature.com/articles/d41586-023-00073-6" target="_blank" rel="noopener">Nature&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Past vote data outperformed the polls. How did it go so wrong?</title><link>https://solmessing.netlify.app/post/what-the-polls-got-wrong-in-2020/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/post/what-the-polls-got-wrong-in-2020/</guid><description>&lt;p>It’s becoming clear that the 2020 polls underestimated Trump’s support by anywhere from a 4-8 point margin depending on your accounting&amp;ndash;a significantly worse miss than in 2016, when &lt;a href="https://fivethirtyeight.com/features/the-polls-are-all-right/" target="_blank" rel="noopener">state polls were off but the national polls did relatively well&lt;/a>.&lt;/p>
&lt;p>In fact, this year we were better off using projections based on past vote history in each state to predict how things would go in battleground states, as I&amp;rsquo;ll show below.&lt;/p>
&lt;p>But I also want to start to ask questions about what happened this time around. The polling from 2018 looked encouraging, convincing many pollsters that the post-2016 reckoning had fixed many issues called out in the &lt;a href="https://www.aapor.org/Education-Resources/Reports/An-Evaluation-of-2016-Election-Polls-in-the-U-S.aspx" target="_blank" rel="noopener">2016 AAPOR report on election polling&lt;/a>. After 2018, FiveThirtyEight wrote that the &lt;a href="https://fivethirtyeight.com/features/the-polls-are-all-right/" target="_blank" rel="noopener">&amp;ldquo;Polls are Alright&amp;rdquo;&lt;/a>.&lt;/p>
&lt;p>But the second Miami-Dade reported results from the 2020 election, we knew something was probably wrong with the 2020 polls.&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">Here&amp;#39;s another chart of polls vs. returns that splits the data by how much the polls underestimated Trump. One place where the polls most underestimated Trump is Wisconsin (off by -9 points). Note returns are not yet verified and states are still finalizing their counts. &lt;a href="https://t.co/iM8mjqoAuK">pic.twitter.com/iM8mjqoAuK&lt;/a>&lt;/p>&amp;mdash; Stefan Wojcik (@stefanjwojcik) &lt;a href="https://twitter.com/stefanjwojcik/status/1325786708022079488?ref_src=twsrc%5Etfw">November 9, 2020&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>As Stefan notes (we worked together at Pew Research Center&amp;rsquo;s Data Labs), the error seems slightly lower in key battleground states, though the polls missed big in WI, perhaps in part due to its horrifically bad voter file data.&lt;/p>
&lt;p>Unlike 2016, both state and national polls appeared to underestimate Trump&amp;rsquo;s support, as this early (Nov 7) analysis from &lt;a href="https://twitter.com/thomasjwood" target="_blank" rel="noopener">Tom Wood&lt;/a> shows:&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">Current as to the afternoon on the 7th, and with Senate results too. &lt;a href="https://t.co/EGZGarRPNj">pic.twitter.com/EGZGarRPNj&lt;/a>&lt;/p>&amp;mdash; Tom Wood (@thomasjwood) &lt;a href="https://twitter.com/thomasjwood/status/1325199348553162752?ref_src=twsrc%5Etfw">November 7, 2020&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;!-- [![normal](/img/TWpollingerror.jpeg)](https://twitter.com/thomasjwood/status/1325199348553162752) -->
&lt;h2 id="polling-versus-past-votes">Polling versus past votes&lt;/h2>
&lt;p>Perhaps what surprised me the most about polling this time around was when I went to evaluate some election projections I put together in April that we used internally at Acronym to help evaluate where we might want to spend. I pulled in the &lt;a href="https://www.nytimes.com/live/2020/presidential-polls-trump-biden" target="_blank" rel="noopener">NYTimes polling averages&lt;/a> and compared them with the latest state-level presidential results from the AP. I then did the same for the April projections. Turns out the projections were significantly more accurate than the polling averages:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/PollingVSPastVoteProj.png" alt="normal" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>We used these projections, and other extant data (including the fact that there are two Senate races in play), when making what turned out to be a very lucky decision to start spending money in Georgia. We were one of the biggest and earliest spenders in that race.&lt;/p>
&lt;p>What are these projections? I simply took the last two state-level Presidential and U.S. House election totals, estimated each state&amp;rsquo;s &amp;ldquo;trajectory,&amp;rdquo; and added that to each state&amp;rsquo;s Democratic margin from the previous cycle.&lt;/p>
&lt;p>(Note that I also weighted 60-40 toward the Presidential results, and slightly regularized both the latest margin and the trajectory toward zero.)&lt;/p>
&lt;p>Informing this approach is work from &lt;a href="https://catalist.us/yair-ghitza-phd/" target="_blank" rel="noopener">Yair Ghitza&lt;/a> describing what went wrong in 2016, which suggested polarization and other state-level trends would continue, in addition to national trends or &amp;ldquo;uniform swing.&amp;rdquo; &lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">This paper from &lt;a href="https://twitter.com/SimonJackman?ref_src=twsrc%5Etfw">@SimonJackman&lt;/a> also deserves a big hat tip &lt;a href="https://t.co/CTTpYDPwl2">https://t.co/CTTpYDPwl2&lt;/a>&lt;/p>&amp;mdash; Sol Messing (@SolomonMg) &lt;a href="https://twitter.com/SolomonMg/status/1325564912798752773?ref_src=twsrc%5Etfw">November 8, 2020&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>I should note that this may only have worked because of something peculiar about this election cycle&amp;ndash;I haven&amp;rsquo;t gone an back-tested this approach or anything like that.&lt;/p>
&lt;p>Seems I was not the only one who noticed this kind of pattern:&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">A similar observation from &lt;a href="https://twitter.com/gelliottmorris?ref_src=twsrc%5Etfw">@gelliottmorris&lt;/a> &lt;a href="https://t.co/XSUAhGBZfb">https://t.co/XSUAhGBZfb&lt;/a>&lt;/p>&amp;mdash; Sol Messing (@SolomonMg) &lt;a href="https://twitter.com/SolomonMg/status/1325522770890027008?ref_src=twsrc%5Etfw">November 8, 2020&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;h2 id="what-went-wrong-the-usual-suspects">What went wrong: The Usual Suspects&lt;/h2>
&lt;p>Humble-brag aside, it&amp;rsquo;s worth asking what might have gone wrong with polling in 2020?&lt;/p>
&lt;p>The &lt;a href="https://www.aapor.org/Education-Resources/Reports/An-Evaluation-of-2016-Election-Polls-in-the-U-S.aspx" target="_blank" rel="noopener">2016 AAPOR report on election polling&lt;/a> provides some guidance for how we might start to examine issues with the 2020 polls.&lt;/p>
&lt;p>&lt;strong>Undecided voters&lt;/strong>: Undecideds broke toward Trump late in the election in 2016&amp;ndash;polls found as many as 13 percent of voters were &lt;a href="https://fivethirtyeight.com/features/the-invisible-undecided-voter/" target="_blank" rel="noopener">undecided on election day or planned to vote for a third party&lt;/a>. According to Poynter, there were &lt;a href="https://www.poynter.org/fact-checking/2020/2020-is-not-like-2016-heres-whats-different/" target="_blank" rel="noopener">half as many of these voters in 2020&lt;/a>, so this is unlikely to be as big a factor as in 2016.&lt;/p>
&lt;p>&lt;strong>Low education non-response &amp;amp; adjustment&lt;/strong>: In 2016, individuals lower levels of education were much less likely to answer polls but still voted, and broke for Trump. The national polls adjusted for this but state level polls did not, which is partially why forecasting models that rely on state-level polls missed so hard.&lt;/p>
&lt;p>While many state-level pollsters did this in 2020, Pew Research Center still &lt;a href="https://www.pewresearch.org/methods/2020/08/18/a-resource-for-state-preelection-polling/" target="_blank" rel="noopener">found problems with state level polling this time around&lt;/a>, for example failing to adjust for race and education simultaneously&amp;ndash;non-college whites are far more likely to support Trump than non-college non-whites.&lt;/p>
&lt;p>What&amp;rsquo;s more, pollsters adjusted only for college/non-college, which may not have been enough. They might need to use more fine grained adjustment&amp;ndash;accounting for whether respondents have a high school degree and a college degree. Also error/missing data when people complete education in a survey means trouble if you want to fully fix the issue.&lt;/p>
&lt;p>&lt;strong>Volunteerism &amp;amp; civic engagement&lt;/strong>: Even if you adjust for low levels of non-response among individuals with lower education, pollsters still may have problems reaching &lt;a href="https://www.pewresearch.org/fact-tank/2015/07/21/the-challenges-of-polling-when-fewer-people-are-available-to-be-polled/" target="_blank" rel="noopener">low civic engagement voters, a bias that seems to persist even after modeling/weighting adjustments&lt;/a>. In the past this hasn&amp;rsquo;t mattered as much, but these folks may be showing up to the polls for Trump.&lt;/p>
&lt;h2 id="other-potential-factors">Other Potential Factors&lt;/h2>
&lt;p>&lt;strong>Likely voter models&lt;/strong>: This is difficult to fully unpack since each polling house does this slightly differently and not all publish their methods—some ask a battery of voter questions, some use models, some recruit off the voter file. But there’s only a weak relationship between who votes and who scores high on the likely voter battery. To make matters worse, 2020 was a very high-turnout election, which could have introduced even more instability into likely voter models.&lt;/p>
&lt;p>Another important point from Peter Suzman is that likely voter screens could have inflated estimates of Dem turnout if they asked if respondents had already voted&amp;mdash;it was Democrats who voted early.&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">Easily. An &amp;quot;unlikely&amp;quot; voter (by some screen) on the Dem side that has already voted gets counted; his/her exact counterpart on the GOP side that votes on election day doesn&amp;#39;t. &lt;br>I commented before the election on the possibility that polls would skew Dem because of this.&lt;/p>&amp;mdash; Peter Suzman (@Biomaven) &lt;a href="https://twitter.com/Biomaven/status/1325545770230161408?ref_src=twsrc%5Etfw">November 8, 2020&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>However, that would only explain error in likely voter models, not polling based on registered voters, which also seemed to miss big this cycle, as I pointed out:&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">Still interesting but an issue w this explanation is that while it could interact with likely voter models, it’s unlikely to explain the error in registered voter samples which rely on the voter file over likely voter question batteries. (HT an old friend) &lt;a href="https://t.co/F2xciDV7vr">https://t.co/F2xciDV7vr&lt;/a>&lt;/p>&amp;mdash; Sol Messing (@SolomonMg) &lt;a href="https://twitter.com/SolomonMg/status/1325605403636486146?ref_src=twsrc%5Etfw">November 9, 2020&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>&lt;strong>COVID-19&lt;/strong>: I wrote about &lt;a href="https://solomonmg.github.io/post/trumps-chances-are-better-than-they-look/" target="_blank" rel="noopener">this back in June&lt;/a>. It&amp;rsquo;s possible that COVID-19 made lines long and kept people home in urban areas and non-white communities. Yes we had record turnout but all it takes is a few percent of people who encounter a bit of voting friction, who fail to register in person, don’t get in person canvassing/gotv contact, don’t vote by mail early, and/or don’t vote in vote in person.&lt;/p>
&lt;p>At the same time, &lt;a href="https://www.nytimes.com/2020/11/10/upshot/polls-what-went-wrong.html" target="_blank" rel="noopener">David Shor points out&lt;/a> in a piece by Nate Cohn at the New York Times, that &amp;ldquo;&amp;hellip;after lockdown, Democrats just started taking surveys, because they were locked at home and didn’t have anything else to do.&amp;rdquo;&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/DavidShorOLNCNYT.png" alt="normal" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Without Dems doing the usual in-person registration drives, organizing, canvassing, etc. plus long lines in the hardest hit areas, and with Democrats taking surveys at unusually high rates, we might expect to see Trump overperform in areas hit hardest by COVID-19.&lt;/p>
&lt;p>And indeed the data show just that. &lt;a href="https://www.npr.org/sections/health-shots/2020/11/06/930897912/many-places-hard-hit-by-covid-19-leaned-more-toward-trump-in-2020-than-2016" target="_blank" rel="noopener">NPR has a nice visualization of this&lt;/a>:&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">One potential explanation for Trump&amp;#39;s higher performance in areas hit hard by COVID is that by staying home, Dems didn&amp;#39;t do the usual in-person registration drives, organizing, canvassing, etc. NPR has a nice visualization of this &lt;a href="https://t.co/a8DeO27giE">https://t.co/a8DeO27giE&lt;/a> &lt;a href="https://t.co/1Kc9qNE4jJ">pic.twitter.com/1Kc9qNE4jJ&lt;/a>&lt;/p>&amp;mdash; Sol Messing (@SolomonMg) &lt;a href="https://twitter.com/SolomonMg/status/1326348370869415937?ref_src=twsrc%5Etfw">November 11, 2020&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>Another possibility is that shutdowns, school closings, and job losses stoked anger &amp;amp; resentment in centrist &amp;amp; right-leaning voters. I remember watching a local FB group quickly organize around the issue of school-openings and eventually morph into a hub for protests.&lt;/p>
&lt;p>EDIT: I took a look at his performance by the urbanicity and racial makeup of those counties and here&amp;rsquo;s what I found:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/share_over_2016_pct_nonwhite_covid_up_weighted.png" alt="normal" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/share_over_2016_pct_urban_covid_up_weighted.png" alt="normal" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Trump outperforms 2016 in non-white counties, and UNDER-performs in mostly-white counties. Same for more urban counties. That&amp;rsquo;s consistent w/ covid hitting non-white counties much harder in terms of registration, long-lines, and lower VBM rates.&lt;/p>
&lt;p>That seems to stand in sharp contrast to speculation that Trump would be hit hardest in areas where people are most likely to know someone with COVID.&lt;/p>
&lt;p>&lt;strong>Shy Trump voters&lt;/strong>: There&amp;rsquo;s a hypothesis out there that people are embarrassed to admit that they would vote for Trump. The evidence for this is limited&amp;ndash;Kyle Dropp and co at Morning Consult did some experimental work on this and found that people were slightly more likely &lt;a href="https://morningconsult.com/form/shy-trump-2020/" target="_blank" rel="noopener">in the 2016 primaries (but NOT the General and not in 2020)&lt;/a> to say that they would vote for Trump when answering via online survey compared with speaking with a live pollster over the phone. But they&amp;rsquo;ve done many follow-on surveys since and the pattern doesn&amp;rsquo;t persist.&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">One reason &amp;quot;shy Trump voters&amp;quot; doesn&amp;#39;t make as much sense as you&amp;#39;d think as an explanation for polling errors this year is that Republican candidates for Congress generally outperformed their polls by more than Trump did.&lt;/p>&amp;mdash; Nate Silver (@NateSilver538) &lt;a href="https://twitter.com/NateSilver538/status/1324948324718436352?ref_src=twsrc%5Etfw">November 7, 2020&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>I am skeptical that this could be as much of a factor as some on social media seem to be claiming, but it&amp;rsquo;s hard to get good data to answer this question and acknowledge that absence of evidence is not evidence of absence. A number of commentators have claimed that since the polls underestimated support for all Republicans, this is an unlikely explanation.&lt;/p>
&lt;p>That sounds pretty air-tight at first glance but it&amp;rsquo;s possible that some undecideds, perhaps embarrassed about having Trump as a figurehead of the Republican party, refused to say with certainty who they would actually vote for. Nevertheless, based on the pattern of results we&amp;rsquo;ve seen so far, this really can&amp;rsquo;t explain very much of the polling error this time around.&lt;/p>
&lt;h2 id="the-role-of-election-forecasts">The Role of Election Forecasts&lt;/h2>
&lt;p>If you&amp;rsquo;re a forecaster, it&amp;rsquo;s very easy to look at all the polling data and come away with overconfident estimates of a candidate’s support. Many forecasters in 2016 did just that, failing to account for the fact that error between states and pollsters were likely correlated, and producing estimates that put Clinton’s chances above 95%.&lt;/p>
&lt;p>The Huffington Post famously &lt;a href="https://www.huffpost.com/entry/nate-silver-election-forecast_n_581e1c33e4b0d9ce6fbc6f7f" target="_blank" rel="noopener">roasted FiveThirtyEight&lt;/a> for trying to adjust for this state-level polling error the day before the 2016 election.&lt;/p>
&lt;p>But even when forecasters get it right, forecasting can create firm expectations that one candidate will win, which in 2016 was complicated by destiny-narrative driven by media coverage of election forecasting.&lt;/p>
&lt;p>Sean Westwood, Yph Lelkes and I recently published a &lt;a href="https://solomonmg.github.io/pdf/aggregator.pdf" target="_blank" rel="noopener">research paper&lt;/a> in the Journal of Politics showing just how much additional confidence forecasts give us, and wrote about the implications for the 2020 election in a recent &lt;a href="https://www.usatoday.com/story/opinion/2020/10/01/election-forecasts-can-wrong-you-still-need-vote-column/5857993002/" target="_blank" rel="noopener">USA Today op ed&lt;/a>.&lt;/p>
&lt;p>I believe it was the sharp violation of expectations that was so disappointing to Clinton supporters and so invigorating for the MAGA crowd—the Washington elite had underestimated &amp;ldquo;real Americans&amp;rdquo; yet again.&lt;/p></description></item><item><title>Trump's chances are better than they look</title><link>https://solmessing.netlify.app/post/trumps-chances-are-better-than-they-look/</link><pubDate>Sat, 20 Jun 2020 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/post/trumps-chances-are-better-than-they-look/</guid><description>&lt;p>According to the latest polling research, Trump’s chances of hanging on to power beyond 2020 look pretty dismal. Nate Cohn published an impressive battleground &lt;a href="https://www.nytimes.com/2020/06/25/upshot/poll-2020-biden-battlegrounds.html" target="_blank" rel="noopener">poll from New York Times/Sienna&lt;/a> showing Biden ahead of Trump by at least six points in pivotal states. The Economist’s forecast, powered by Elliott Morris and Andrew Gelman, is suggesting Biden is likely to get 64% of electoral college votes, and that if the election were held 100 times Biden would &lt;a href="https://statmodeling.stat.columbia.edu/2020/06/12/election-2020-is-coming-our-poll-aggregation-model-with-elliott-morris-of-the-economist/" target="_blank" rel="noopener">win 90 times to Trump’s 10&lt;/a>.&lt;/p>
&lt;p>At this point I would like to remind you of that feeling you felt on election night 2016. When a month earlier, &lt;a href="https://www.cnn.com/2016/10/23/politics/hillary-clinton-donald-trump-presidential-polls/index.html" target="_blank" rel="noopener">CNN&amp;rsquo;s &amp;lsquo;Poll of Polls&amp;rsquo; had Clinton up by 9 points&lt;/a> and two prominent forecasters put Clinton’s chances at 99%. Remember that?&lt;/p>
&lt;p>I could probably stop there, but I’m not going to because although we&amp;rsquo;ve fixed some of the issues from 2016, we have COVID-19. And COVID will mess with our election in ways very likely to hurt Democrats, and I know of no pollster factoring this into their method or likely voter model.&lt;/p>
&lt;p>After 2016, Sean Westwood, Yphtach Lelkes and I began a multi-year research project (recently published in the &lt;a href="https://www.journals.uchicago.edu/doi/abs/10.1086/708682?mobileUi=0" target="_blank" rel="noopener">Journal of Politics&lt;/a>) and found that when you have high confidence that one candidate will win, &lt;a href="https://solomonmg.github.io/project/projecting_confidence/" target="_blank" rel="noopener">you’re less likely to vote&lt;/a>. The fact that everyone thought Clinton would win in 2016 shaped Comey&amp;rsquo;s decision to release his infamous letter that &lt;a href="https://fivethirtyeight.com/features/the-comey-letter-probably-cost-clinton-the-election/" target="_blank" rel="noopener">some believe cost Clinton the election&lt;/a>, changed the way campaigns operated, and likely &lt;a href="https://solomonmg.github.io/pdf/aggregator.pdf" target="_blank" rel="noopener">lowered Democratic turnout&lt;/a>.&lt;/p>
&lt;p>In addition to showing this in an experiment, one pattern that clearly pops out in the data we analyzed (ANES timeseries) is that people who think the leading candidate will win by quite a bit report voting at about a 3% lower rate. That&amp;rsquo;s in line with other research showing that early exit polls indicating one candidate is likely to win &lt;a href="https://www.jstor.org/preview-page/10.2307/2748722?seq=1" target="_blank" rel="noopener">decrease turnout&lt;/a>, and are more likely to &lt;a href="https://repository.upenn.edu/cgi/viewcontent.cgi?referer=&amp;amp;httpsredir=1&amp;amp;article=1018&amp;amp;context=asc_papers" target="_blank" rel="noopener">affect Democrats&lt;/a>. Yet this is by no means an upper bound&amp;mdash;one study found more decisive exit polling &lt;a href="https://www.sciencedirect.com/science/article/pii/S0014292115000483" target="_blank" rel="noopener">depressed turnout by 11 points&lt;/a>.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/closerace_vote_anes.png" alt="normal" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>While it&amp;rsquo;s if anything a noisy indicator of the influence Clinton&amp;rsquo;s ostensible lead may have had on Democrats compared with Republicans, the proportion of Democrats who thought Clinton would ‘win by quite a bit’ was much higher in 2016 than for Republicans, and much higher than it&amp;rsquo;d been in many years.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/anes_turnout_closerace_mc_tall.png" alt="normal" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>To be clear, I no longer occupy the role of dispassionate observer&amp;ndash;I’m actively working in politics at the moment.&lt;/p>
&lt;p>So while I like seeing Biden up, let me explain exactly why the margins we’re seeing could be a polling mirage.&lt;/p>
&lt;h2 id="covid-19">COVID-19&lt;/h2>
&lt;p>Are pollsters accounting for the likely decline in urban turnout due to COVID-19? Not if they are assuming typical levels of turnout across urban and rural areas.&lt;/p>
&lt;p>Make no mistake, COVID-19 is already affecting the political process&amp;mdash;look at voter registration. As many colleagues who regularly deal with registration data have warned me, the usual rush of new voter registrations, often from young voters, have &amp;ldquo;fallen off a cliff.&amp;rdquo; Registration numbers started stronger than ever as the new year began, but as &lt;a href="https://fivethirtyeight.com/features/voter-registrations-are-way-way-down-during-the-pandemic/" target="_blank" rel="noopener">538 notes, fell to unprecedented levels in March&lt;/a> as pandemic social distancing measures took effect.&lt;/p>
&lt;p>&lt;a href="https://fivethirtyeight.com/features/voter-registrations-are-way-way-down-during-the-pandemic/" target="_blank" rel="noopener">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/538-voter-registrations-are-way-way-down-during-the-pandemic.png" alt="normal" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/a>&lt;/p>
&lt;p>So it&amp;rsquo;s already hurting Democrats in terms of new registrations, but what might all this mean on election day? At first blush, it may be tempting to say to yourself, “COVID is affecting old people more than the young, and they break conservative so the left is probably fine,” before feeling slightly ashamed that you’re thinking about strategic considerations before the loss of life and sadness this statement implies.&lt;/p>
&lt;p>Think a little deeper and you’ll likely realize that so far COVID-19 has affected left-leaning people in left-leaning places&amp;mdash;&lt;a href="https://www.npr.org/2020/04/12/832455226/what-coronavirus-exposes-about-americas-political-divide" target="_blank" rel="noopener">non-White voters in urban areas&lt;/a> far more than their suburban/rural counterparts. Even the recent &lt;a href="https://www.theatlantic.com/politics/archive/2020/06/coronavirus-surge-sun-belt-could-doom-trump/613495/" target="_blank" rel="noopener">surge in cases in sunbelt states&lt;/a> is hitting urban and non-White regions hardest.&lt;/p>
&lt;p>What’s more, conservatives seem to be far more likely to be willing risk going out and about than liberals. A Pew study shows &lt;a href="https://www.pewresearch.org/fact-tank/2020/05/07/americans-remain-concerned-that-states-will-lift-restrictions-too-quickly-but-partisan-differences-widen/" target="_blank" rel="noopener">Republicans are far more likely&lt;/a> to support lifting COVID restrictions quickly than Democrats.&lt;/p>
&lt;p>&lt;a href="https://www.pewresearch.org/fact-tank/2020/05/07/americans-remain-concerned-that-states-will-lift-restrictions-too-quickly-but-partisan-differences-widen/" target="_blank" rel="noopener">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/covid-partisan-differences-widen-Pew.png" alt="wide" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/a>&lt;/p>
&lt;p>With a deadly pandemic raging, will urban and non-urban voters go to the polls at the usual rates?&lt;/p>
&lt;p>Post-pandemic primary voting has meant a vast reduction in the number of polling places and a big increase in mail-in-ballots. We&amp;rsquo;re seeing this in post-pandemic primaries like this Tuesday&amp;rsquo;s in &lt;a href="https://www.nytimes.com/2020/06/23/us/politics/kentucky-new-york-election-recap.html?action=click&amp;amp;module=Top%20Stories&amp;amp;pgtype=Homepage" target="_blank" rel="noopener">Kentucky, New York, and Virginia&lt;/a>.&lt;/p>
&lt;p>In New York&amp;rsquo;s primary, there were reports of &lt;a href="https://www.thecity.nyc/2020/6/23/21300471/nyc-primary-missing-ballots-busted-machines-pandemic" target="_blank" rel="noopener">missing mail in ballots&lt;/a>. Kentucky also saw reports of &lt;a href="https://www.motherjones.com/politics/2020/06/kentucky-slashes-polling-places-voting-rights-mcgrath-booker-lebron-james/" target="_blank" rel="noopener">long lines that disportionately hit Black neighborhoods&lt;/a>, in a primary that will determine the Democrat who runs against Senate Majority Leader Mitch McConnell.&lt;/p>
&lt;p>What at first looks like maybe a silver lining is the surge in voting by mail-in ballot. And while Trump sees mail-in ballots as a &lt;a href="https://www.politico.com/news/2020/06/19/trump-interview-mail-voting-329307" target="_blank" rel="noopener">threat to his re-election&lt;/a>, the evidence is far from clear that widespread voting by mail would hurt his chances.&lt;/p>
&lt;p>On the contrary, Stanford’s Andy Hall estimates that universal vote by mail &lt;a href="https://www.pnas.org/content/117/25/14052" target="_blank" rel="noopener">should have no impact on either party&amp;rsquo;s vote share&lt;/a>. However, as they note, vote by mail may very well have a disparate impact on minority voters, and their estimates assume that every voter is mailed a ballot, rather than needing to opt-in to voting by mail.&lt;/p>
&lt;p>And just today, the Supreme Court &lt;a href="https://www.nytimes.com/2020/06/26/us/supreme-court-texas-vote-by-mail.html?action=click&amp;amp;module=Top%20Stories&amp;amp;pgtype=Homepage" target="_blank" rel="noopener">denied an emergency request&lt;/a> to allow all citizens in Texas to vote by mail. That’s not the last word, but conservatives are actively fighting measures like this one, which would have made it far easier to prepare to handle a deluge of mail-in ballots in the fall.&lt;/p>
&lt;p>Furthermore, we’re already seeing evidence in the primaries of &lt;a href="https://www.nytimes.com/2020/03/09/us/virus-election-voting.html" target="_blank" rel="noopener">poll-workers failing to show up&lt;/a>, lengthening the already long lines in urban areas that discourage voters.&lt;/p>
&lt;p>If &lt;a href="https://faculty.ucmerced.edu/thansford/Articles/The%20Republicans%20Should%20Pray%20for%20Rain%20-%20Weather,%20Turnour,%20and%20Voting%20in%20U.S.%20Presidential%20Elections.pdf" target="_blank" rel="noopener">a little bit of rain can depress turnout in urban areas&lt;/a>, fear of a deadly pandemic that spreads when you’re standing in line seems likely to as well.&lt;/p>
&lt;p>What’s more, it’s going to take longer to count mail in ballots, and there will almost certainly be confusion about results &lt;a href="https://www.poynter.org/fact-checking/2020/be-patient-on-election-night-2020-counting-the-returns-will-take-time/" target="_blank" rel="noopener">as Poynter recently noted&lt;/a>. Based on the President’s rhetoric around voting by mail, there will almost certainly be legal disputes about the legitimacy of certain results if not the election writ large.&lt;/p>
&lt;p>Buckle up.&lt;/p>
&lt;h2 id="things-change">Things Change&lt;/h2>
&lt;p>Six months ago the big story was the prospect of war with Iran after Trump killed Sulamani. The political world is fundamentally different now and it&amp;rsquo;s more than possible that something important will happen between now and election day with political consequences.&lt;/p>
&lt;p>Does that matter? Andrew Gelman (yes, the same) and Gary King have a paper suggesting it doesn&amp;rsquo;t&amp;mdash;showing that we can &lt;a href="https://www.jstor.org/preview-page/10.2307/194212?seq=1" target="_blank" rel="noopener">predict elections remarkably well despite how much polls fluctuate&lt;/a>. &amp;ldquo;Thus, the general campaign for president seems irrelevant to the outcome &amp;hellip; despite all the media coverage of campaign strategy&amp;hellip; &lt;em>except in very close elections&lt;/em>.&amp;rdquo; And &lt;a href="https://pollyvote.com/en/components/models/retrospective/fundamentals-plus-models/time-for-change-model/" target="_blank" rel="noopener">Alan Abramowitz&amp;rsquo;s forecasting model&lt;/a> which is the kind of model they are referencing and which has done extremely well in the past, has &lt;a href="https://www.rasmussenreports.com/public_content/political_commentary/commentary_by_alan_i_abramowitz/assessing_trump_s_chances_forecasting_the_2020_presidential_election" target="_blank" rel="noopener">Trump&amp;rsquo;s chances in 2020 nearly even&lt;/a> (though both the economy and Trump&amp;rsquo;s polling numbers have suffered since).&lt;/p>
&lt;p>So even if you&amp;rsquo;re one of those people who think that in general the ebb and flow of historical events largely does not impact U.S. elections, it may matter more in 2020 than in a typical year, and that&amp;rsquo;s before you even factor in a global pandemic that has upended life in America.&lt;/p>
&lt;p>OK, but how many people are really undecided about Trump? When asked who they’d vote for, 8 percent of people in Nate Cohn’s poll said something other than Biden or Trump.&lt;/p>
&lt;p>According to the American Association of Public Opinion Research 2016 post-mortem, &lt;a href="https://www.aapor.org/Education-Resources/Reports/An-Evaluation-of-2016-Election-Polls-in-the-U-S.aspx" target="_blank" rel="noopener">a tsunami of undecided voters went to Trump&lt;/a>, which was a major reason we thought Clinton was going to win in 2016. One controversial possibility is that some of these undecided voters were actually &amp;ldquo;&lt;a href="https://morningconsult.com/2016/11/03/shy-trump-social-desirability-undercover-voter-study/" target="_blank" rel="noopener">shy Trump supporters&lt;/a>,&amp;rdquo; which might explain the swing. Of course, another controversial possibility is that &lt;a href="https://fivethirtyeight.com/features/the-comey-letter-probably-cost-clinton-the-election/" target="_blank" rel="noopener">the Comey letter cost Clinton the election&lt;/a>.&lt;/p>
&lt;p>Regardless, Trump is more well-known now than in 2016 and there &lt;em>are&lt;/em> fewer undecideds this time around. But we have no clue how these folks will break in 2020, and in 2016 they broke for Trump.&lt;/p>
&lt;h2 id="correcting-for-political-engagement">Correcting for Political Engagement&lt;/h2>
&lt;p>That 8 percent undecided number above may very well be an underestimate. Arguably the biggest problem in survey research today is that you can&amp;rsquo;t fully adjust for the &lt;a href="https://www.pewresearch.org/fact-tank/2015/07/21/the-challenges-of-polling-when-fewer-people-are-available-to-be-polled/" target="_blank" rel="noopener">bias toward high political knowledge respondents&lt;/a>. And low political knowledge voters are more likely than others to be undecided.&lt;/p>
&lt;h2 id="education">Education&lt;/h2>
&lt;p>Likewise, it&amp;rsquo;s difficult to survey Americans with low education. The &lt;strong>vast majority&lt;/strong> of polls fail to recruit a representative swath of these potential voters and cannot fully adjust away the bias.&lt;/p>
&lt;p>OK so what?&lt;/p>
&lt;p>No election has &lt;a href="https://www.pewresearch.org/fact-tank/2016/11/09/behind-trumps-victory-divisions-by-race-gender-education/" target="_blank" rel="noopener">split on education like 2016&lt;/a> going back to the beginning of Pew Research Center&amp;rsquo;s data on this in 1980. Non-college Whites voted for Trump over their college educated counterparts by a &lt;strong>35 point margin&lt;/strong>. And the best retrospective analyses show that his biggest gains have &lt;a href="https://williammarble.co/docs/vb.pdf" target="_blank" rel="noopener">come from low-education White moderates in battleground states&lt;/a> (and &lt;em>not&lt;/em> as many have presumed, from those with conservative views on race and immigration, across the educational spectrum).&lt;/p>
&lt;p>Many 2016 polls did not adjust their samples to account for education&amp;mdash;something that mattered far less in the past and something not easy to do correctly. They systematically underestimated Trump’s support in part because of this issue.&lt;/p>
&lt;p>Although the NYT/Sienna poll and now many others do target and weight by education to increase representativeness, the &lt;a href="https://int.nyt.com/data/documenttools/nyt-siena-poll-methodology-june-2020/f6f533b4d07f4cbe/full.pdf" target="_blank" rel="noopener">methodology&lt;/a> shows this poll (along with most) lump together everyone without a college degree. While the &lt;a href="https://www.aapor.org/Education-Resources/Reports/An-Evaluation-of-2016-Election-Polls-in-the-U-S.aspx" target="_blank" rel="noopener">AAPOR report concludes this may be ok&lt;/a>, Trump&amp;rsquo;s support does appear to increase as education decreases, which means failing to disaggregate &amp;ldquo;no high school degree,&amp;rdquo; &amp;ldquo;high school degree,&amp;rdquo; and &amp;ldquo;some college&amp;rdquo; when adjusting for education may very well result in some bias in favor of Trump.&lt;/p>
&lt;h2 id="higher-error-in-subnational-polls">Higher Error in Subnational Polls&lt;/h2>
&lt;p>The NYT/Sienna poll is one of the best subnational polls out there, but the error in battleground polls like this is generally &lt;a href="https://yougov.co.uk/topics/politics/articles-reports/2016/11/11/first-thoughts-polling-problems-2016-us-elections" target="_blank" rel="noopener">higher than national polls&lt;/a>. It&amp;rsquo;s harder to reach the right mix of people in individual states in a short period of time which increases the error. By error, I mean the actual error in predicting presidential vote share, not the reported &amp;ldquo;margin of error,&amp;rdquo; which is usually &lt;a href="https://www.nytimes.com/2016/10/06/upshot/when-you-hear-the-margin-of-error-is-plus-or-minus-3-percent-think-7-instead.html" target="_blank" rel="noopener">&lt;strong>around half the actual error&lt;/strong>&lt;/a>.&lt;/p>
&lt;p>The reported margin of error here is about 2%, so doubling that, 4%, plus the 8 percent who didn&amp;rsquo;t say Biden or Trump means there may be 12% wiggle room, possibly more.&lt;/p>
&lt;h2 id="issues-with-the-voter-file">Issues with the Voter File&lt;/h2>
&lt;p>The way pollsters recruit people for their survey has a huge impact on accuracy. If you don’t get data from the right mix of people you’re not going to get a good sense of which candidate is ahead, and you can only get so much juice out of adjusting your polls using approaches like weighting.&lt;/p>
&lt;p>Pollsters often use random-digit dialing to get a representative sample, but many of the best election surveys run today are now conducted by calling people from the voter file. The &lt;a href="https://www.nytimes.com/2018/09/06/upshot/live-poll-explainer.html" target="_blank" rel="noopener">Times used the voter file&lt;/a> in part so they could poll congressional districts, which are drawn in such idiosyncratic shapes that they don’t line up with area codes nor almost any other data set with phone numbers.&lt;/p>
&lt;p>The Times uses the voter file to target specific subsets of the population that are hard to reach, such as low-education voters. Unfortunately, running a voter-file based poll may still not get enough low-education voters&amp;mdash;as &lt;a href="https://www.pewresearch.org/methods/2018/10/09/performance-of-the-samples/" target="_blank" rel="noopener">Pew Research Center’s voter file study&lt;/a> showed (it used the same voter file vendor as does the Times&amp;mdash;L2). So you have to rely on statistical adjustment, increasing error.&lt;/p>
&lt;p>What’s more, in that Pew study only &lt;a href="https://www.pewresearch.org/methods/2018/10/09/comparing-survey-sampling-strategies-random-digit-dial-vs-voter-files/#overview-of-study-methodology" target="_blank" rel="noopener">62% of respondents who answered on a cell phone&lt;/a> were the actual person on the voter file. And these quality issues vary a lot by state&amp;mdash;remember, the file is first gathered by the secretary of state and is subject to local laws and regulations. For example, Wisconsin’s voter file is notoriously bad.&lt;/p>
&lt;p>All this increases total survey error and the chance that systematic biases will creep in.&lt;/p>
&lt;p>While the polling does indeed suggest better news than if it showed Trump ahead, this is still very likely a highly competitive race.&lt;/p></description></item><item><title>Projecting Confidence</title><link>https://solmessing.netlify.app/project/projecting_confidence/</link><pubDate>Mon, 18 May 2020 22:48:29 -0500</pubDate><guid>https://solmessing.netlify.app/project/projecting_confidence/</guid><description>&lt;p>Inspired by Donald Trump&amp;rsquo;s surprise victory over Hillary Clinton in the 2016 general election, &lt;a href="https://www.dartmouth.edu/~seanjwestwood/" target="_blank" rel="noopener">Sean Westwood&lt;/a>, &lt;a href="http://ylelkes.com/" target="_blank" rel="noopener">Yphtach Lelkes&lt;/a> and I set out to interrogate the question of whether elecion forecasts&amp;mdash;particularly probablistic forecasts&amp;mdash;might create a sense of inevitability, and ultimately lead people to stay home on election day.&lt;/p>
&lt;p>Clinton herself was quoted in &lt;a href="http://nymag.com/daily/intelligencer/2017/05/hillary-clinton-life-after-election.html?mid=nymag_press" target="_blank" rel="noopener">New York Magazine&lt;/a> after the election:&lt;/p>
&lt;blockquote>
&lt;p>I had people literally seeking absolution&amp;hellip; ‘&lt;em>I’m so sorry I didn’t vote. I didn’t think you needed me.&lt;/em>’ I don’t know how we’ll ever calculate how many people thought it was in the bag, because the percentages kept being thrown at people — ‘&lt;em>Oh, she has an 88 percent chance to win!&lt;/em>’&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Is it plausible that forecasting could have affected the election?&lt;/strong>&lt;/p>
&lt;p>For this phenomena to affect an election, it must:&lt;/p>
&lt;ol>
&lt;li>be visible in the media so it reaches potential voters,&lt;/li>
&lt;li>depress turnout, and&lt;/li>
&lt;li>affect one side more than the other. In the case of 2016, that means affecting Clinton&amp;rsquo;s supporters (and/or Clinton campaigners) more than Trump&amp;rsquo;s.&lt;/li>
&lt;/ol>
&lt;p>We found evidence for all of the above. First, witness the rise of forecasts since 2008, when FiveThirtyEight first came on the scene:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/forecast_google_news.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>What&amp;rsquo;s more, there is good evidence that one side will be more affected. Our research (see results below) suggests that &lt;em>candidate who is ahead in the polls is more affected&lt;/em> by probablistic forecasts. In 2016, that was Hillary.&lt;/p>
&lt;p>And irrespective of 2016, it&amp;rsquo;s outlets with a &lt;em>left-leaning audience&lt;/em> that publish and cover election forecasts. The websites that present their poll aggregation results in terms of probabilities have left-leaning (negative) social media audiences&amp;mdash;only realclearpolitics.com, which doesn’t emphasize win-probabilities, has a conservative audience:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/bma_science_alignment.png" alt="half" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>These data come from the average self-reported ideology of people who share links to various sites hosting poll-aggregators on Facebook, data that come from &lt;a href="http://science.sciencemag.org/content/early/2015/05/06/science.aaa1160.full" target="_blank" rel="noopener">this paper&lt;/a>’s &lt;a href="http://dx.doi.org/10.7910/DVN/LDJ7MS" target="_blank" rel="noopener">replication materials&lt;/a>.&lt;/p>
&lt;p>When you look at the balance of coverage of probabilistic forecasts on major television broadcasts, there is more coverage on MSNBC, which has a more liberal audience.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/msnbc_mentions.png" alt="half" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>&lt;strong>How much influence do forecasters really have?&lt;/strong>&lt;/p>
&lt;p>It&amp;rsquo;s increadibly difficult to tease out when one media outlet is influencing another. However, a freak event in 2018 allows us to get some traction on this question, and suggests that FiveThirtyEight&amp;rsquo;s 2018 coverage was highly influential.&lt;/p>
&lt;p>After FiveThirtyEight&amp;rsquo;s real-time forecast suddenely moved the the GOP&amp;rsquo;s odds of taking the House from single digits to about 60% at around 8:15PM, PredictIt&amp;rsquo;s odds on the GOP rose above 50-50, &amp;amp; &lt;em>U.S. government bond yields rose 2-4 basis points.&lt;/em> FiveThirtyEight then altered it&amp;rsquo;s prediction system and the markets calmed down.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/538-markets.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>This spike seems to have occurred because a number of big, &lt;a href="https://fivethirtyeight.com/live-blog/2018-election-results-coverage/#3495" target="_blank" rel="noopener">Republican-dominated districts started reporting returns before those that went toward Democrats&lt;/a> and because it was making inferences from partial vote counts:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/538realtimepolling.jpg" alt="half" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>This was &lt;a href="https://ftalphaville.ft.com/2018/11/07/1541617447000/Debt-markets-let-us-know-what-they-think-about-Republicans-last-night/" target="_blank" rel="noopener">first reported by Colby Smith &amp;amp; Brian Greeley of FT.com&lt;/a>. They report that because markets expected to see more inflation under a Republican House (high spending, low taxes) the U.S. Bond yield rose.&lt;/p>
&lt;p>Was this just a correlation? Possibly, but there was pretty much nothing else happening in the U.S., and it was like 1 am in Europe, as pointed out in the FT.com piece above.&lt;/p>
&lt;p>Josh Tucker suggested that &lt;a href="http://themonkeycage.org/2012/10/convergence-between-polls-and-prediction-markets-in-us-presidential-election/" target="_blank" rel="noopener">538 might be driving prediction markets&lt;/a> back in 2012 in a Monkey Cage blogpost.&lt;/p>
&lt;p>&lt;strong>Our research on forecasting and perception&lt;/strong>&lt;/p>
&lt;p>&lt;a href="https://solmessing.netlify.app/pdf/aggregator.pdf">Our research&lt;/a> shows that probablistic election forecasts make a race look less competitive. Participants in a national probability survey-experiment were substantially more certain that one candidate would win a hypothetical race after seeing a probablistic forecast than after seeing the equivalent vote share estimate and margin of error. This is a big effect&amp;mdash;those are confidence intervals not standard errors, with p-values below $$10^{-11}$$.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/certaintyc.png" alt="normal" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>&lt;strong>Why do people do this?&lt;/strong>&lt;/p>
&lt;p>More research is needed here but we do have some leads. First, small differences in the election metric most familiar to the public—vote share estimates—generally correspond to very large differences in the probability of a candidate’s chance of victory.&lt;/p>
&lt;p>Andy Gelman referenced this in passing in a &lt;a href="https://andrewgelman.com/2012/10/22/is-it-meaningful-to-talk-about-a-probability-of-65-7-that-obama-will-win-the-election/" target="_blank" rel="noopener">2012 blogpost&lt;/a> questioning the decimal precision (0.1 percent) that 538 used to communicate its forecast on its website:&lt;/p>
&lt;blockquote>
&lt;p>That’s right: a change in 0.1 of win probability corresponds to a 0.004 percentage point share of the two-party vote. I can’t see that it can possibly make sense to imagine an election forecast with that level of precision&amp;hellip;&lt;/p>
&lt;/blockquote>
&lt;p>Second, people sometimes confuse probabilistic forecasts with vote share projections, and incorrectly conclude that a candidate is projected to say win 85% percent of the vote, rather than to having an 85% chance of winning the election. About 1 in 10 peope did this in our experiment.&lt;/p>
&lt;p>As &lt;a href="https://twitter.com/jbenton/status/1059898288139354112" target="_blank" rel="noopener">Joshua Benton pointed out in a tweet&lt;/a>, TalkingPointsMemo.com &lt;a href="https://talkingpointsmemo.com/news/issa-calls-race-early" target="_blank" rel="noopener">made this very mistake&lt;/a>:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/TPMCorrection.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Finally, people tend to think in qualitative terms about the probability of events {%cite sunstein2002probability%}, {%cite keren1991calibration%}. An 85% likelihood that something will happen means it&amp;rsquo;s going to happen. These studies may help explain why after the 2016 election, so many criticized forecasters for “getting it wrong” (see &lt;a href="https://www.nytimes.com/2016/11/10/technology/the-data-said-clinton-would-win-why-you-shouldnt-have-believed-it.html" target="_blank" rel="noopener">this&lt;/a> and &lt;a href="http://www.slate.com/articles/news_and_politics/politics/2016/01/nate_silver_said_donald_trump_had_no_shot_where_did_he_go_wrong.html" target="_blank" rel="noopener">this&lt;/a>).&lt;/p>
&lt;p>&lt;strong>What about voting?&lt;/strong>&lt;/p>
&lt;p>Perhaps most critically, we show that probabilistic forecasts showing more of a blowout can lower voting. In Study 1, we find limited evidence of this based on self reports. In Study 2, we show that when participants are faced with incentives designed to simulate real world voting, they are less likely to vote when probabilistic forecasts show higher odds of one candidate winning. Yet they are not responsive to changes in vote share.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/FT_18.01.03_prob_vote.png" alt="normal" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>&lt;strong>Could this actually affect real world voting?&lt;/strong>&lt;/p>
&lt;p>Consider 2016&amp;mdash;an unusually high number of Democrats thought the leading candidate would &lt;em>win by quite a bit&lt;/em>:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/anes_turnout_closerace_mc_tall.png" alt="normal" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>And people who say the leading candidate will &lt;em>win by quite a bit&lt;/em> in pre-election polling are about three percentage points less likely to say they voted after the election than people who say it’s a close race. That’s after controlling for election year, prior turnout, and party identification.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/closerace_vote_anes.png" alt="normal" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>The data here are from the &lt;a href="https://electionstudies.org" target="_blank" rel="noopener">American National Election Study (ANES)&lt;/a> and go back to 1952.&lt;/p>
&lt;p>Past social science research also provides evidence that the perception of a close race boosts turnout. Some of the best evidence comes from work that analyzes the effects of releasing exit polling results before voting ends, which clearly removes uncertainty. Work examining the effects of East Coast television networks’ “early calls” for one candidate or another on West Coast turnout generally find small but substantively meaningful effects, despite the fact that these calls occur &lt;a href="https://repository.upenn.edu/cgi/viewcontent.cgi?article=1018&amp;amp;context=asc_papers" target="_blank" rel="noopener">late on election day&lt;/a>, see also &lt;a href="https://academic.oup.com/poq/article-pdf/50/3/331/5135691/50-3-331.pdf?casa_token=Ize8hznQUHAAAAAA:6tMTMfvGSN4tFXyyXbwkew4E47cLlCG8FegNu4ulkzqHE3hJZMzfurBb-Y1GWQcvLbZTYUysOMebxg" target="_blank" rel="noopener">this&lt;/a>. Similar work exploiting voting reform as a natural experiment &lt;a href="https://eprints.qut.edu.au/83681/1/1-s2.0-S0014292115000483-main.pdf" target="_blank" rel="noopener">shows a full 11 percentage point decrease&lt;/a> in turnout in the French overseas territories that voted after exit polls were released. These designs are not confounded with the tendency for campaigns to invest more in campaigns in competitive races.&lt;/p>
&lt;p>Researchers consistently find robust correlations between tighter elections and higher turnout &lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S0261379405000910" target="_blank" rel="noopener">see this&lt;/a>; and &lt;a href="https://biopen.bi.no/bi-xmlui/bitstream/handle/11250/2389104/Geys_ES%202016.pdf?sequence=5&amp;amp;isAllowed=y" target="_blank" rel="noopener">this&lt;/a> for reviews]. Furthermore, there is evidence from statistical models that &lt;a href="https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1052&amp;amp;context=poliscifacpub" target="_blank" rel="noopener">prior election returns also explain turnout&lt;/a> above and beyond campaign spending, particularly when good polling data is unavailable.&lt;/p>
&lt;p>Field experiments provide additional evidence that perceptions of higher electoral competition increases turnout. This work finds substantive effects on turnout when polling results showing a closer race are delivered &lt;a href="%28https://huber.research.yale.edu/materials/67_paper.pdf%29">via telephone&lt;/a> [among those who were reached] but null results when relying on &lt;a href="https://www.nber.org/papers/w23071" target="_blank" rel="noopener">postcards&lt;/a> to deliver closeness messages. Finally, one study conducted in the weeks leading up to the 2012 presidential election found higher rates of self-reported, post-election turnout when delivering ostensible polling results showing Obama neck-and-neck with Romney &lt;a href="http://www.aapor.org/AAPOR_Main/media/AnnualMeetingProceedings/2013/Session_C-5-1-Vannette.pdf" target="_blank" rel="noopener">which was not consistent with the extant polling data showing a comfortable Obama lead&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Could this affect politicians as well?&lt;/strong>&lt;/p>
&lt;p>Candidates’ perceptions of the closeness of an election can affect campaigning and representation {%cite enos2015campaign%}, {%cite Mutz:1997wy%}.&lt;/p>
&lt;p>These perceptions can also shape policy decisions—-for example, prior to the 2016 election, the Obama administration’s confidence in a Clinton victory was reportedly a factor in the muted response to &lt;a href="https://www.washingtonpost.com/graphics/2017/world/national-security/obama-putin-election-hacking/" target="_blank" rel="noopener">Russian intervention in the election&lt;/a>.&lt;/p>
&lt;p>And former FBI Director James Comey, because of his confidence in a Clinton victory, said he felt that it was his duty to write a letter to Congress on October 28 saying he was reopening the investigation into her emails. Comey explained his actions based on his certain belief in a Clinton win: &amp;lsquo;&amp;rsquo;[S]he&amp;rsquo;s gonna be elected president, and if I hide this from the American people, she&amp;rsquo;ll be illegitimate the moment she&amp;rsquo;s elected, the moment this comes out&amp;rsquo;&amp;rsquo; {%cite keneally_2018%}. Nate Silver at one point said &amp;lsquo;&amp;rsquo;&lt;a href="https://fivethirtyeight.com/features/the-comey-letter-probably-cost-clinton-the-election/" target="_blank" rel="noopener">the Comey letter probably cost Clinton the Election&lt;/a>.&amp;rsquo;&amp;rsquo;&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/ComeyABCCNNresize.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>&lt;strong>Media coverage&lt;/strong>
&lt;a href="https://www.washingtonpost.com/news/politics/wp/2018/02/06/clintons-achilles-heel-in-2016-may-have-been-overconfidence/?utm_term=.619133ce9312" target="_blank" rel="noopener">Washington Post&lt;/a>, &lt;a href="https://fivethirtyeight.com/features/politics-podcast-whats-so-wrong-with-nancy-pelosi/" target="_blank" rel="noopener">FiveThirthyEight’s Politics Podcast&lt;/a>, &lt;a href="http://nymag.com/intelligencer/2018/02/americans-dont-understand-election-probabilities.html?gtm=bottom&amp;amp;gtm=bottom" target="_blank" rel="noopener">New York Magazine&lt;/a>, &lt;a href="https://politicalwire.com/2018/02/06/election-forecasts-lower-voter-turnout/" target="_blank" rel="noopener">Political Wire&lt;/a>.&lt;/p></description></item><item><title>Facebook Condor URLs Data Release</title><link>https://solmessing.netlify.app/project/condor_data_release/</link><pubDate>Mon, 18 May 2020 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/project/condor_data_release/</guid><description>&lt;p>On January 17, 2020 my team at Facebook launched one of the largest social science data sets ever constructed. It’s meant to facilitate research on misinformation from across the web, shared and spread on Facebook.&lt;/p>
&lt;p>&lt;a href="https://solmessing.netlify.app/pdf/Facebook_DP_URLs_Dataset.pdf">Full details on the release here&lt;/a>.&lt;/p>
&lt;p>We also released the &lt;a href="https://github.com/facebookresearch/URL-Sanitization" target="_blank" rel="noopener">URL santization framework&lt;/a>, which I implemented (and which my SWE colleagues refactored).&lt;/p>
&lt;p>What makes this data release unprecedented is that it contains &lt;em>exposure data&lt;/em> describing external links that billions of users saw and read while using the site.&lt;/p>
&lt;p>The data set goes beyond URL-level data, breaking down exposure and interactions by month, country, age, gender, and in the U.S., political page affinity (see Barbera et al 2015).&lt;/p>
&lt;p>The data contain two tables: (1) a “URL attributes” table describing the 38 million URLs in the data set, including how many times users tagged those posts as containing misinformation, harassment, etc. and (2) a “breakdown” table, which aggregates counts of actions taken on urls, broken out by user demographics and URL attributes.&lt;/p>
&lt;p>The &lt;a href="https://solmessing.netlify.app/pdf/Facebook_DP_URLs_Dataset.pdf">technical documentation&lt;/a> reflects more work than most papers I&amp;rsquo;ve written: . This list of authors reflects the scale of this massive team effort, and that&amp;rsquo;s before you include increadibly helpful advice we got from a number of computer scientists in the academy listed in the acknowledgements.&lt;/p>
&lt;p>Perhaps most importantly, this release provides guarantees about anonymity in an incredibly rigorous way&amp;ndash;action-level differential privacy, while preserving more underlying signal in the data.&lt;/p></description></item><item><title>Impression of Influence</title><link>https://solmessing.netlify.app/project/impression_of_influence/</link><pubDate>Sun, 17 May 2020 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/project/impression_of_influence/</guid><description>&lt;p>&lt;a href="https://solmessing.netlify.app/pdf/GrimmerWestwoodMessingBook.pdf">&lt;strong>The Impression of Influence: Legislator Communication, Representation, and Democratic Accountability&lt;/strong>&lt;/a>
&lt;strong>Princeton University Press, 2015&lt;/strong>. With Justin Grimmer and Sean
Westwood&lt;/p>
&lt;ul>
&lt;li>Media: &lt;a href="http://www.mischiefsoffaction.com/2015/01/its-frequency-not-size-compromise.html" target="_blank" rel="noopener">Mischiefs of Faction&lt;/a>.&lt;/li>
&lt;/ul></description></item><item><title>Why Election Forecasting Matters</title><link>https://solmessing.netlify.app/post/response-to-fivethirtyeights-podcast-about-our-paper-projecting-confidence/</link><pubDate>Sun, 26 Apr 2020 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/post/response-to-fivethirtyeights-podcast-about-our-paper-projecting-confidence/</guid><description>&lt;p>Do you remember the night of Nov 8, 2016? I was glued to election coverage and obsessively checking probabilistic forecasts, wondering whether Clinton might do &lt;em>so well&lt;/em> that she’d win in places like my home state of Arizona. Although FiveThirtyEight had Clinton&amp;rsquo;s chances at beating Trump at around 70%, most other forecasters had her at around 90%.&lt;/p>
&lt;p>When she lost, &lt;a href="https://www.nytimes.com/2016/11/10/us/politics/donald-trump-election-reaction.html" target="_blank" rel="noopener">many on both sides of the aisle&lt;/a> were shocked. My co-authors and I wondered if America&amp;rsquo;s seeming confidence in a Clinton victory wasn’t driven in part by increasing coverage of probabilistic forecasts. And, if a Clinton victory looked inevitable, what did that do to turnout?&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/forecast_google_news.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>We weren&amp;rsquo;t alone. Clinton herself was quoted in &lt;a href="http://nymag.com/daily/intelligencer/2017/05/hillary-clinton-life-after-election.html?mid=nymag_press" target="_blank" rel="noopener">New York Magazine&lt;/a> after the election:&lt;/p>
&lt;blockquote>
&lt;p>I had people literally seeking absolution&amp;hellip; ‘&lt;em>I’m so sorry I didn’t vote. I didn’t think you needed me.&lt;/em>’ I don’t know how we’ll ever calculate how many people thought it was in the bag, because the percentages kept being thrown at people — ‘&lt;em>Oh, she has an 88 percent chance to win!&lt;/em>’&lt;/p>
&lt;/blockquote>
&lt;p>Enter our recent &lt;a href="http://www.pewresearch.org/fact-tank/2018/02/06/use-of-election-forecasts-in-campaign-coverage-can-confuse-voters-and-may-lower-turnout/" target="_blank" rel="noopener">blog post&lt;/a> and &lt;a href="https://papers.ssrn.com/abstract=3117054" target="_blank" rel="noopener">paper released on SSRN&lt;/a>, “Projecting confidence: How the probabilistic horse race confuses and de-mobilizes the public,” by Sean Westwood, Solomon Messing, and Yphtach Lelkes. While our work cannot definitively say whether probabilistic forecasts played a decisive role in the 2016 election, it does indeed show that compared to more conventional vote share projections, probabilistic forecasts can confuse people, can give people more confidence that the candidate depicted as being ahead will win, may decrease turnout, and that liberals in the U.S. are more likely to encounter them. We appreciate the media attention to this work, including coverage by the &lt;a href="https://www.washingtonpost.com/news/politics/wp/2018/02/06/clintons-achilles-heel-in-2016-may-have-been-overconfidence/" target="_blank" rel="noopener">Washington Post&lt;/a>, &lt;a href="http://nymag.com/daily/intelligencer/2018/02/americans-dont-understand-election-probabilities.html" target="_blank" rel="noopener">New York Magazine&lt;/a>, and the &lt;a href="https://politicalwire.com/2018/02/06/election-forecasts-lower-voter-turnout/" target="_blank" rel="noopener">Political Wire&lt;/a>. What&amp;rsquo;s more, FiveThirtyEight devoted much of their Feb. 12 &lt;a href="https://fivethirtyeight.com/features/politics-podcast-whats-so-wrong-with-nancy-pelosi/" target="_blank" rel="noopener">Politics Podcast&lt;/a> to a spirited, and at points critical discussion of our work. We are open to criticism and will respond to some of the questions raised in this post. Below, we’ll show that the evidence in our study and in other research is &lt;em>not&lt;/em> inconsistent with our headline, as the hosts suggest—we&amp;rsquo;ll detail the evidence that probabilistic forecasts &lt;em>confuse&lt;/em> people, irrespective of their technical accuracy. We’ll also discuss where we agree with the podcast hosts. Furthermore, we&amp;rsquo;ll discuss a few topics which, judging from the hosts discussion, may not have come through clearly enough in our paper. We&amp;rsquo;ll reiterate what this work contributes to social science—how the paper adds to our understanding of how people think about probabilistic forecasts and how they may decrease voting, particularly for the leading candidate&amp;rsquo;s supporters and among liberals in the U.S. We’ll then walk readers through the way we mapped vote share projections to probabilities in the study. Finally we’ll discuss why this work matters, and conclude by pointing out future research we’d like to see in this area.&lt;/p>
&lt;p>&lt;strong>What’s new here?&lt;/strong>&lt;/p>
&lt;p>The research contains a number findings that are new to social science:&lt;/p>
&lt;ul>
&lt;li>Presenting forecasted win-probabilities gives potential voters the impression that one candidate will win more decisively, compared with vote share projections (Study 1).&lt;/li>
&lt;li>Higher win probabilities, but &lt;em>not&lt;/em> vote share estimates, &lt;em>decrease voting&lt;/em> in the face of the trade-offs embedded in our election simulation (Study 2). This helps confirm the findings in Study 1 and adds to the evidence from past research that people vote at lower rates when they perceive an election to be uncompetitive.&lt;/li>
&lt;li>In 2016, probabilistic forecasts were covered more extensively than in the past and tended to be covered by outlets with more liberal audiences.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Where we agree&lt;/strong>&lt;/p>
&lt;p>If what you care about is conveying an accurate sense of whether one candidate will win, probabilistic forecasts do this slightly better than vote share. And, they seem to give people an edge on accuracy when interpreting the vote share if your candidate is behind. Of course, people can be confused and still end up being accurate, as we&amp;rsquo;ll discuss below.&lt;/p>
&lt;p>We also agree that people often do not accurately judge the likelihood of victory after seeing a vote share projection. That makes sense because, as the study shows, people appear to largely ignore the margin of error, which they’d need to map between vote share estimates and win probabilities.&lt;/p>
&lt;p>We also agree that a lot of past work shows that people stay home when they think an election isn’t close. What we’re adding to that body of work is evidence that compared with vote share projections, probabilistic forecasts give people the impression that one candidate will win more decisively, and may thus &lt;em>more powerfully&lt;/em> affect turnout.&lt;/p>
&lt;p>&lt;strong>Does the evidence in our study contradict our headline?&lt;/strong>&lt;/p>
&lt;p>Our headline isn’t about accuracy, it’s about &lt;em>confusion&lt;/em>. And the evidence from this research and past work taken as a whole suggests that probabilistic forecasts confuse people — something that came up at the end of segment — even if the result sometimes is technically higher accuracy.&lt;/p>
&lt;p>1. People in the study who saw only probabilistic forecasts were more likely to confuse probability and vote share. After seeing probabilistic forecasts, 8.6% of respondents mixed up vote share and probability, while only 0.6% of respondents did so after seeing vote share projections. We’re defining “mixed-up” as reporting the win-probability we provided as the vote share and vice-versa.&lt;/p>
&lt;p>2. Figure 2B (Study 1) shows that people get their candidate’s likelihood of winning very wrong, even when we explicitly told them the probability a candidate will win. It’s true that they got slightly closer with a probability forecast, but they are still far off.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/likelihood_loess.png" alt="normal" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Why might this be? A lot of past research and evidence suggests that people have trouble understanding probabilities, as noted at the end of the podcast. People have a tendency to think about &lt;a href="https://chicagounbound.uchicago.edu/cgi/viewcontent.cgi?article=1384&amp;amp;context=law_and_economics" target="_blank" rel="noopener">probabilities in subjective terms&lt;/a>, so they have trouble understanding &lt;a href="https://www.ncbi.nlm.nih.gov/pubmed/26161749" target="_blank" rel="noopener">medical risks&lt;/a> and even &lt;a href="http://pubman.mpdl.mpg.de/pubman/item/escidoc:2101059/component/escidoc:2101058/GG_30_Chance_2005.pdf" target="_blank" rel="noopener">weather forecasts&lt;/a>.&lt;/p>
&lt;p>Nate Silver has himself &lt;a href="https://fivethirtyeight.com/features/the-media-has-a-probability-problem/" target="_blank" rel="noopener">made the argument&lt;/a> that &lt;a href="https://www.nytimes.com/2016/11/10/technology/the-data-said-clinton-would-win-why-you-shouldnt-have-believed-it.html" target="_blank" rel="noopener">the backlash&lt;/a> we saw to data and analytics in the wake of the 2016 election is due in part to the media misunderstanding probabilistic forecasts.&lt;/p>
&lt;p>As the podcast hosts pointed out, people &lt;em>underestimated&lt;/em> the true likelihood of winning after seeing both probabilistic forecasts and vote share projections. It’s possible that people are skeptical of any probabilistic forecast in light of the 2016 election. It’s possible they interpreted the likelihood not as hard-nosed odds, but in rather subjective terms — &lt;em>what might happen&lt;/em>, &lt;a href="https://chicagounbound.uchicago.edu/cgi/viewcontent.cgi?article=1384&amp;amp;context=law_and_economics" target="_blank" rel="noopener">consistent with past research&lt;/a>. Regardless, they do not appear to reason about probability in a way that is consistent with how election forecasters define the probability of winning.&lt;/p>
&lt;p>3. Looking at how people reason about vote share — the way people have traditionally encountered polling data — it’s clear from our results that when a person’s candidate is ahead and they see a probabilistic forecast, they rather dramatically overestimate the vote share. On the other hand, when they are behind, they get closer to the right answer.&lt;/p>
&lt;p>But we know from past research that people have a “&lt;a href="http://journals.sagepub.com/doi/abs/10.1177/0956797609356421" target="_blank" rel="noopener">wishful&lt;/a> &lt;a href="http://www.tandfonline.com/doi/abs/10.1207/s15324834basp1304_6" target="_blank" rel="noopener">thinking&lt;/a>” &lt;a href="https://academic.oup.com/ijpor/article-abstract/9/2/105/713900" target="_blank" rel="noopener">bias&lt;/a>, meaning they say their candidate is doing better than polling data suggests. That’s why there’s a positive bias when people are evaluating how their candidate will do, according to Figure 2A (Study 1).&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/votea.png" alt="normal" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>The pattern in the data suggest that people are more accurate after seeing a probabilistic forecast for a losing candidate because of this effect, and &lt;em>not&lt;/em> necessarily because they better understand that candidate’s actual chances of victory.&lt;/p>
&lt;p>4. Perhaps even more importantly, &lt;em>none&lt;/em> of the results here changed when we excluded the margin of error from the projections we presented to people. That suggests that the public may not understand error in the same way that statisticians do, and therefore may not be well-equipped to understand what goes into changes in probabilistic forecast numbers. And of course, very small changes in vote share projection numbers and estimates of error correspond to much larger swings in probabilistic forecasts.&lt;/p>
&lt;p>5. Finally, as we point out in the paper, if probabilistic forecasters do not account for total error, they can really overestimate a candidate’s probability of winning. Of course, that’s because an estimate of the probability of victory bakes in estimates of error, which recent work has found is &lt;a href="http://www.stat.columbia.edu/~gelman/research/unpublished/polling-errors.pdf" target="_blank" rel="noopener">often about twice as large as the estimates of sampling error&lt;/a> provided in many polls.&lt;/p>
&lt;p>As &lt;a href="https://fivethirtyeight.com/features/the-polls-were-skewed-toward-democrats/" target="_blank" rel="noopener">Nate Silver has alluded to&lt;/a>, if the forecaster does not account unobserved error, including error that may be correlated across surveys — he/she will artificially inflate the estimated probability of victory or defeat. Of course, FiveThirtyEight &lt;em>does&lt;/em> attempt to account for this error, and released far more conservative forecasts than others in this space in 2016.&lt;/p>
&lt;p>Speaking in part to this issue, &lt;a href="http://www.stat.columbia.edu/~gelman/research/published/what_learned_in_2016_5.pdf" target="_blank" rel="noopener">Andrew Gelman and Julia Azari&lt;/a> recently concluded that “polling uncertainty could best be expressed not by speculative win probabilities but rather by using the traditional estimate and margin of error.” They seemed to be speaking about other forecasters, and did not directly reference FiveThirtyEight.&lt;/p>
&lt;p>At the end of the day, it’s easy to see that a vote share projection of 55% means that “55% of the votes will go to Candidate A, according to our polling data and assumptions.” However, it’s less clear that an 87% win probability means that “if the election were held 1000 times, Candidate A would win 870 times, and lose 130 times, based on our polling data and assumptions.”&lt;/p>
&lt;p>And most critically, we show that probabilistic forecasts showing more of a blowout could potentially lower voting. In Study 1, we provide limited evidence of this based on self reports. In Study 2, we show that when participants are faced with incentives designed to simulate real world voting, they are less likely to vote when probabilistic forecasts show higher odds of one candidate winning. Yet they are not responsive to changes in vote share.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/FT_18.01.03_prob_vote.png" alt="normal" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>&lt;strong>What’s with our mapping between vote share and probability?&lt;/strong>&lt;/p>
&lt;p>The podcast questions how a 55% vote share with a 2% margin of error is equivalent to an 87% win probability. This illustrates a common problem people have when trying to understand win probabilities — -it’s difficult to reason about the relationship between win-probabilities and vote share without actually running the numbers.&lt;/p>
&lt;p>You can express a projection as either (1) the average vote share (can be an electoral college vote share or the popular vote share)&lt;/p>
&lt;p>$$ \hat \mu_v = \frac{1}{N}\sum_{i}^{N}\bar x_i$$&lt;/p>
&lt;p>and margin of error&lt;/p>
&lt;p>$$\hat \mu_v \pm T^{0.975}_{df = N} \times \frac{ \hat \sigma_v}{\sqrt N}$$&lt;/p>
&lt;p>Here the average for each survey is $$\bar x_i$$, and there are $$N = 20$$ surveys.&lt;/p>
&lt;p>Or (2) the probability of winning — the probability that the vote share is greater than half, based on the observed vote share and standard error:&lt;/p>
&lt;p>$$1 - \Phi \left (\frac{.5 - \hat \mu_v}{\hat\sigma_v} \right )$$&lt;/p>
&lt;p>Going back to the example above, here’s the R code to generate those quantities:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-R" data-lang="R">&lt;span class="line">&lt;span class="cl">&lt;span class="n">svy_mean&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">.55&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">svy_SD&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">0.04483415&lt;/span> &lt;span class="c1"># see appendix &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">N_svy&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">20&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">margin_of_error&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">qt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">.975&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">df&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">N_svy&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">svy_SD&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="nf">sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N_svy&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">svy_mean&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">[1]&lt;/span> &lt;span class="m">0.55&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">margin_of_error&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">[1]&lt;/span> &lt;span class="m">0.02091224&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">prob_win&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="nf">pnorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">.50&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mean&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">svy_mean&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sd&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">svy_SD&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">prob_win&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">[1]&lt;/span> &lt;span class="m">0.8676222&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>More details about this approach are in our appendix. This is similar to how the Princeton Election Consortium generated win probabilities in 2016.&lt;/p>
&lt;p>Of course, one can also use an approach based on simulation, as FiveThirtyEight does. In the case of the data we generated for our hypothetical election in Study 1, this approach is not necessary. However we recognize that in the case of real-world presidential elections, a simulation approach has clear advantages by virtue of allowing more flexible statistical assumptions and a better accounting of error.&lt;/p>
&lt;p>&lt;strong>Why does this matter?&lt;/strong>&lt;/p>
&lt;p>To be clear, we are not analyzing real-world election returns. However, a lot of past &lt;a href="https://huber.research.yale.edu/materials/67_paper.pdf" target="_blank" rel="noopener">research&lt;/a> &lt;a href="http://www2.gsu.edu/~polsnn/priorbeliefs.pdf" target="_blank" rel="noopener">shows&lt;/a> that when people &lt;a href="https://www.jstor.org/stable/1953324?seq=1#page_scan_tab_contents" target="_blank" rel="noopener">think&lt;/a> an election is &lt;a href="https://repository.upenn.edu/cgi/viewcontent.cgi?referer=&amp;amp;httpsredir=1&amp;amp;article=1018&amp;amp;context=asc_papers" target="_blank" rel="noopener">in the bag&lt;/a>, they tend to &lt;a href="https://www.sciencedirect.com/science/article/pii/S0014292115000483" target="_blank" rel="noopener">vote in real-world elections&lt;/a> at &lt;a href="https://www.jstor.org/stable/2748722?seq=1#page_scan_tab_contents" target="_blank" rel="noopener">lower rates&lt;/a>. Our study provides evidence that probabilistic forecasts give people more confidence that one candidate will win and suggestive evidence that we should expect them to vote at lower rates after seeing probabilistic forecasts.&lt;/p>
&lt;p>This matters &lt;em>a lot more&lt;/em> if one candidate’s potential voters are differentially affected, and there’s evidence that may be the case.&lt;/p>
&lt;p>1. Figure 2C in Study 1 suggests that the &lt;em>candidate who is ahead&lt;/em> in the polls will be more affected by the increased certainty that probabilistic forecasts convey.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/certaintyc.png" alt="normal" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>2. When you look at the balance of coverage of probabilistic forecasts on major television broadcasts, there is more coverage on MSNBC, which has a more liberal audience.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/msnbc_mentions.png" alt="half" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>3. Consider who shares this material in social media&amp;ndash;specifically the average self-reported ideology of people who share links to various sites hosting poll-aggregators on Facebook, data that come from &lt;a href="http://science.sciencemag.org/content/early/2015/05/06/science.aaa1160.full" target="_blank" rel="noopener">this paper&lt;/a>’s &lt;a href="http://dx.doi.org/10.7910/DVN/LDJ7MS" target="_blank" rel="noopener">replication materials&lt;/a>. The websites that present their results in terms of probabilities have left-leaning (negative) social media audiences. Only realclearpolitics.com, which doesn’t emphasize win-probabilities, has a conservative audience:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/bma_science_alignment.png" alt="half" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>4. In 2016, the proportion of American National Election Study (ANES) respondents who thought the leading candidate would “win by quite a bit” was unusually high for Democrats&amp;hellip;&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/anes_turnout_closerace_mc_tall.png" alt="normal" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>5. And we know that people who say the leading presidential candidate will “win by quite a bit” in pre-election polling are about three percentage points less likely to report voting shortly after the election than people who say it’s a close race — and that’s after conditioning on election year, prior turnout, and party identification. The data here are from the ANES and go back to 1952.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/closerace_vote_anes.png" alt="normal" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>These data do not conclusively show that probabilistic forecasts affected turnout in the 2016 election, but they do raise questions about the real world consequences of probabilistic forecasts.&lt;/p>
&lt;p>&lt;strong>What about media narratives?&lt;/strong>&lt;/p>
&lt;p>We acknowledge that these effects may change depending on the context in which people encounter them — though people can certainly encounter a lone probability number in media coverage of probabilistic forecasts. We also acknowledge that our work cannot address how these effects compare to and/or interact with media narratives.&lt;/p>
&lt;p>However, other work that is relevant to this question has found that aggregating all polls &lt;a href="https://academic.oup.com/poq/article-abstract/80/4/943/2738970?redirectedFrom=fulltext" target="_blank" rel="noopener">reduces the likelihood that news outlets&lt;/a> focus on unusual polls that are more sensational or support a particular narrative.&lt;/p>
&lt;p>In some ways, the widespread success and reliance on these forecasts represents a triumph of scientific communication. In addition to greater precision compared with one-off horserace polls, probabilistic forecasts can quantify how likely a given U.S. presidential candidate is to win using polling data and complex simulation, rather than leaving the task of making sense of state and national polls to speculative commentary about “paths to victory,” as we point out in the &lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3117054" target="_blank" rel="noopener">paper&lt;/a>. And as one of the hosts noted, we aren’t calling for an end to election projections.&lt;/p>
&lt;p>&lt;strong>Future work&lt;/strong>&lt;/p>
&lt;p>We agree with the hosts that there are open questions about whether the public gives more weight to these probabilistic forecasts than other polling results and speculative commentary. We have also heard questions raised about how much probabilistic forecasts might &lt;em>drive&lt;/em> media narratives. These questions may prove difficult to answer and we encourage research that explores them.&lt;/p>
&lt;p>We hope this research continues to create a dialogue about how to best communicate polling data to the public. We would love to see more research into how the public consumes and is affected by election projections, including finding the most effective ways to convey uncertainty.&lt;/p></description></item><item><title>Know your data - Pricing diamonds using scatterplots and predictive models</title><link>https://solmessing.netlify.app/post/visualization-series-scatterplot-understanding-the-diamond-market/</link><pubDate>Sun, 02 Feb 2020 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/post/visualization-series-scatterplot-understanding-the-diamond-market/</guid><description>&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/fbfa61f67413c8e2805c507a14b38c24c5373265.png" alt="ggpairs" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>My last post railed against the &lt;a href="https://solmessing.netlify.app/post/visualization-series-insight-from-cleveland-and-tufte-on-plotting-numeric-data-by-groups/">bad visualizations that people often use to plot quantitive data by groups, and pitted pie charts, bar charts and dot plots against each other for two visualization tasks.  Dot plots came out on top&lt;/a>.  I argued that this is because humans are good at the cognitive task of comparing position along a common scale, compared to making judgements about length, area, shading, direction, angle, volume, curvature, etc.&amp;mdash;a finding credited to Cleveland and McGill.  I enjoyed writing it and people seemed to like it, so I&amp;rsquo;m continuing my visualization series with the scatterplot.&lt;/p>
&lt;h2 id="scatterplots">Scatterplots&lt;/h2>
&lt;p>A scatterplot is a two-dimensional plane on which we record the intersection of two measurements for a set of case items&amp;ndash;usually two quantitative variables.  Just as humans are good at comparing position along a common scale in one dimension, our visual capabilities allow us to make fast, accurate judgements and recognize patterns when presented with a series of dots in two dimensions. This makes the scatterplot a valuable tool for data analysts both when exploring data and when communicating results to others.&lt;/p>
&lt;p>In this post&amp;mdash;part 1&amp;mdash;I&amp;rsquo;ll demonstrate various uses for scatterplots and outline some strategies to help make sure key patterns are not obscured by the scale or qualitative group-level differences in the data (e.g., the relationship between test scores and income differs for men and women). The motivation in this post is to come up with a model of diamond prices that you can use to help make sure you don&amp;rsquo;t get ripped off, specified based on insight from exploratory scatterplots combined with (somewhat) informed speculation. In part 2, I&amp;rsquo;ll discuss the use of panels aka facets aka small multiples to shed additional light on key patterns in the data, and local regression (loess) to examine central tendencies in the data. There are far fewer bad examples of this kind of visualization in the wild than the 3D barplots and pie charts mocked in my last post, though I was still able to find &lt;a href="http://www.showmethemath.com/Concepts_Explained/Scatter_Plot/homeworkScatterPlotAnswer.gif" target="_blank" rel="noopener">this lovely scatterplot + trend-line&lt;/a>.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/0e5cd98eb90fbc27e55e776f3303057ef7a35dcb.gif" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h2 id="scatterplots-and-the-cartesian-coordinate-system">Scatterplots and the Cartesian coordinate system&lt;/h2>
&lt;p>The scatterplot has a richer history than the visualizations I wrote about in my last post.  The scatterplot&amp;rsquo;s face forms a two-dimensional Cartesian coordinate system, and DeCartes&amp;rsquo; invention/discovery of this eponymous plane in around 1657 represents one of the most fundamental developments in science.  The Cartesian plane unites measurement, algebra, and geometry, depicting the relationship between variables (or functions) visually. Prior to the Cartesian plane, mathematics was divided into algebra and geometry, and the unification of the two made many new developments possible.  Of course, this includes modern map-making&amp;mdash;cartography, but the &lt;a href="http://en.wikipedia.org/wiki/Cartesian_coordinate_system#History" target="_blank" rel="noopener">Cartesian plane was also an important step in the development of calculus&lt;/a>, without which very little of our modern would would be possible.&lt;/p>
&lt;p>The scatterplot is a powerful tool to help understand the relationship between variables, and especially if that relationship is non-linear. Say you want to get a sense of whether you&amp;rsquo;re paying the right price when shopping for a diamond. You can use data on the price and characteristics of many diamonds to help figure out whether the price advertised for any given diamond is reasonable, and you can use scatterplots to help figure out how to model that data in a sensible way. Consider the important relationship between the price of a diamond and its carat weight (which corresponds to its size):&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/635214c79e184de850272a4790deed0dc870a49a.png" alt="caratprice" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>A few things pop out right away.  We can see a non-linear relationship, and we can also see that the dispersion (variance) of the relationship also increases as carat size increases. With just a quick look at a scatterplot of the data, we&amp;rsquo;ve learned two important things about the functional relationship between price and carat size. And, we also therefore learned that running a linear model on this data as-is would be a bad idea.&lt;/p>
&lt;h2 id="diamonds">Diamonds&lt;/h2>
&lt;p>&lt;a name="Diamonds">&lt;/a>&lt;/p>
&lt;p>If you&amp;rsquo;ve ever used R, you&amp;rsquo;ve probably seen references to the diamonds data set that ships with Hadley Wickham&amp;rsquo;s ggplot2. It records the carat size and the price of more than 50 thousand diamonds, from &lt;a href="http://www.diamondse.info/" target="_blank" rel="noopener">http://www.diamondse.info/&lt;/a> collected in &lt;a href="http://r.789695.n4.nabble.com/Year-of-data-collection-for-diamonds-dataset-in-ggplot2-td4506598.html" target="_blank" rel="noopener">in 2008&lt;/a>, and if you&amp;rsquo;re in the market for a diamond, exploring this data set can help you understand what&amp;rsquo;s in store and at what price point. This is particularly useful because each diamond is unique in a way that isn&amp;rsquo;t true of most manufactured products we are used to buying&amp;mdash;you can&amp;rsquo;t just plug a model number and look up the price on Amazon. And even an expert cannot cannot incorporate as much information about price as a picture of the entire market informed by data (though there&amp;rsquo;s no substitute for qualitative expertise to make sure your diamond is what the retailer claims).&lt;/p>
&lt;p>But even if you&amp;rsquo;re not looking to buy a diamond, the socioeconomic and political history of the diamond industry is fascinating. Diamonds birthed the mining industry in South Africa, which is now by far the largest and most advanced economy in Africa.  I worked a summer in Johannesburg, and can assure you that South Africa&amp;rsquo;s cities look far more like L.A. and San Francisco than Lagos, Cairo, Mogadishu, Nairobi, or Rabat.  Diamonds drove the British and Dutch to colonize southern Africa in the first place, and have stoked conflicts ranging from the Boer Wars to modern day wars in Sierra Leone, Liberia, Côte d&amp;rsquo;Ivoire, Zimbabwe and the DRC, where the 200 carat Millennium Star diamond was sold to DeBeers at the height of the civil war in the 1990s.  Diamonds were one of the few assets that Jews could conceal from the Nazis during &lt;a href="http://www.archives.gov/research/holocaust/articles-and-papers/turning-history-into-justice.html" target="_blank" rel="noopener">the &amp;ldquo;Aryanization of Jewish property&amp;rdquo;&lt;/a> in the 1930s, and the Congressional Research Service reports that &lt;a href="http://royce.house.gov/uploadedfiles/rl30751.pdf" target="_blank" rel="noopener">Al Qaeda has used conflict diamonds to skirt international sanctions and finance operations from the 1998 East Africa Bombings to the September 11th attacks&lt;/a>.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/c7c2fd41f3cf8c8b7423ab84c12dfbd14fed71ca.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Though the diamonds data set is full of prices and fairly esoteric certification ratings, hidden in the data are reflections of how a &lt;a href="http://www.nytimes.com/2013/05/05/fashion/weddings/how-americans-learned-to-love-diamonds.html" target="_blank" rel="noopener">legendary marketing campaign permeated and was subsumed by our culture&lt;/a>, hints about how different social strata responded, and insight into how the diamond market functions as a result.&lt;/p>
&lt;p>&lt;a href="http://www.theatlantic.com/magazine/archive/1982/02/have-you-ever-tried-to-sell-a-diamond/304575/" target="_blank" rel="noopener">The story starts in 1870&lt;/a> according to The Atlantic, when many tons of diamonds were discovered in South Africa near the Orange River.  Until then, diamonds were rare&amp;mdash;only a few pounds were mined from India and Brazil each year.  At the time diamonds had no use outside of jewelry as they do today in many industrial applications, so price depended only on scarce supply.  Hence, the project&amp;rsquo;s investors formed the De Beers Cartel in 1888 to control the global price&amp;mdash;by most accounts the most successful cartel in history, &lt;a href="http://en.wikipedia.org/wiki/De_Beers#Diamond_monopoly" target="_blank" rel="noopener">controlling 90% of the world&amp;rsquo;s diamond supply until about 2000&lt;/a>.  But World War I and the Great Depression saw diamond sales plummet.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/0346d018cd97813448c1ce59a20353ed7c900013.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>In 1938, according to the New York Times&amp;rsquo; account, the De Beers cartel wrote Philadelphia ad agency N. W. Ayer &amp;amp; Son, to investigate whether &amp;ldquo;the use of propaganda in various forms&amp;rdquo; might jump-start diamond sales in the U.S., which looked like the only potentially viable market at the time.  Surveys showed diamonds were low on the list of priorities among most couples contemplating marriage&amp;mdash;a luxury for the rich, &amp;ldquo;money down the drain.&amp;rdquo;  Frances Gerety, who the Times compares to Madmen&amp;rsquo;s Peggy Olson, took on the DeBeers&amp;rsquo; account at N.W. Ayer &amp;amp; Son, and worked toward the company&amp;rsquo;s goal &amp;ldquo;to create a situation where almost every person pledging marriage feels compelled to acquire a diamond engagement ring.&amp;rdquo;  A few years later, she coined the slogan, &amp;ldquo;Diamonds are forever.&amp;rdquo;&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/90a8efdb27b9c59fbf6566591c14165beab4bfd6.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>The Atlantic&amp;rsquo;s Jay Epstein argues that this campaign gave birth to modern demand-advertising&amp;mdash;the objective was not direct sales, nor brand strengthening, but simply to impress the glamour, sentiment and emotional charge contained in the product itself.  The company gave diamonds to movie stars, sent out press packages emphasizing the size of diamonds celebrities gave each other, loaned diamonds to socialites attending prominent events like the Academy Awards and Kentucky Derby, and persuaded the British royal family to wear diamonds over other gems.  The diamond was also marketed as a status symbol, to reflect &amp;ldquo;a man&amp;rsquo;s &amp;hellip; success in life,&amp;rdquo; in ads with &amp;ldquo;the aroma of tweed, old leather and polished wood which is characteristic of a good club.&amp;rdquo;  A 1980s ad introduced the two-month benchmark: &amp;ldquo;Isn&amp;rsquo;t two months&amp;rsquo; salary a small price to pay for something that lasts forever?&amp;rdquo;&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/4f1b0bd62c2ab1b46e453b707d2981d84879d4ed.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>By any reasonable measure, Frances Gerety succeeded&amp;mdash;getting engaged means getting a diamond ring in America. Can you think of a movie where two people get engaged without a diamond ring? When you announce your engagement on Facebook, what icon does the site display?  Still think this marketing campaign might not be the most successful mass-persuasion effort in history?  I present to you a James Bond film, whose title bears the diamond cartel&amp;rsquo;s trademark:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/f4eda42fe398a38837e01e97e4d07606a20171fe.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Awe-inspiring and terrifying.  Let&amp;rsquo;s open the data set.  &lt;/p>
&lt;p>The first thing you should consider doing is plotting key variables against each other using the ggpairs() function.  This function plots every variable against every other, pairwise.  For a data set with as many rows as the diamonds data, you may want to sample first otherwise things will take a long time to render.  Also, if your data set has more than about ten columns, there will be too many plotting windows, so subset on columns first.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Uncomment these lines and install if necessary:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#install.packages(&amp;#39;GGally&amp;#39;)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#install.packages(&amp;#39;ggplot2&amp;#39;)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#install.packages(&amp;#39;scales&amp;#39;)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#install.packages(&amp;#39;memisc&amp;#39;)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ggplot2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">GGally&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">scales&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">data&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">diamonds&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">diasamp&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">diamonds&lt;/span>&lt;span class="nf">[sample&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="o">:&lt;/span>&lt;span class="nf">length&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">diamonds&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">price&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="m">10000&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="n">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">ggpairs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">diasamp&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">params&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">shape&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">I&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;.&amp;#39;&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">outlier.shape&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">I&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;.&amp;#39;&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!-- * R style note: I started using the "=" operator over "&lt;-" after reading [John Mount's post on the topic](http://www.win-vector.com/blog/2013/04/prefer-for-assignment-in-r/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=prefer-for-assignment-in-r), which shows how using "&lt;-" (but not "=") incorrectly can result in silent errors.  There are other good reasons: 1.) WordPress and R-Bloggers occasionally mangle "&lt;-" thinking it is HTML code in ways unpredictable to me; 2.) "=" is what every other programming language uses; and 3.) (as pointed out by Alex Foss in comments) consider "foo&lt;-3" --- did the author mean to assign foo to 3 or to compare foo to -3?  Plus, 4.) the way R interprets that expression depends on white space---and if I'm using an editor like Emacs or Sublime where I don't have a shortcut key assigned to "&lt;-", I sometimes get the whitespace wrong.  This means spending extra time and brainpower on debugging, both of which are in short supply.  
-->
&lt;p>Anyway, here&amp;rsquo;s the plot:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/5226c40443deaf7c082cd464531f4e27c0f151be.png" alt="ggpairs" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>What&amp;rsquo;s happening is that ggpairs is plotting each variable against the other in a pretty smart way. In the lower-triangle of plot matrix, it uses grouped histograms for qualitative-qualitative pairs and scatterplots for quantitative-quantitative pairs.  In the upper-triangle, it plots grouped histograms for qualitative-qualitative pairs (using the x-instead of y-variable as the grouping factor), boxplots for qualitative-quantitative pairs, and provides the correlation for quantitative-quantitative pairs.
What we really care about here is price, so let&amp;rsquo;s focus on that.  We can see what might be relationships between price and clarity, and color, which we&amp;rsquo;ll keep in mind for later when we start modeling our data, but the critical factor driving price is the size/weight of a diamond. Yet as we saw above, the relationship between price and diamond size is non-linear. What might explain this pattern?  On the supply side, larger contiguous chunks of diamonds without significant flaws are probably much harder to find than smaller ones.  This may help explain the exponential-looking curve&amp;mdash;and I thought I noticed this when I was shopping for a diamond for my soon-to-be wife. Of course, this is related to the fact that the weight of a diamond is a function of volume, and volume is a function of x * y * z, suggesting that we might be especially interested in the cubed-root of carat weight.&lt;/p>
&lt;p>On the demand side, customers in the market for a less expensive, smaller diamond are probably more sensitive to price than more well-to-do buyers. Many less-than-one-carat customers would surely never buy a diamond were it not for the social norm of presenting one when proposing.  And, there are &lt;em>fewer&lt;/em> consumers who can afford a diamond larger than one carat.  Hence, we shouldn&amp;rsquo;t expect the market for bigger diamonds to be as competitive as that for smaller ones, so it makes sense that the variance as well as the price would increase with carat size.&lt;/p>
&lt;p>Often the distribution of any monetary variable will be highly skewed and vary over orders of magnitude. This can result from path-dependence (e.g., the rich get richer) and/or the multiplicitive processes (e.g., year on year inflation) that produce the ultimate price/dollar amount. Hence, it&amp;rsquo;s a good idea to look into compressing any such variable by putting it on a log scale (for more take a look at &lt;a href="http://www.r-statistics.com/2013/05/log-transformations-for-skewed-and-wide-distributions-from-practical-data-science-with-r/" target="_blank" rel="noopener">this guest post on Tal Galili&amp;rsquo;s blog&lt;/a>).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">qplot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">price&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">data&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">diamonds&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">binwidth&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">100&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">theme_bw&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">ggtitle&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;Price&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">qplot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">price&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">data&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">diamonds&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">binwidth&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">0.01&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">scale_x_log10&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">theme_bw&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">ggtitle&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;Price (log10)&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/0cc713fc225f620a8284b05a00900ec51e379221.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/cdd7c96b96c0c2deef8844c214703a2243a3070b.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Indeed, we can see that the prices for diamonds are heavily skewed, but when put on a log10 scale seem much better behaved (i.e., closer to the bell curve of a normal distribution).  In fact, we can see that the data show some evidence of bimodality on the log10 scale, consistent with our two-class, &amp;ldquo;rich-buyer, poor-buyer&amp;rdquo; speculation about the nature of customers for diamonds.
Let&amp;rsquo;s re-plot our data, but now let&amp;rsquo;s put price on a log10 scale:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">qplot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">carat&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">price&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">data&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">diamonds&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">scale_y_continuous&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">trans&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nf">log10_trans&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">theme_bw&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">ggtitle&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;Price (log10) by Carat&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/f7fcbbe09988fd9cddc474aecbbd918c448b7575.png" alt="caratpricelog10" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Better, though still a little funky&amp;mdash;let&amp;rsquo;s try using use the cube-root of carat as we speculated about above:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">cubroot_trans&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">function&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="nf">trans_new&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;cubroot&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">transform&lt;/span>&lt;span class="o">=&lt;/span> &lt;span class="nf">function&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="nf">^&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="m">3&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">inverse&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">function&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">x^3&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">qplot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">carat&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">price&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">data&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">diamonds&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">scale_x_continuous&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">trans&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nf">cubroot_trans&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">limits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">0.2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">3&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">breaks&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">0.2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">0.5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">scale_y_continuous&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">trans&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nf">log10_trans&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">limits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">350&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">15000&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">breaks&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">350&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">1000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">5000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">10000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">15000&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">theme_bw&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">ggtitle&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;Price (log10) by Cubed-Root of Carat&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/271f4d38318b2eefdbe9c5ca6ed34a467e1dab66.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Nice, looks like an almost-linear relationship after applying the transformations above to get our variables on a nice scale.&lt;/p>
&lt;h2 id="overplotting">Overplotting&lt;/h2>
&lt;hr>
&lt;p>Note that until now I haven&amp;rsquo;t done anything about overplotting&amp;mdash;where multiple points take on the same value, often due to rounding.  Indeed, price is rounded to dollars and carats are rounded to two digits.  Not bad, though when we&amp;rsquo;ve got this much data we&amp;rsquo;re going to have some serious overplotting.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">head&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">sort&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">table&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">diamonds&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">carat&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">decreasing&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">TRUE&lt;/span> &lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">head&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">sort&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">table&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">diamonds&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">price&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">decreasing&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">TRUE&lt;/span> &lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code> 0.3 0.31 1.01 0.7 0.32 1
2604 2249 2242 1981 1840 1558
605 802 625 828 776 698
132 127 126 125 124 121
&lt;/code>&lt;/pre>
&lt;p>Often you can deal with this by making your points smaller, using &amp;ldquo;jittering&amp;rdquo; to randomly shift points to make multiple points visible, and using transparency, which can be done in ggplot using the &amp;ldquo;alpha&amp;rdquo; parameter.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">ggplot&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="n">data&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">diamonds&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">aes&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">carat&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">price&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">geom_point&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">alpha&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">0.5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">.75&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#39;jitter&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">scale_x_continuous&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">trans&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nf">cubroot_trans&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">limits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">0.2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">3&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">breaks&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">0.2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">0.5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">scale_y_continuous&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">trans&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nf">log10_trans&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">limits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">350&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">15000&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">breaks&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">350&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">1000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">5000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">10000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">15000&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">theme_bw&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">ggtitle&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;Price (log10) by Cubed-Root of Carat&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/8f93ee4c395a7382a2893b01d640b65e7006ebe9.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>This gives us a better sense of how dense and sparse our data is at key places.&lt;/p>
&lt;h2 id="using-color-to-understand-qualitative-factors">Using Color to Understand Qualitative Factors&lt;/h2>
&lt;hr>
&lt;p>When I was looking around at diamonds, I also noticed that clarity seemed to factor in to price.  Of course, many consumers are looking for a diamond of a certain size, so we shouldn&amp;rsquo;t expect clarity to be as strong a factor as carat weight. And I must admit that even though my grandparents were jewelers, I initially had a hard time discerning a diamond rated VVS1 from one rated SI2. Surely most people need a loop to tell the difference. And, &lt;a href="http://www.bluenile.com/diamonds/diamond-cut" target="_blank" rel="noopener">according to BlueNile, the cut of a diamond has a much more consequential impact on that &amp;ldquo;fiery&amp;rdquo; quality that jewelers describe as the quintessential characteristic of a diamond&lt;/a>.  On clarity, the website states, &amp;ldquo;&lt;a href="http://www.bluenile.com/diamonds/diamond-clarity" target="_blank" rel="noopener">Many of these imperfections are microscopic, and do not affect a diamond&amp;rsquo;s beauty in any discernible way&lt;/a>.&amp;rdquo; Yet, clarity seems to explain an awful lot of the remaining variance in price when we visualize it as a color on our plot:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">ggplot&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="n">data&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">diamonds&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">aes&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">carat&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">price&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">colour&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">clarity&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">geom_point&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">alpha&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">0.5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">.75&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#39;jitter&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">scale_colour_brewer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">type&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#39;div&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">guide&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">guide_legend&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">title&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">NULL&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">reverse&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">T&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">override.aes&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">alpha&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="p">)))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">scale_x_continuous&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">trans&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nf">cubroot_trans&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">limits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">0.2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">3&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">breaks&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">0.2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">0.5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">scale_y_continuous&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">trans&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nf">log10_trans&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">limits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">350&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">15000&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">breaks&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">350&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">1000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">5000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">10000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">15000&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">theme_bw&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nf">theme&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">legend.key&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">element_blank&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">ggtitle&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;Price (log10) by Cubed-Root of Carat and Color&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/8ec0d95374a095c4268c9cfad923354f819bbdfb.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;hr>
&lt;p>Despite what BlueNile says, we don&amp;rsquo;t see as much variation on cut (though most diamonds in this data set are ideal cut anyway):&lt;/p>
&lt;hr>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">ggplot&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="n">data&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">diamonds&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">aes&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">carat&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">price&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">colour&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">cut&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">geom_point&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">alpha&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">0.5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">.75&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#39;jitter&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">scale_colour_brewer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">type&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#39;div&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">guide&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">guide_legend&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">title&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">NULL&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">reverse&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">T&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">override.aes&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">alpha&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="p">)))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">scale_x_continuous&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">trans&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nf">cubroot_trans&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">limits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">0.2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">3&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">breaks&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">0.2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">0.5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">scale_y_continuous&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">trans&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nf">log10_trans&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">limits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">350&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">15000&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">breaks&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">350&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">1000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">5000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">10000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">15000&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">theme_bw&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nf">theme&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">legend.key&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">element_blank&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">ggtitle&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;Price (log10) by Cube-Root of Carat and Cut&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/1ef4c90edca2b6e2014d25f2a901a221f9926f6e.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Color seems to explain some of the variance in price as well, though &lt;a href="http://www.bluenile.com/diamonds/diamond-color" target="_blank" rel="noopener">BlueNile states that all color grades from D-J are basically not noticeable&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">ggplot&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="n">data&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">diamonds&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">aes&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">carat&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">price&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">colour&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">color&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">geom_point&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">alpha&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">0.5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">.75&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#39;jitter&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">scale_colour_brewer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">type&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#39;div&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">guide&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">guide_legend&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">title&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">NULL&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">reverse&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">T&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">override.aes&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">alpha&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="p">)))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">scale_x_continuous&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">trans&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nf">cubroot_trans&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">limits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">0.2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">3&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">breaks&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">0.2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">0.5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">scale_y_continuous&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">trans&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nf">log10_trans&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">limits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">350&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">15000&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">breaks&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">350&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">1000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">5000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">10000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">15000&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">theme_bw&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nf">theme&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">legend.key&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">element_blank&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">ggtitle&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;Price (log10) by Cube-Root of Carat and Color&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/277ef617f6ebe3749e64a855be635406ea80dba7.png" alt="caratpricecolorlog10" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>At this point, we&amp;rsquo;ve got a pretty good idea of how we might model price. But there are a few problems with our 2008 data&amp;mdash;not only do we need to account for inflation but the diamond market is quite different now than it was in 2008. In fact, when I fit models to this data then attempted to predict the price of diamonds I found on the market, I kept getting predictions that were far too low. After some additional digging, I found the &lt;a href="http://www.bain.com/publications/articles/global-diamond-report-2013.aspx" target="_blank" rel="noopener">Global Diamond Report&lt;/a>. It turns out that prices plummeted in 2008 due to the global financial crisis, and since then prices (at least for wholesale polished diamond) have grown at a roughly a 6 percent compound annual rate. The &lt;a href="http://diamonds.blogs.com/diamonds_update/diamond-prices/" target="_blank" rel="noopener">rapidly-growing number of couples in China buying diamond engagement rings&lt;/a> might also help explain this increase. After looking at data on PriceScope, I realized that &lt;a href="http://www.pricescope.com/diamond-prices/diamond-prices-chart" target="_blank" rel="noopener">diamond prices grew unevenly across different carat sizes&lt;/a>, meaning that the model I initially estimated couldn&amp;rsquo;t simply be adjusted by inflation. While I could have done ok with that model, I really wanted to estimate a new model based on fresh data.&lt;/p>
&lt;p>Thankfully I was able to put together a &lt;a href="https://github.com/solomonm/diamonds-data/blob/master/dinfo.py" target="_blank" rel="noopener">python script to scrape diamondse.info&lt;/a> without too much trouble. This dataset is about 10 times the size of the 2008 diamonds data set and features diamonds from all over the world certified by an array of authorities besides just the Gemological Institute of America (GIA). You can read in this data as follows (be forewarned&amp;mdash;it&amp;rsquo;s over 500K rows):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#install.packages(&amp;#39;RCurl&amp;#39;)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;RCurl&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">diamondsurl&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">getBinaryURL&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;https://raw.github.com/solomonm/diamonds-data/master/BigDiamonds.Rda&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">load&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">rawConnection&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">diamondsurl&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>My &lt;a href="https://github.com/solomonm/diamonds-data" target="_blank" rel="noopener">github repository has the code necessary to replicate each of the figures above&lt;/a>&amp;mdash;most look quite similar, though this data set contains much more expensive diamonds than the original. Regardless of whether you&amp;rsquo;re using the original diamonds data set or the current larger diamonds data set, you can estimate a model based on what we learned from our scatterplots. We&amp;rsquo;ll regress carat, the cubed-root of carat, clarity, cut and color on log-price. I&amp;rsquo;m using only GIA-certified diamonds in this model and looking only at diamonds under $10K because these are the type of diamonds sold at most retailers I&amp;rsquo;ve seen and hence the kind I care most about. By trimming the most expensive diamonds from the dataset, our model will also be less likely to be thrown off by outliers at the high end of price and carat. The new data set has mostly the same columns as the old one, so we can just run the following (if you want to run it on the old data set, just set data=diamonds).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">diamondsbig&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">logprice&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">diamondsbig&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">price&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">m1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">lm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">logprice&lt;/span>&lt;span class="o">~&lt;/span> &lt;span class="nf">I&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">carat&lt;/span>&lt;span class="nf">^&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="m">3&lt;/span>&lt;span class="p">)),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">data&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">diamondsbig[diamondsbig&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">price&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="m">10000&lt;/span> &lt;span class="o">&amp;amp;&lt;/span> &lt;span class="n">diamondsbig&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">cert&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s">&amp;#39;GIA&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">]&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">m2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">update&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">. &lt;/span>&lt;span class="o">+&lt;/span> &lt;span class="n">carat&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">m3&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">update&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">. &lt;/span>&lt;span class="o">+&lt;/span> &lt;span class="n">cut&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">m4&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">update&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">. &lt;/span>&lt;span class="o">+&lt;/span> &lt;span class="n">color&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">clarity&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#install.packages(&amp;#39;memisc&amp;#39;)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">memisc&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">mtable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">m2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">m3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">m4&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here are the results for my recently scraped data set:&lt;/p>
&lt;pre>&lt;code>===============================================================
m1 m2 m3 m4
---------------------------------------------------------------
(Intercept) 2.671*** 1.333*** 0.949*** -0.464***
(0.003) (0.012) (0.012) (0.009)
I(carat^(1/3)) 5.839*** 8.243*** 8.633*** 8.320***
(0.004) (0.022) (0.021) (0.012)
carat -1.061*** -1.223*** -0.763***
(0.009) (0.009) (0.005)
cut: V.Good 0.120*** 0.071***
(0.002) (0.001)
cut: Ideal 0.211*** 0.131***
(0.002) (0.001)
color: K/L 0.117***
(0.003)
color: J/L 0.318***
(0.002)
color: I/L 0.469***
(0.002)
color: H/L 0.602***
(0.002)
color: G/L 0.665***
(0.002)
color: F/L 0.723***
(0.002)
color: E/L 0.756***
(0.002)
color: D/L 0.827***
(0.002)
clarity: I1 0.301***
(0.006)
clarity: SI2 0.607***
(0.006)
clarity: SI1 0.727***
(0.006)
clarity: VS2 0.836***
(0.006)
clarity: VS1 0.891***
(0.006)
clarity: VVS2 0.935***
(0.006)
clarity: VVS1 0.995***
(0.006)
clarity: IF 1.052***
(0.006)
---------------------------------------------------------------
R-squared 0.888 0.892 0.899 0.969
N 338946 338946 338946 338946
===============================================================
&lt;/code>&lt;/pre>
&lt;p>Now those are some very nice R-squared values&amp;mdash;we are accounting for almost all of the variance in price with the 4Cs.  If we want to know what whether the price for a diamond is reasonable, we can now use this model and exponentiate the result (since we took the log of price).  We need to multiply the result by exp(sigma^2/2), because the our error is no longer zero in expectation:&lt;/p>
&lt;p>$$
\begin{align*}
E(log(y) \mid \mathbf{X} = \mathbf{x}) &amp;amp;= E(\mathbf{X}\beta + \epsilon)\
E(y \mid \mathbf{X} = \mathbf{x}) &amp;amp;= E( exp( \mathbf{X}\beta + \epsilon ) )\
&amp;amp;= E( exp( \mathbf{X}\beta ) \times exp( \epsilon ) ) \
&amp;amp;= E( exp( \mathbf{X}\beta ) ) \times E( exp( \epsilon ) ) \
&amp;amp;= exp(\mathbf{X}\hat\beta) \times exp( \frac{\hat\sigma^2}{2} )
\end{align*}
$$&lt;/p>
&lt;p>To dig further into that last step, have a look at the &lt;a href="http://en.wikipedia.org/wiki/Log-normal_distribution#Arithmetic_moments" target="_blank" rel="noopener">Wikipedia page on log-normal distributed variables&lt;/a>.
Thanks to &lt;a href="https://sites.google.com/site/miguelgodinhomatos/" target="_blank" rel="noopener">Miguel&lt;/a> for catching this.
Let&amp;rsquo;s take a look at an example from Blue Nile. I&amp;rsquo;ll use the full model, m4.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Example from BlueNile&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Round 1.00 Very Good I VS1 $5,601&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">thisDiamond&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">data.frame&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">carat&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">1.00&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cut&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#39;V.Good&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">color&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#39;I&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">clarity&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#39;VS1&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">modEst&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">predict&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">newdata&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">thisDiamond&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">interval&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#39;prediction&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">level&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">.95&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">modEst&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="nf">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">summary&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m4&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">sigma^2&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The results yield an expected value for price given the characteristics of our diamond and the upper and lower bounds of a 95% CI&amp;mdash;note that because this is a linear model, predict() is just multiplying each model coefficient by each value in our data. Turns out that this diamond is a touch pricier than expected value under the full model, though it is by no means outside our 95% CI. BlueNile has by most accounts a better reputation than diamondse.info however, and reputation is worth a lot in a business that relies on easy-to-forge certificates and one in which the non-expert can be easily fooled.&lt;/p>
&lt;p>This illustrates an important point about generalizing a model from one data set to another. First, there may be important differences between data sets&amp;mdash;as I&amp;rsquo;ve speculated about above&amp;mdash;making the estimates systematically biased. Second, overfitting&amp;mdash;our model may be fitting noise present in data set. Even a model cross-validated against out-of-sample predictions can be over-fit to noise that results in differences between data sets. Of course, while this model may give you a sense of whether your diamond is a rip-off against diamondse.info diamonds, it&amp;rsquo;s not clear that diamondse.info should be regarded as a source of universal truth about whether the price of a diamond is reasonable. Nonetheless, to have the expected price at diamondse.info with a 95% interval is a lot more information than we had about the price we should be willing to pay for a diamond before we started this exercise.&lt;/p>
&lt;p>An important point&amp;mdash;even though we can predict diamondse.info prices almost perfectly based on a function of the 4c&amp;rsquo;s, one thing that you should NOT conclude from this exercise is that &lt;em>where&lt;/em> you buy your diamond is irrelevant, which apparently used to be conventional wisdom in some circles.  You will almost surely pay more if you &lt;a href="http://www.businessweek.com/articles/2013-05-06/tiffany-vs-dot-costco-which-diamond-ring-is-better" target="_blank" rel="noopener">buy the same diamond at Tiffany&amp;rsquo;s versus Costco&lt;/a>.  But &lt;a href="https://web.archive.org/web/20140217105722/http://www.costco.com:80/2.12-ctw-Round-Brilliant-Cut-Internally-Flawless,-D-Color-Diamond-%22Audrey%22-Platinum-Wedding-Set.product.100006730.html" target="_blank" rel="noopener">Costco sells some pricy diamonds&lt;/a> as well. Regardless, you can use this kind of model to give you an indication of whether you&amp;rsquo;re overpaying.&lt;/p>
&lt;p>Of course, the value of a natural diamond is largely socially constructed. Like money, diamonds are only valuable because society says they are&amp;mdash;-there&amp;rsquo;s no obvious economic efficiencies to be gained or return on investment in a diamond, except perhaps in a very subjective sense concerning your relationship with your significant other. To get a sense for just how much value is socially constructed, you can compare the price of a natural diamond to a synthetic diamond, which thanks to recent technological developments are of comparable quality to a &amp;ldquo;natural&amp;rdquo; diamond. Of course, natural diamonds fetch a dramatically higher price.&lt;/p>
&lt;p>One last thing&amp;mdash;there are few guarantees in life, and I offer none here. Though what we have here seems pretty good, data and models are never infallible, and obviously you can still get taken (or be persuaded to pass on a great deal) based on this model. Always shop with a reputable dealer, and make sure her incentives are aligned against selling you an overpriced diamond or worse one that doesn&amp;rsquo;t match its certificate. There&amp;rsquo;s no substitute for establishing a personal connection and lasting business relationship with an established jeweler you can trust.&lt;/p>
&lt;h2 id="one-final-consideration">One Final Consideration&lt;/h2>
&lt;hr>
&lt;p>Plotting your data can help you understand it and can yield key insights.  But even scatterplot visualizations can be deceptive if you&amp;rsquo;re not careful.  Consider another data set the comes with the alr3 package&amp;mdash;soil temperature data from Mitchell, Nebraska, collected by Kenneth G. Hubbard from 1976-1992, which I came across in Weisberg, S. (2005). &lt;em>Applied Linear Regression&lt;/em>, 3rd edition. New York: Wiley (from which I&amp;rsquo;ve shamelessly stolen this example).
Let&amp;rsquo;s plot the data, naively:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#install.packages(&amp;#39;alr3&amp;#39;)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">alr3&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">data&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Mitchell&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">qplot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Month&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Temp&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Mitchell&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nf">theme_bw&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/f02902f327e42be253eb2e245e5876de3395060b.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Looks kinda like noise.  What&amp;rsquo;s the story here?
When all else fails, think about it.
What&amp;rsquo;s on the X axis?  Month.  What&amp;rsquo;s on the Y-axis?  Temperature.  Hmm, well there are seasons in Nebraska, so temperature should fluctuate every 12 months.&lt;/p>
&lt;p>But we&amp;rsquo;ve put more than 200 months in a pretty tight space.&lt;/p>
&lt;p>Let&amp;rsquo;s stretch it out and see how it looks:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/11fe76cda575e832918edc0bccf89bd911dc04ee.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Don&amp;rsquo;t make that mistake.&lt;/p>
&lt;p>That concludes part I of this series on scatterplots.  Part II will illustrate the advantages of using facets/panels/small multiples, and show how tools to fit trendlines including linear regression and local regression (loess) can help yield additional insight about your data.&lt;/p>
&lt;p>You can also learn more about &lt;a href="https://www.udacity.com/course/ud651" target="_blank" rel="noopener">exploratory data analysis via this Udacity course taught by my colleagues Dean Eckles and Moira Burke, and Chris Saden&lt;/a>, which will be coming out in the next few weeks.&lt;/p></description></item><item><title>Facebook Privacy-Protected Full URLs Data Set</title><link>https://solmessing.netlify.app/publication/messing-2020/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/messing-2020/</guid><description>&lt;ul>
&lt;li>Largest ever social science data set released, protected using &lt;a href="https://arxiv.org/abs/2002.04049" target="_blank" rel="noopener">action-level differential privacy&lt;/a>.&lt;/li>
&lt;li>See also &lt;a href="socialscience.one/blog/update-social-science-one">Social Science One&amp;rsquo;s annoucement&lt;/a>.&lt;/li>
&lt;li>Media coverage: &lt;a href="https://www.sciencemag.org/news/2020/02/researchers-finally-get-access-data-facebook-s-role-political-discourse" target="_blank" rel="noopener">Science&lt;/a>, &lt;a href="https://www.nature.com/articles/d41586-019-02966-x" target="_blank" rel="noopener">Nature&lt;/a>, &lt;a href="https://www.poynter.org/fact-checking/2019/what-can-researchers-find-among-the-32-million-urls-facebook-just-released-to-social-science-one/" target="_blank" rel="noopener">Poynter&lt;/a>, &lt;a href="https://www.wired.com/story/facebook-social-network-becomes-social-science-subject/" target="_blank" rel="noopener">Wired&lt;/a>, &lt;a href="https://www.ft.com/content/6133b90e-e23f-11e9-9743-db5a370481bc" target="_blank" rel="noopener">Financial Times&lt;/a>, &lt;a href="https://techcrunch.com/2018/07/11/facebook-independent-research-commission-social-science-one-will-share-a-petabyte-of-user-data/" target="_blank" rel="noopener">Tech Crunch&lt;/a>,
&lt;a href="https://www.theverge.com/2018/7/11/17561144/facebook-election-research-database-link-proposal" target="_blank" rel="noopener">The Verge&lt;/a>.&lt;/li>
&lt;/ul></description></item><item><title>Guidelines for Implementing and Auditing Differentially Private Systems</title><link>https://solmessing.netlify.app/publication/kifer-2020-guidelines/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/kifer-2020-guidelines/</guid><description/></item><item><title>Display Jupyter Notebooks with Academic</title><link>https://solmessing.netlify.app/post/jupyter/</link><pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/post/jupyter/</guid><description>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">IPython.core.display&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Image&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Image&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./index_1_0.png" alt="png" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Welcome to Academic!&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>Welcome to Academic!
&lt;/code>&lt;/pre>
&lt;h2 id="install-python-and-jupyterlab">Install Python and JupyterLab&lt;/h2>
&lt;p>&lt;a href="https://www.anaconda.com/distribution/#download-section" target="_blank" rel="noopener">Install Anaconda&lt;/a> which includes Python 3 and JupyterLab.&lt;/p>
&lt;p>Alternatively, install JupyterLab with &lt;code>pip3 install jupyterlab&lt;/code>.&lt;/p>
&lt;h2 id="create-or-upload-a-jupyter-notebook">Create or upload a Jupyter notebook&lt;/h2>
&lt;p>Run the following commands in your Terminal, substituting &lt;code>&amp;lt;MY-WEBSITE-FOLDER&amp;gt;&lt;/code> and &lt;code>&amp;lt;SHORT-POST-TITLE&amp;gt;&lt;/code> with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">mkdir -p &amp;lt;MY-WEBSITE-FOLDER&amp;gt;/content/post/&amp;lt;SHORT-POST-TITLE&amp;gt;/
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">cd&lt;/span> &amp;lt;MY-WEBSITE-FOLDER&amp;gt;/content/post/&amp;lt;SHORT-POST-TITLE&amp;gt;/
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">jupyter lab index.ipynb
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>jupyter&lt;/code> command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.&lt;/p>
&lt;h2 id="edit-your-post-metadata">Edit your post metadata&lt;/h2>
&lt;p>The first cell of your Jupter notebook will contain your post metadata (&lt;a href="https://sourcethemes.com/academic/docs/front-matter/" target="_blank" rel="noopener">front matter&lt;/a>).&lt;/p>
&lt;p>In Jupter, choose &lt;em>Markdown&lt;/em> as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">---
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">title: My post&amp;#39;s title
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">date: 2019-09-01
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># Put any other Academic metadata here...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">---
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Edit the metadata of your post, using the &lt;a href="https://sourcethemes.com/academic/docs/managing-content" target="_blank" rel="noopener">documentation&lt;/a> as a guide to the available options.&lt;/p>
&lt;p>To set a &lt;a href="https://sourcethemes.com/academic/docs/managing-content/#featured-image" target="_blank" rel="noopener">featured image&lt;/a>, place an image named &lt;code>featured&lt;/code> into your post&amp;rsquo;s folder.&lt;/p>
&lt;p>For other tips, such as using math, see the guide on &lt;a href="https://wowchemy.com/docs/content/writing-markdown-latex/" target="_blank" rel="noopener">writing content with Academic&lt;/a>.&lt;/p>
&lt;h2 id="convert-notebook-to-markdown">Convert notebook to Markdown&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir&lt;span class="o">=&lt;/span>.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="example">Example&lt;/h2>
&lt;p>This post was created with Jupyter. The orginal files can be found at &lt;a href="https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter" target="_blank" rel="noopener">https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter&lt;/a>&lt;/p></description></item><item><title>Slides</title><link>https://solmessing.netlify.app/slides/example/</link><pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/slides/example/</guid><description>&lt;h1 id="create-slides-in-markdown-with-wowchemy">Create slides in Markdown with Wowchemy&lt;/h1>
&lt;p>&lt;a href="https://wowchemy.com/" target="_blank" rel="noopener">Wowchemy&lt;/a> | &lt;a href="https://wowchemy.com/docs/content/slides/" target="_blank" rel="noopener">Documentation&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="features">Features&lt;/h2>
&lt;ul>
&lt;li>Efficiently write slides in Markdown&lt;/li>
&lt;li>3-in-1: Create, Present, and Publish your slides&lt;/li>
&lt;li>Supports speaker notes&lt;/li>
&lt;li>Mobile friendly slides&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="controls">Controls&lt;/h2>
&lt;ul>
&lt;li>Next: &lt;code>Right Arrow&lt;/code> or &lt;code>Space&lt;/code>&lt;/li>
&lt;li>Previous: &lt;code>Left Arrow&lt;/code>&lt;/li>
&lt;li>Start: &lt;code>Home&lt;/code>&lt;/li>
&lt;li>Finish: &lt;code>End&lt;/code>&lt;/li>
&lt;li>Overview: &lt;code>Esc&lt;/code>&lt;/li>
&lt;li>Speaker notes: &lt;code>S&lt;/code>&lt;/li>
&lt;li>Fullscreen: &lt;code>F&lt;/code>&lt;/li>
&lt;li>Zoom: &lt;code>Alt + Click&lt;/code>&lt;/li>
&lt;li>&lt;a href="https://revealjs.com/pdf-export/" target="_blank" rel="noopener">PDF Export&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="code-highlighting">Code Highlighting&lt;/h2>
&lt;p>Inline code: &lt;code>variable&lt;/code>&lt;/p>
&lt;p>Code block:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">porridge&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;blueberry&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">if&lt;/span> &lt;span class="n">porridge&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s2">&amp;#34;blueberry&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Eating...&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;h2 id="math">Math&lt;/h2>
&lt;p>In-line math: $x + y = z$&lt;/p>
&lt;p>Block math:&lt;/p>
&lt;p>$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p>
&lt;hr>
&lt;h2 id="fragments">Fragments&lt;/h2>
&lt;p>Make content appear incrementally&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">{{% fragment %}} One {{% /fragment %}}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">{{% fragment %}} **Two** {{% /fragment %}}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">{{% fragment %}} Three {{% /fragment %}}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Press &lt;code>Space&lt;/code> to play!&lt;/p>
&lt;span class="fragment " >
One
&lt;/span>
&lt;span class="fragment " >
&lt;strong>Two&lt;/strong>
&lt;/span>
&lt;span class="fragment " >
Three
&lt;/span>
&lt;hr>
&lt;p>A fragment can accept two optional parameters:&lt;/p>
&lt;ul>
&lt;li>&lt;code>class&lt;/code>: use a custom style (requires definition in custom CSS)&lt;/li>
&lt;li>&lt;code>weight&lt;/code>: sets the order in which a fragment appears&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="speaker-notes">Speaker Notes&lt;/h2>
&lt;p>Add speaker notes to your presentation&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-markdown" data-lang="markdown">&lt;span class="line">&lt;span class="cl">{{% speaker_note %}}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">-&lt;/span> Only the speaker can read these notes
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">-&lt;/span> Press &lt;span class="sb">`S`&lt;/span> key to view
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> {{% /speaker_note %}}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Press the &lt;code>S&lt;/code> key to view the speaker notes!&lt;/p>
&lt;aside class="notes">
&lt;ul>
&lt;li>Only the speaker can read these notes&lt;/li>
&lt;li>Press &lt;code>S&lt;/code> key to view&lt;/li>
&lt;/ul>
&lt;/aside>
&lt;hr>
&lt;h2 id="themes">Themes&lt;/h2>
&lt;ul>
&lt;li>black: Black background, white text, blue links (default)&lt;/li>
&lt;li>white: White background, black text, blue links&lt;/li>
&lt;li>league: Gray background, white text, blue links&lt;/li>
&lt;li>beige: Beige background, dark text, brown links&lt;/li>
&lt;li>sky: Blue background, thin dark text, blue links&lt;/li>
&lt;/ul>
&lt;hr>
&lt;ul>
&lt;li>night: Black background, thick white text, orange links&lt;/li>
&lt;li>serif: Cappuccino background, gray text, brown links&lt;/li>
&lt;li>simple: White background, black text, blue links&lt;/li>
&lt;li>solarized: Cream-colored background, dark green text, blue links&lt;/li>
&lt;/ul>
&lt;hr>
&lt;section data-noprocess data-shortcode-slide
data-background-image="/media/boards.jpg"
>
&lt;h2 id="custom-slide">Custom Slide&lt;/h2>
&lt;p>Customize the slide style and background&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-markdown" data-lang="markdown">&lt;span class="line">&lt;span class="cl">{{&lt;span class="p">&amp;lt;&lt;/span> &lt;span class="nt">slide&lt;/span> &lt;span class="na">background-image&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#34;/media/boards.jpg&amp;#34;&lt;/span> &lt;span class="p">&amp;gt;&lt;/span>}}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">{{&lt;span class="p">&amp;lt;&lt;/span> &lt;span class="nt">slide&lt;/span> &lt;span class="na">background-color&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#34;#0000FF&amp;#34;&lt;/span> &lt;span class="p">&amp;gt;&lt;/span>}}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">{{&lt;span class="p">&amp;lt;&lt;/span> &lt;span class="nt">slide&lt;/span> &lt;span class="na">class&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#34;my-style&amp;#34;&lt;/span> &lt;span class="p">&amp;gt;&lt;/span>}}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;h2 id="custom-css-example">Custom CSS Example&lt;/h2>
&lt;p>Let&amp;rsquo;s make headers navy colored.&lt;/p>
&lt;p>Create &lt;code>assets/css/reveal_custom.css&lt;/code> with:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-css" data-lang="css">&lt;span class="line">&lt;span class="cl">&lt;span class="p">.&lt;/span>&lt;span class="nc">reveal&lt;/span> &lt;span class="nt">section&lt;/span> &lt;span class="nt">h1&lt;/span>&lt;span class="o">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">.&lt;/span>&lt;span class="nc">reveal&lt;/span> &lt;span class="nt">section&lt;/span> &lt;span class="nt">h2&lt;/span>&lt;span class="o">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">.&lt;/span>&lt;span class="nc">reveal&lt;/span> &lt;span class="nt">section&lt;/span> &lt;span class="nt">h3&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">color&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="kc">navy&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;h1 id="questions">Questions?&lt;/h1>
&lt;p>&lt;a href="https://discord.gg/z8wNYzb" target="_blank" rel="noopener">Ask&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://wowchemy.com/docs/content/slides/" target="_blank" rel="noopener">Documentation&lt;/a>&lt;/p></description></item><item><title>Projecting confidence: How the probabilistic horse race confuses and demobilizes the public</title><link>https://solmessing.netlify.app/publication/wlm-2019-projecting/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/wlm-2019-projecting/</guid><description>&lt;ul>
&lt;li>Supplimentary &lt;a href="https://solmessing.netlify.app/pdf/aggregatorSM.pdf">materials&lt;/a>&lt;/li>
&lt;li>Cited by &lt;a href="https://fivethirtyeight.com/features/politics-podcast-whats-so-wrong-with-nancy-pelosi/" target="_blank" rel="noopener">FiveThirthyEight&amp;rsquo;s Politics
Podcast&lt;/a>
as influential in decision to change forecast presentation.&lt;/li>
&lt;li>Media coverage: &lt;a href="https://www.washingtonpost.com/news/politics/wp/2018/02/06/clintons-achilles-heel-in-2016-may-have-been-overconfidence/?utm_term=.b71fb84da00f" target="_blank" rel="noopener">Washington
Post&lt;/a>,
&lt;a href="http://nymag.com/daily/intelligencer/2018/02/americans-dont-understand-election-probabilities.html" target="_blank" rel="noopener">New York
Magazine&lt;/a>,
&lt;a href="https://politicalwire.com/2018/02/06/election-forecasts-lower-voter-turnout/" target="_blank" rel="noopener">Political
Wire&lt;/a>.&lt;/li>
&lt;/ul></description></item><item><title>Tumor formation of adult stem cell transplants in rodent arthritic joints</title><link>https://solmessing.netlify.app/publication/chapelin-2019-tumor/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/chapelin-2019-tumor/</guid><description/></item><item><title>How to break regression</title><link>https://solmessing.netlify.app/post/how-to-break-regression/</link><pubDate>Wed, 13 Jun 2018 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/post/how-to-break-regression/</guid><description>&lt;p>Regression models are a cornerstone of modern social science. They’re at the heart of efforts to estimate causal relationships between variables in a multivariate environment and are the basic building blocks of many machine learning models. Yet social scientists can run into a lot of situations where regression models break.&lt;/p>
&lt;p>Famed social psychologist Richard Nisbett &lt;a href="https://www.edge.org/conversation/richard_nisbett-the-crusade-against-multiple-regression-analysis" target="_blank" rel="noopener">recently argued&lt;/a> that regression analysis is so misused and misunderstood that analyses based on multiple regression “are often somewhere between meaningless and quite damaging.” (He was mainly talking about cases in which researchers publish correlational results that are covered in the media as causal statements about the world.)&lt;/p>
&lt;p>Below, I’ll walk through some of the potential pitfalls you might encounter when you fire up your favorite &lt;a href="https://seanjtaylor.com/post/39573264781/the-statistics-software-signal" target="_blank" rel="noopener">statistical software&lt;/a> package and run regressions. Specifically, I’ll be using simulation in R as an educational tool to help you better understand the ways in which regressions can break.&lt;/p>
&lt;p>&lt;strong>Using simulations to unpack regression&lt;/strong>&lt;/p>
&lt;p>The idea of using R simulations to help understand regression models was inspired by Ben Ogorek’s &lt;a href="http://anythingbutrbitrary.blogspot.com/2016/01/how-to-create-confounders-with.html" target="_blank" rel="noopener">post&lt;/a> on regression confounders and collider bias.&lt;/p>
&lt;p>The great thing about using simulation in this way is that you control the world that generates your data. The code I’ll introduce below represents the true &lt;em>data-generating process&lt;/em>,since I’m using R’s random number generators to simulate the data. In real life, of course, we only have the data we observe, and we don’t really know how the data-generating process works unless we have a solid theory (like Newtonian physics or evolution) where the system of relevant variables and causal relationships is well understood and to which there is really no analogous phenomenon in social science.&lt;/p>
&lt;p>What I’ll do here is create a dataset based on two random standard normal variables by simulating them using the &lt;em>rnorm()&lt;/em> function, which draws random values from a normal distribution with mean 0 and standard deviation 1, unless you specify otherwise. I’ll create a functional relationship between y and x such that a 1 unit increase in x will be associated with a .4 unit increase in y.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-R" data-lang="R">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># make the code reproducible by setting a random number seed&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">set.seed&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">100&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># When everything works:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">N&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="m">1000&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">x&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">rnorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">y&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="m">.4&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nf">rnorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">hist&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">hist&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Now estimate our model:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">summary&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">lm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Call&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">lm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">formula&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Residuals&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Min&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="n">Q&lt;/span> &lt;span class="n">Median&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="n">Q&lt;/span> &lt;span class="n">Max&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">-3.0348&lt;/span> &lt;span class="m">-0.7013&lt;/span> &lt;span class="m">0.0085&lt;/span> &lt;span class="m">0.6212&lt;/span> &lt;span class="m">3.1688&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Coefficients&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Estimate&lt;/span> &lt;span class="n">Std.&lt;/span> &lt;span class="n">Error&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="n">value&lt;/span> &lt;span class="nf">Pr&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;gt;|&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">|&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">(&lt;/span>&lt;span class="n">Intercept&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="m">0.003921&lt;/span> &lt;span class="m">0.031039&lt;/span> &lt;span class="m">0.126&lt;/span> &lt;span class="m">0.899&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">x&lt;/span> &lt;span class="m">0.413415&lt;/span> &lt;span class="m">0.030129&lt;/span> &lt;span class="m">13.722&lt;/span> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="m">2e-16&lt;/span> &lt;span class="o">***&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">---&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Signif.&lt;/span> &lt;span class="n">codes&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0&lt;/span> ‘&lt;span class="o">***&lt;/span>’ &lt;span class="m">0.001&lt;/span> ‘&lt;span class="o">**&lt;/span>’ &lt;span class="m">0.01&lt;/span> ‘&lt;span class="o">*&lt;/span>’ &lt;span class="m">0.05&lt;/span> ‘&lt;span class="n">.’&lt;/span> &lt;span class="m">0.1&lt;/span> ‘ ’ &lt;span class="m">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Residual&lt;/span> &lt;span class="n">standard&lt;/span> &lt;span class="n">error&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.9814&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="m">998&lt;/span> &lt;span class="n">degrees&lt;/span> &lt;span class="n">of&lt;/span> &lt;span class="n">freedom&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Multiple&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">squared&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.1587&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Adjusted&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">squared&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.1579&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="bp">F&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">statistic&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">188.3&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="m">1&lt;/span> &lt;span class="n">and&lt;/span> &lt;span class="m">998&lt;/span> &lt;span class="n">DF&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">value&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="m">2.2e-16&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Plot it&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ggplot2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">qplot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nf">geom_smooth&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">method&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#39;lm&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nf">theme_bw&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nf">ggtitle&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;The Perfect Regression&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Notice that the model estimates the functional relationship between x and y that I simulated quite well. The plot looks like this:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://cdn-images-1.medium.com/max/1600/1*0zIR7Mtuak5DamPOgEiHog.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>What about omitted variables? Our machinery actually still works if there is another factor causing y, as long as it is &lt;em>uncorrelated&lt;/em> with x.&lt;/p>
&lt;p>&lt;strong>The dreaded omitted variable bias&lt;/strong>&lt;/p>
&lt;p>Omitted variable bias (OVB) is much feared, and judging by the top internet search results, not well understood. Some top sources say it occurs when “&lt;a href="http://carecon.org.uk/UWEcourse/OVbias.pdf" target="_blank" rel="noopener">an important&lt;/a>” variable is missing or when a variable that “&lt;a href="https://en.wikipedia.org/wiki/Omitted-variable_bias" target="_blank" rel="noopener">is correlated&lt;/a>” with both x and y is missing. I even found a university &lt;a href="http://www3.wabash.edu/econometrics/EconometricsBook/chap18.htm" target="_blank" rel="noopener">econometrics&lt;/a> course that defined OVB this way.&lt;/p>
&lt;p>But neither of those definitions are quite right. OVB occurs when a variable that &lt;em>causes&lt;/em> y is missing from the model (and is correlated with x). Let’s call that variable w. Because w is in play when we consider the causal relationship between x and y, it’s often referred to as “endogenous” or a “confounding variable.”&lt;/p>
&lt;p>The example below first demonstrates that w, our confounding variable, will bias our results if we fail to include it in our model. The next two examples are essentially a re-telling of the &lt;a href="http://anythingbutrbitrary.blogspot.com/2016/01/how-to-create-confounders-with.html" target="_blank" rel="noopener">post I mentioned above&lt;/a> on collider bias, but emphasizing slightly different points.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-R" data-lang="R">&lt;span class="line">&lt;span class="cl">&lt;span class="n">w&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">rnorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">x&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="m">.5&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">w&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nf">rnorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">y&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="m">.4&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="m">.3&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">w&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nf">rnorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">m1&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">lm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">summary &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># Omitted variable bias&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Call&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">lm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">formula&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Residuals&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Min&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="n">Q&lt;/span> &lt;span class="n">Median&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="n">Q&lt;/span> &lt;span class="n">Max&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">-3.2190&lt;/span> &lt;span class="m">-0.7025&lt;/span> &lt;span class="m">0.0314&lt;/span> &lt;span class="m">0.7120&lt;/span> &lt;span class="m">3.1158&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Coefficients&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Estimate&lt;/span> &lt;span class="n">Std.&lt;/span> &lt;span class="n">Error&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="n">value&lt;/span> &lt;span class="nf">Pr&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;gt;|&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">|&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">(&lt;/span>&lt;span class="n">Intercept&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="m">0.01126&lt;/span> &lt;span class="m">0.03310&lt;/span> &lt;span class="m">0.34&lt;/span> &lt;span class="m">0.734&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">x&lt;/span> &lt;span class="m">0.50179&lt;/span> &lt;span class="m">0.03049&lt;/span> &lt;span class="m">16.46&lt;/span> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="m">2e-16&lt;/span> &lt;span class="o">***&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">---&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Signif.&lt;/span> &lt;span class="n">codes&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0&lt;/span> ‘&lt;span class="o">***&lt;/span>’ &lt;span class="m">0.001&lt;/span> ‘&lt;span class="o">**&lt;/span>’ &lt;span class="m">0.01&lt;/span> ‘&lt;span class="o">*&lt;/span>’ &lt;span class="m">0.05&lt;/span> ‘&lt;span class="n">.’&lt;/span> &lt;span class="m">0.1&lt;/span> ‘ ’ &lt;span class="m">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Residual&lt;/span> &lt;span class="n">standard&lt;/span> &lt;span class="n">error&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">1.046&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="m">998&lt;/span> &lt;span class="n">degrees&lt;/span> &lt;span class="n">of&lt;/span> &lt;span class="n">freedom&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Multiple&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">squared&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.2135&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Adjusted&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">squared&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.2127&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="bp">F&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">statistic&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">270.9&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="m">1&lt;/span> &lt;span class="n">and&lt;/span> &lt;span class="m">998&lt;/span> &lt;span class="n">DF&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">value&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="m">2.2e-16&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There it is: classic omitted variable bias. We only observed x, and the influence of the omitted variable w was attributed to x in our model. If you re-rerun the regression with w in the model, you no longer get biased estimates.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-R" data-lang="R">&lt;span class="line">&lt;span class="cl">&lt;span class="n">m2&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">lm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">w&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">summary &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># No omitted variable bias after conditioning on w&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Call&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">lm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">formula&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">w&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Residuals&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Min&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="n">Q&lt;/span> &lt;span class="n">Median&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="n">Q&lt;/span> &lt;span class="n">Max&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">-3.2748&lt;/span> &lt;span class="m">-0.6632&lt;/span> &lt;span class="m">-0.0001&lt;/span> &lt;span class="m">0.6933&lt;/span> &lt;span class="m">2.9664&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Coefficients&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Estimate&lt;/span> &lt;span class="n">Std.&lt;/span> &lt;span class="n">Error&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="n">value&lt;/span> &lt;span class="nf">Pr&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;gt;|&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">|&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">(&lt;/span>&lt;span class="n">Intercept&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="m">0.02841&lt;/span> &lt;span class="m">0.03141&lt;/span> &lt;span class="m">0.905&lt;/span> &lt;span class="m">0.366&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">x&lt;/span> &lt;span class="m">0.40627&lt;/span> &lt;span class="m">0.03132&lt;/span> &lt;span class="m">12.973&lt;/span> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="m">2e-16&lt;/span> &lt;span class="o">***&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">w&lt;/span> &lt;span class="m">0.32344&lt;/span> &lt;span class="m">0.03439&lt;/span> &lt;span class="m">9.405&lt;/span> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="m">2e-16&lt;/span> &lt;span class="o">***&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">---&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Signif.&lt;/span> &lt;span class="n">codes&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0&lt;/span> ‘&lt;span class="o">***&lt;/span>’ &lt;span class="m">0.001&lt;/span> ‘&lt;span class="o">**&lt;/span>’ &lt;span class="m">0.01&lt;/span> ‘&lt;span class="o">*&lt;/span>’ &lt;span class="m">0.05&lt;/span> ‘&lt;span class="n">.’&lt;/span> &lt;span class="m">0.1&lt;/span> ‘ ’ &lt;span class="m">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Residual&lt;/span> &lt;span class="n">standard&lt;/span> &lt;span class="n">error&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.9927&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="m">997&lt;/span> &lt;span class="n">degrees&lt;/span> &lt;span class="n">of&lt;/span> &lt;span class="n">freedom&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Multiple&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">squared&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.3024&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Adjusted&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">squared&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.301&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="bp">F&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">statistic&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">216.1&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="m">2&lt;/span> &lt;span class="n">and&lt;/span> &lt;span class="m">997&lt;/span> &lt;span class="n">DF&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">value&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="m">2.2e-16&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that the regression errors, also known as residuals, are correlated with w:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-R" data-lang="R">&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now, recall above that I wrote that it’s wrong to say that OVB occurs when our omitted variable is correlated with both x and y. And yet w, x and w and y are all correlated in this first example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-R" data-lang="R">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">cor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">w&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">m1&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">residuals&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">[1]&lt;/span> &lt;span class="m">0.2597859&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>So why can’t we just say that OVB occurs when our omitted variable is correlated with both x and y? As the next example will show, correlation isn’t enough — w needs to &lt;em>cause&lt;/em> both x and y. We can easily imagine a case in which we don’t have causality but we still see this kind of correlation — when x and y both cause w.&lt;/p>
&lt;p>Let’s make this a little more concrete. Suppose we care about the effect of news media consumption (x) on voter turnout (y). One factor that some researchers think may cause both news media consumption and turnout is political interest (w). If we only measure media consumption and voter turnout, political interest is likely to confound our estimates.&lt;/p>
&lt;p>But another school of thought from social psychology — along the lines of self-perception theory and &lt;a href="https://en.wikipedia.org/wiki/Cognitive_dissonance" target="_blank" rel="noopener">cognitive dissonance&lt;/a> — suggests that the causality could be reversed: Voting behavior might be mostly determined by other factors, and casting a ballot might prompt us to be &lt;em>more&lt;/em> interested in political developments in the future. Similarly, watching the news might prompt us to become &lt;em>more&lt;/em> interested in politics. Let’s suppose that second school of thought is right. If so, our simulated data will look like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-R" data-lang="R">&lt;span class="line">&lt;span class="cl">&lt;span class="n">media_consumption_x&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">rnorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">voter_turnout_y&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="m">.1&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">media_consumption_x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nf">rnorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Political interest increases after consuming media and participating, and, &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># in this hypothetical world, does *not* increase media consuption or participation&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">political_interest_w&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="m">1.2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">media_consumption_x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="m">.6&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">voter_turnout_y&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nf">rnorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">cormat&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">cor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">as.matrix&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">data.frame&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">media_consumption_x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">voter_turnout_y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">political_interest_w&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">round&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cormat&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">media_consumption_x&lt;/span> &lt;span class="n">voter_turnout_y&lt;/span> &lt;span class="n">political_interest_w&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">media_consumption_x&lt;/span> &lt;span class="m">1.00&lt;/span> &lt;span class="m">0.11&lt;/span> &lt;span class="m">0.70&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">voter_turnout_y&lt;/span> &lt;span class="m">0.11&lt;/span> &lt;span class="m">1.00&lt;/span> &lt;span class="m">0.46&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">political_interest_w&lt;/span> &lt;span class="m">0.70&lt;/span> &lt;span class="m">0.46&lt;/span> &lt;span class="m">1.00&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>As you can see, all factors are again correlated with each other. But this time, if we &lt;em>only&lt;/em> include x (media consumption) and y (turnout) in the equation, we get the correct estimate:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-R" data-lang="R">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">summary&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">lm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">voter_turnout_y&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">media_consumption_x&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Call&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">lm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">formula&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">voter_turnout_y&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">media_consumption_x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Residuals&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Min&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="n">Q&lt;/span> &lt;span class="n">Median&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="n">Q&lt;/span> &lt;span class="n">Max&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">-2.8460&lt;/span> &lt;span class="m">-0.6972&lt;/span> &lt;span class="m">-0.0076&lt;/span> &lt;span class="m">0.6702&lt;/span> &lt;span class="m">3.3925&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Coefficients&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Estimate&lt;/span> &lt;span class="n">Std.&lt;/span> &lt;span class="n">Error&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="n">value&lt;/span> &lt;span class="nf">Pr&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;gt;|&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">|&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">(&lt;/span>&lt;span class="n">Intercept&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="m">-0.01202&lt;/span> &lt;span class="m">0.03217&lt;/span> &lt;span class="m">-0.374&lt;/span> &lt;span class="m">0.708839&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">media_consumption_x&lt;/span> &lt;span class="m">0.11719&lt;/span> &lt;span class="m">0.03321&lt;/span> &lt;span class="m">3.529&lt;/span> &lt;span class="m">0.000436&lt;/span> &lt;span class="o">***&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">---&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Signif.&lt;/span> &lt;span class="n">codes&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0&lt;/span> ‘&lt;span class="o">***&lt;/span>’ &lt;span class="m">0.001&lt;/span> ‘&lt;span class="o">**&lt;/span>’ &lt;span class="m">0.01&lt;/span> ‘&lt;span class="o">*&lt;/span>’ &lt;span class="m">0.05&lt;/span> ‘&lt;span class="n">.’&lt;/span> &lt;span class="m">0.1&lt;/span> ‘ ’ &lt;span class="m">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Residual&lt;/span> &lt;span class="n">standard&lt;/span> &lt;span class="n">error&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">1.014&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="m">998&lt;/span> &lt;span class="n">degrees&lt;/span> &lt;span class="n">of&lt;/span> &lt;span class="n">freedom&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Multiple&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">squared&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.01233&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Adjusted&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">squared&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.01134&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="bp">F&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">statistic&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">12.46&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="m">1&lt;/span> &lt;span class="n">and&lt;/span> &lt;span class="m">998&lt;/span> &lt;span class="n">DF&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">value&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.0004359&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>What makes defining omitted variable bias based on correlation so dangerous is that if we now include w (political interest), we will get a different kind of bias — what’s called &lt;a href="https://en.wikipedia.org/wiki/Collider_%28epidemiology%29" target="_blank" rel="noopener">collider bias&lt;/a> or &lt;a href="https://www.annualreviews.org/doi/10.1146/annurev-soc-071913-043455" target="_blank" rel="noopener">endogenous selection bias&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-R" data-lang="R">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">summary&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">lm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">voter_turnout_y&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">media_consumption_x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">political_interest_w&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Call&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">lm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">formula&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">voter_turnout_y&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">media_consumption_x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">political_interest_w&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Residuals&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Min&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="n">Q&lt;/span> &lt;span class="n">Median&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="n">Q&lt;/span> &lt;span class="n">Max&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">-2.1569&lt;/span> &lt;span class="m">-0.5981&lt;/span> &lt;span class="m">-0.0129&lt;/span> &lt;span class="m">0.5701&lt;/span> &lt;span class="m">2.8356&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Coefficients&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Estimate&lt;/span> &lt;span class="n">Std.&lt;/span> &lt;span class="n">Error&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="n">value&lt;/span> &lt;span class="nf">Pr&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;gt;|&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">|&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">(&lt;/span>&lt;span class="n">Intercept&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="m">0.003155&lt;/span> &lt;span class="m">0.027098&lt;/span> &lt;span class="m">0.116&lt;/span> &lt;span class="m">0.907&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">media_consumption_x&lt;/span> &lt;span class="m">-0.437084&lt;/span> &lt;span class="m">0.039102&lt;/span> &lt;span class="m">-11.178&lt;/span> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="m">2e-16&lt;/span> &lt;span class="o">***&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">political_interest_w&lt;/span> &lt;span class="m">0.444571&lt;/span> &lt;span class="m">0.021928&lt;/span> &lt;span class="m">20.274&lt;/span> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="m">2e-16&lt;/span> &lt;span class="o">***&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">---&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Signif.&lt;/span> &lt;span class="n">codes&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0&lt;/span> ‘&lt;span class="o">***&lt;/span>’ &lt;span class="m">0.001&lt;/span> ‘&lt;span class="o">**&lt;/span>’ &lt;span class="m">0.01&lt;/span> ‘&lt;span class="o">*&lt;/span>’ &lt;span class="m">0.05&lt;/span> ‘&lt;span class="n">.’&lt;/span> &lt;span class="m">0.1&lt;/span> ‘ ’ &lt;span class="m">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Residual&lt;/span> &lt;span class="n">standard&lt;/span> &lt;span class="n">error&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.854&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="m">997&lt;/span> &lt;span class="n">degrees&lt;/span> &lt;span class="n">of&lt;/span> &lt;span class="n">freedom&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Multiple&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">squared&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.3007&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Adjusted&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">squared&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.2993&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="bp">F&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">statistic&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">214.3&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="m">2&lt;/span> &lt;span class="n">and&lt;/span> &lt;span class="m">997&lt;/span> &lt;span class="n">DF&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">value&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="m">2.2e-16&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Simpson’s paradox&lt;/strong>&lt;/p>
&lt;p>Simpson’s paradox often occurs in social science (and medicine, too) when you pool data instead of conditioning it on group membership (i.e., adding it as a factor in your regression model).&lt;/p>
&lt;p>Suppose that, all other things being equal, consuming media causes a slight shift in policy preferences toward the left. But, on average, Republicans consume more news than non-Republicans. And we know that generally Republicans have much more right-leaning preferences.&lt;/p>
&lt;p>If we just measure media consumption and policy preferences without including Republicans in the model, we’ll actually estimate that the effect goes in the direction &lt;em>opposite&lt;/em> of the true causal effect.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-R" data-lang="R">&lt;span class="line">&lt;span class="cl">&lt;span class="n">N&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="m">1000&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Let&amp;#39;s say that 40% of people in this population are Republicans&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">republican&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">rbinom&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">.4&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># And they consume more media&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">media_consumption&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="m">.75&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">republican&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nf">rnorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Consuming more media causes a slight leftward shift in policy&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># preferences, and Republicans have more right-leaning preferences&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">policy_prefs&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="m">-.2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">media_consumption&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="m">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">republican&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nf">rnorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># for easier plotting later&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">df&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">data.frame&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">media_consumption&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">policy_prefs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">republican&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">df&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">republican&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">factor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;non-republican&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;republican&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="n">[df&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">republican&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="n">]&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># If we don&amp;#39;t condition on being Republican, we&amp;#39;ll actually estimate&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># that the effect goes in the *opposite* direction&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">summary&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">lm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">policy_prefs&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">media_consumption&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Call&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">lm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">formula&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">policy_prefs&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">media_consumption&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Residuals&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Min&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="n">Q&lt;/span> &lt;span class="n">Median&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="n">Q&lt;/span> &lt;span class="n">Max&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">-3.6108&lt;/span> &lt;span class="m">-0.9559&lt;/span> &lt;span class="m">-0.0198&lt;/span> &lt;span class="m">0.9257&lt;/span> &lt;span class="m">3.9537&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Coefficients&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Estimate&lt;/span> &lt;span class="n">Std.&lt;/span> &lt;span class="n">Error&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="n">value&lt;/span> &lt;span class="nf">Pr&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;gt;|&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">|&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">(&lt;/span>&lt;span class="n">Intercept&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="m">0.68923&lt;/span> &lt;span class="m">0.04323&lt;/span> &lt;span class="m">15.94&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="m">2e-16&lt;/span> &lt;span class="o">***&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">media_consumption&lt;/span> &lt;span class="m">0.15269&lt;/span> &lt;span class="m">0.03966&lt;/span> &lt;span class="m">3.85&lt;/span> &lt;span class="m">0.000126&lt;/span> &lt;span class="o">***&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">---&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Signif.&lt;/span> &lt;span class="n">codes&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0&lt;/span> ‘&lt;span class="o">***&lt;/span>’ &lt;span class="m">0.001&lt;/span> ‘&lt;span class="o">**&lt;/span>’ &lt;span class="m">0.01&lt;/span> ‘&lt;span class="o">*&lt;/span>’ &lt;span class="m">0.05&lt;/span> ‘&lt;span class="n">.’&lt;/span> &lt;span class="m">0.1&lt;/span> ‘ ’ &lt;span class="m">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Residual&lt;/span> &lt;span class="n">standard&lt;/span> &lt;span class="n">error&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">1.317&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="m">998&lt;/span> &lt;span class="n">degrees&lt;/span> &lt;span class="n">of&lt;/span> &lt;span class="n">freedom&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Multiple&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">squared&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.01463&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Adjusted&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">squared&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.01365&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="bp">F&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">statistic&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">14.82&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="m">1&lt;/span> &lt;span class="n">and&lt;/span> &lt;span class="m">998&lt;/span> &lt;span class="n">DF&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">value&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.0001257&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Naive plot&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">qplot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">media_consumption&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">policy_prefs&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nf">geom_smooth&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">method&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#39;lm&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nf">theme_bw&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nf">ggtitle&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;Naive estimate (Simpson&amp;#39;s Paradox)&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The estimate goes in the opposite direction of the true effect! Here’s what the plot looks like:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://cdn-images-1.medium.com/max/1600/1*2gxiWJN7ElkYAO-wuvakzA.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>To resolve this paradox, we need to add a factor in the model that indicates whether or not a respondent is a Republican. Adding that factor lets us estimate &lt;em>separate&lt;/em> slopes for Republicans and non-Republicans. Note that this is &lt;em>not&lt;/em> like estimating an interaction term, where two explanatory variables are multiplied together. It’s not that the slopes are &lt;em>different&lt;/em>, we just need to estimate separate ones for Republicans and non-Republicans.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-R" data-lang="R">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Condition on being a Republican to get the right estimates&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">summary&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">lm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">policy_prefs&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">media_consumption&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">republican&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Call&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">lm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">formula&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">policy_prefs&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">media_consumption&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">republican&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Residuals&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Min&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="n">Q&lt;/span> &lt;span class="n">Median&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="n">Q&lt;/span> &lt;span class="n">Max&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">-3.5518&lt;/span> &lt;span class="m">-0.6678&lt;/span> &lt;span class="m">-0.0186&lt;/span> &lt;span class="m">0.6562&lt;/span> &lt;span class="m">3.3009&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Coefficients&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Estimate&lt;/span> &lt;span class="n">Std.&lt;/span> &lt;span class="n">Error&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="n">value&lt;/span> &lt;span class="nf">Pr&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;gt;|&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">|&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">(&lt;/span>&lt;span class="n">Intercept&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="m">0.05335&lt;/span> &lt;span class="m">0.03904&lt;/span> &lt;span class="m">1.366&lt;/span> &lt;span class="m">0.172&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">media_consumption&lt;/span> &lt;span class="m">-0.13615&lt;/span> &lt;span class="m">0.03111&lt;/span> &lt;span class="m">-4.376&lt;/span> &lt;span class="m">1.34e-05&lt;/span> &lt;span class="o">***&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">republican&lt;/span> &lt;span class="m">1.93049&lt;/span> &lt;span class="m">0.06758&lt;/span> &lt;span class="m">28.565&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="m">2e-16&lt;/span> &lt;span class="o">***&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">---&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Signif.&lt;/span> &lt;span class="n">codes&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0&lt;/span> ‘&lt;span class="o">***&lt;/span>’ &lt;span class="m">0.001&lt;/span> ‘&lt;span class="o">**&lt;/span>’ &lt;span class="m">0.01&lt;/span> ‘&lt;span class="o">*&lt;/span>’ &lt;span class="m">0.05&lt;/span> ‘&lt;span class="n">.’&lt;/span> &lt;span class="m">0.1&lt;/span> ‘ ’ &lt;span class="m">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Residual&lt;/span> &lt;span class="n">standard&lt;/span> &lt;span class="n">error&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.9774&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="m">997&lt;/span> &lt;span class="n">degrees&lt;/span> &lt;span class="n">of&lt;/span> &lt;span class="n">freedom&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Multiple&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">squared&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.4581&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Adjusted&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">squared&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.457&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="bp">F&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">statistic&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">421.4&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="m">2&lt;/span> &lt;span class="n">and&lt;/span> &lt;span class="m">997&lt;/span> &lt;span class="n">DF&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">value&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="m">2.2e-16&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Conditioning on being Republican&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">qplot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">media_consumption&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">policy_prefs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">data&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">df&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">colour&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">republican&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nf">scale_color_manual&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">values&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;blue&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#34;red&amp;#34;&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nf">geom_smooth&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">method&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#39;lm&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nf">theme_bw&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nf">ggtitle&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;Conditioning on being a Republican (Simpson&amp;#39;s Paradox)&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here’s what the plot looks like:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://cdn-images-1.medium.com/max/1600/1*rwNGDFcsSSMiGsJoc6ZxsQ.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>&lt;strong>Correlated errors&lt;/strong>&lt;/p>
&lt;p>Another cardinal sin — and one that we should worry a lot about because it often arises from social desirability bias in survey responses — is the phenomenon of correlated errors. This example is inspired by &lt;a href="https://www.nowpublishers.com/article/Details/QJPS-6005" target="_blank" rel="noopener">Vavreck (2007).&lt;/a>&lt;/p>
&lt;p>Here, self-reported turnout and media consumption are caused by a combination of social desirability bias and true turnout and true consumption, respectively:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-R" data-lang="R">&lt;span class="line">&lt;span class="cl">&lt;span class="n">N&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="m">1000&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># The &amp;#34;Truth&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">true_media_consumption&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">rnorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">true_vote&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="m">.1&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">media_consumption&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nf">rnorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># social desirability bias&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">social_desirability&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">rnorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#what we actually observe from self reports:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">self_report_media_consumption&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="n">true_media_consumption&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">social_desirability&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">self_report_vote&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="n">true_vote&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">social_desirability&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let’s compare the estimated effect sizes of the self-reported data and the “true” data:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-R" data-lang="R">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Self reports&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">summary&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">lm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">self_report_vote&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">self_report_media_consumption&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Call&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">lm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">formula&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">self_report_vote&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">self_report_media_consumption&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Residuals&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Min&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="n">Q&lt;/span> &lt;span class="n">Median&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="n">Q&lt;/span> &lt;span class="n">Max&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">-3.9604&lt;/span> &lt;span class="m">-0.7766&lt;/span> &lt;span class="m">0.0142&lt;/span> &lt;span class="m">0.8465&lt;/span> &lt;span class="m">4.1811&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Coefficients&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Estimate&lt;/span> &lt;span class="n">Std.&lt;/span> &lt;span class="n">Error&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="n">value&lt;/span> &lt;span class="nf">Pr&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;gt;|&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">|&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">(&lt;/span>&lt;span class="n">Intercept&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="m">0.02020&lt;/span> &lt;span class="m">0.03951&lt;/span> &lt;span class="m">0.511&lt;/span> &lt;span class="m">0.609&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">self_report_media_consumption&lt;/span> &lt;span class="m">0.54605&lt;/span> &lt;span class="m">0.02716&lt;/span> &lt;span class="m">20.102&lt;/span> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="m">2e-16&lt;/span> &lt;span class="o">***&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">---&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Signif.&lt;/span> &lt;span class="n">codes&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0&lt;/span> ‘&lt;span class="o">***&lt;/span>’ &lt;span class="m">0.001&lt;/span> ‘&lt;span class="o">**&lt;/span>’ &lt;span class="m">0.01&lt;/span> ‘&lt;span class="o">*&lt;/span>’ &lt;span class="m">0.05&lt;/span> ‘&lt;span class="n">.’&lt;/span> &lt;span class="m">0.1&lt;/span> ‘ ’ &lt;span class="m">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Residual&lt;/span> &lt;span class="n">standard&lt;/span> &lt;span class="n">error&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">1.248&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="m">998&lt;/span> &lt;span class="n">degrees&lt;/span> &lt;span class="n">of&lt;/span> &lt;span class="n">freedom&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Multiple&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">squared&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.2882&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Adjusted&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">squared&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.2875&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="bp">F&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">statistic&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">404.1&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="m">1&lt;/span> &lt;span class="n">and&lt;/span> &lt;span class="m">998&lt;/span> &lt;span class="n">DF&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">value&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="m">2.2e-16&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># &amp;#34;Truth&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">summary&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">lm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">true_vote&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">true_media_consumption&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Call&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">lm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">formula&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">true_vote&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">true_media_consumption&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Residuals&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Min&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="n">Q&lt;/span> &lt;span class="n">Median&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="n">Q&lt;/span> &lt;span class="n">Max&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">-3.5814&lt;/span> &lt;span class="m">-0.6677&lt;/span> &lt;span class="m">-0.0077&lt;/span> &lt;span class="m">0.6829&lt;/span> &lt;span class="m">3.4799&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Coefficients&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Estimate&lt;/span> &lt;span class="n">Std.&lt;/span> &lt;span class="n">Error&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="n">value&lt;/span> &lt;span class="nf">Pr&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;gt;|&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">|&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">(&lt;/span>&lt;span class="n">Intercept&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="m">0.01372&lt;/span> &lt;span class="m">0.03217&lt;/span> &lt;span class="m">0.426&lt;/span> &lt;span class="m">0.670&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">true_media_consumption&lt;/span> &lt;span class="m">0.01313&lt;/span> &lt;span class="m">0.03245&lt;/span> &lt;span class="m">0.404&lt;/span> &lt;span class="m">0.686&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Residual&lt;/span> &lt;span class="n">standard&lt;/span> &lt;span class="n">error&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">1.017&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="m">998&lt;/span> &lt;span class="n">degrees&lt;/span> &lt;span class="n">of&lt;/span> &lt;span class="n">freedom&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Multiple&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">squared&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.0001639&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Adjusted&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">squared&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">-0.000838&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="bp">F&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">statistic&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.1636&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="m">1&lt;/span> &lt;span class="n">and&lt;/span> &lt;span class="m">998&lt;/span> &lt;span class="n">DF&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">value&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.686&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The self-reported data is biased toward over-estimating the effect size, a very dangerous problem. How could we fix this? Well, one way is to actually measure social desirability and include it in the model:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-R" data-lang="R">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">summary&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">lm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">self_report_vote&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">self_report_media_consumption&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">social_desirability&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Call&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">lm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">formula&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">self_report_vote&lt;/span> &lt;span class="o">~&lt;/span> &lt;span class="n">self_report_media_consumption&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">social_desirability&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Residuals&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Min&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="n">Q&lt;/span> &lt;span class="n">Median&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="n">Q&lt;/span> &lt;span class="n">Max&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">-3.6042&lt;/span> &lt;span class="m">-0.6774&lt;/span> &lt;span class="m">-0.0127&lt;/span> &lt;span class="m">0.6899&lt;/span> &lt;span class="m">3.4470&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Coefficients&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Estimate&lt;/span> &lt;span class="n">Std.&lt;/span> &lt;span class="n">Error&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="n">value&lt;/span> &lt;span class="nf">Pr&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;gt;|&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">|&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">(&lt;/span>&lt;span class="n">Intercept&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="m">0.01208&lt;/span> &lt;span class="m">0.03220&lt;/span> &lt;span class="m">0.375&lt;/span> &lt;span class="m">0.708&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">self_report_media_consumption&lt;/span> &lt;span class="m">0.01220&lt;/span> &lt;span class="m">0.03246&lt;/span> &lt;span class="m">0.376&lt;/span> &lt;span class="m">0.707&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">social_desirability&lt;/span> &lt;span class="m">1.02245&lt;/span> &lt;span class="m">0.04547&lt;/span> &lt;span class="m">22.487&lt;/span> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="m">2e-16&lt;/span> &lt;span class="o">***&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">---&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Signif.&lt;/span> &lt;span class="n">codes&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0&lt;/span> ‘&lt;span class="o">***&lt;/span>’ &lt;span class="m">0.001&lt;/span> ‘&lt;span class="o">**&lt;/span>’ &lt;span class="m">0.01&lt;/span> ‘&lt;span class="o">*&lt;/span>’ &lt;span class="m">0.05&lt;/span> ‘&lt;span class="n">.’&lt;/span> &lt;span class="m">0.1&lt;/span> ‘ ’ &lt;span class="m">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Residual&lt;/span> &lt;span class="n">standard&lt;/span> &lt;span class="n">error&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">1.017&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="m">997&lt;/span> &lt;span class="n">degrees&lt;/span> &lt;span class="n">of&lt;/span> &lt;span class="n">freedom&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Multiple&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">squared&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.5277&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Adjusted&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">squared&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">0.5268&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="bp">F&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">statistic&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="m">557&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="m">2&lt;/span> &lt;span class="n">and&lt;/span> &lt;span class="m">997&lt;/span> &lt;span class="n">DF&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">value&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="m">2.2e-16&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that this while most people think about social desirability as being a problem related to measurement error, it is essentially the same problem as omitted variable bias, as described above.&lt;/p>
&lt;p>It’s important to remember that omitted variable bias and correlated errors are just two potential problems with regression analysis. Regression models are also not immune to issues associated with low levels of &lt;a href="https://en.wikipedia.org/wiki/Power_%28statistics%29" target="_blank" rel="noopener">statistical power&lt;/a>, the failure to account for the influence of extreme values, and &lt;a href="https://en.wikipedia.org/wiki/Heteroscedasticity" target="_blank" rel="noopener">heteroskedasticity&lt;/a>, among others. But by simulating the data-generating process, researchers can get a good sense of some of the more common ways in which statistical models might depart from reality.&lt;/p></description></item><item><title>Replication of 'Bias in the Flesh'</title><link>https://solmessing.netlify.app/post/replication_of_bias_in_the_flesh/</link><pubDate>Mon, 16 Oct 2017 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/post/replication_of_bias_in_the_flesh/</guid><description>&lt;p>This post presents a replication of &lt;a href="https://solmessing.netlify.app/pdf/HSVmetricsCampaignsDarknessPOQFINAL.pdf">Messing et al&lt;/a>. (2016, study 2), which showed that exposure to darker images of Barack Obama increased stereotype activation, as indicated by the tendency to finish incomplete word prompts---such as &amp;ldquo;W E L _ _ _ _&amp;rdquo;---in stereotype-consistent ways (&amp;ldquo;WELFARE&amp;rdquo;).&lt;/p>
&lt;p>Overall, the replication shows that darker images of even &lt;a href="http://www.sciencedirect.com/science/article/pii/S0022103110002635" target="_blank" rel="noopener">counter-stereotypical exemplars like Barack Obama&lt;/a> can increase stereotype activation, but that the strength of the effect is weaker than conveyed in the original study.  A reanalysis of the original study conducted in the course of this replication effort unearthed a number of problems that, when corrected, yield estimates of the effect that are consistent with those documented in the replication. This reanalysis also follows.&lt;/p>
&lt;p>I'm posting this to&lt;/p>
&lt;ol>
&lt;li>disseminate a &lt;a href="https://solmessing.netlify.app/pdf/HSVmetricsCampaignsDarknessPOQFINAL.pdf">corrected version of the original study&lt;/a>;&lt;/li>
&lt;li>show how I found those problems with the original study in the course of conducting this replication;&lt;/li>
&lt;li>circulate these generally confirmatory findings, along with a pooled analysis revealing a stronger effect among conservatives; and&lt;/li>
&lt;li>provide a demonstration of how replication almost always enhances our knowledge about the original research, which I hope may encourage others to invest the time and money in such efforts.&lt;/li>
&lt;/ol>
&lt;p>First some context.&lt;/p>
&lt;p>The original study that formed the basis of the manuscript shows that more negative campaign ads in 2008 were also more likely to contain darker images of President Obama. In 2009 when I started this work, I was most proud of the method to collect data on skin complexion outlined in study 1.  I included another study, what's now study 3, which shows that 2012 ANES survey-takers were more likely to respond negatively to Chinese characters after being presented with darker images of Obama (this is called the &lt;a href="http://onlinelibrary.wiley.com/doi/10.1111/spc3.12148/abstract" target="_blank" rel="noopener">Affect Misattribution Procedure (AMP)&lt;/a>).
But the AMP was not a true experiment and a reviewer was concerned that Study 3 did not provide sufficiently rigorous, causal evidence that darker images alone can cause negative affect.  So I conducted an experiment that would establish a causal link between darker images of Obama and something I thought was even more important---stereotype activation. There were strong reasons to expect this effect based on past lab studies showing links between &lt;a href="https://ase.tufts.edu/psychology/tuscLab/documents/pubsCognitive2002.pdf" target="_blank" rel="noopener">darker skin&lt;/a> and &lt;a href="https://www.ncbi.nlm.nih.gov/pubmed/12088132" target="_blank" rel="noopener">negative stereotypes about Blacks&lt;/a>, and past observational studies showing far more &lt;a href="https://wcfia.harvard.edu/files/wcfia/files/2007_27_hochschild.pdf" target="_blank" rel="noopener">negative socioeconomic outcomes across the board among darker versus lighter skinned Black Americans&lt;/a>. We found an effect and published the three studies.&lt;/p>
&lt;p>This replication effort was prompted by a post-publication reanalysis and &lt;a href="http://www.ljzigerell.com/?p=3622" target="_blank" rel="noopener">critique&lt;/a>, which raised questions about potential weaknesses in the original analysis. My aim in replicating the study was to bring new data to the discussion and make sure we hadn&amp;rsquo;t polluted the literature with a false discovery.&lt;/p>
&lt;p>The main objection was the way we formed our stereotype consistency index. The items assessing stereotype consistency comprised 11 words with missing blank spaces (e.g., L A _ _). Each fragment had as one possible solution a stereotype-related completion. The complete list follows: L A _ _ (LAZY): C R _ _ _ (CRIME); _ _ O R (POOR); R _ _ (RAP); WEL _ _ _ _ (WELFARE); _ _ C E (RACE); D _ _ _ Y (DIRTY); B R _ _ _ _ _ (BROTHER); _ _ A C K (BLACK); M I _ _ _ _ _ _ (MINORITY); D R _ _ (DRUG).&lt;/p>
&lt;p>The author pointed out that there were many potential ways to analyze the original data---he claimed over 16 thousand. Yet very few of these are consistent with generally accepted research practices. We've known, arguably since the &lt;a href="http://www.jstor.org/stable/2333051?origin=crossref&amp;amp;seq=1#page_scan_tab_contents" target="_blank" rel="noopener">16th century&lt;/a>, that combining several measures reduces measurement error and hence variance in estimation. This is particularly important in social science, and especially for this particular study---it would be unwise to attempt to use a single word completion or an arbitrary subset thereof to measure a complex, noisy construct like stereotype activation as measured via a word completion game. Rather, taking the &lt;a href="https://web.stanford.edu/~jrodden/issues_apsr.pdf" target="_blank" rel="noopener">average or constructing an index based on clustering several measures&lt;/a> should be expected to result in far less measurement error, which is what we did.&lt;/p>
&lt;p>Still, I am sympathetic to concerns about the &lt;a href="http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf" target="_blank" rel="noopener">garden of forking paths&lt;/a>, which is part of the motivation for this replication.&lt;/p>
&lt;p>In the original study, I formed this index based on what I judged to be the most unambiguously negative word-completions (lazy, dirty, poor), consistent with past work suggesting that darker complexion activates the &lt;a href="https://ase.tufts.edu/psychology/tuscLab/documents/pubsCognitive2002.pdf" target="_blank" rel="noopener">most &lt;/a>&lt;a href="https://www.ncbi.nlm.nih.gov/pubmed/12088132" target="_blank" rel="noopener">negative&lt;/a> stereotypes about Blacks. I calculated that these were the three variables that also maximized interclass correlation (ICC). As a robustness check, I also computed a measure that maximized alpha reliability (AR). This measure contained more items, and also seemed to include stereotype-consistent word completions that were on balance negative---lazy, dirty, poor, crime, black, and welfare. I should have but did not report results based on a simple average of these items, which was not conclusive.&lt;/p>
&lt;p>&lt;a href="http://www.ljzigerell.com/?p=3622" target="_blank" rel="noopener">The critical reanalysis&lt;/a> cited above shows a handful of statistically significant patterns that are inconsistent with the expectations in the original study, which is suggestive evidence that it's quite possible to find signal in noise if you're analyzing arbitrary sets of variables with the originally collected data. However, as shown below in the much larger replication sample below, none of these patterns replicate.&lt;/p>
&lt;p>The critique also noted that we did not include an analysis of several trailing questions we included on the original survey. The concern is the &lt;a href="http://science.sciencemag.org/content/345/6203/1502" target="_blank" rel="noopener">file drawer problem&lt;/a> - the incentives against and frequent failure to report null results - which obscures knowledge and is bad for the scientific enterprise.&lt;/p>
&lt;p>I included those measures based on &lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1643225" target="_blank" rel="noopener">past work&lt;/a> using the same images as stimuli, which found that darker images prompted more negative evaluations of Obama among people with more negative associations with Blacks, as measured using the Implicit Associations Test (IAT). But testing a specification that conditioned on our main outcome of interest---stereotype-consistent word completions---would mean conditioning on a post-treatment variable, particularly worrisome since we saw an effect on stereotype activation in the study.&lt;/p>
&lt;p>Below, I pool the data and report another specification that does not require us to condition on post-treatment variables. It takes advantage of the fact that conservatives had significantly higher levels of stereotype activation (which was documented in the original study), and shows that the effect is in fact stronger among this subgroup, providing preliminary evidence in favor of this hypothesis.&lt;/p>
&lt;p>The remainder of this post will present my own reanalysis of the original data, the replication, and finally some additional analysis of the data now possible with the larger, pooled data set.&lt;/p>
&lt;p>&lt;strong>Re-analysis of original data&lt;/strong>&lt;/p>
&lt;p>In the process of collecting data for the replication studies, I used the same interface, simply appending the new data as additional respondents completed the survey experiment. When I geo-coded the IP address data in the full data set, I found a discrepancy between the cases I originally geo-coded as U.S. cases, and the cases that now resolved to U.S. locations in the complete data set.  Many of these respondents appeared in sequence, suggesting they may have been skipped, perhaps due to issues related to connectivity to the geo-location server I used.&lt;/p>
&lt;p>This prompted me to conduct a full re-analysis of the data, which yields smaller estimates of stereotype activation. First, re-estimating the index yielded different items---'black' in place of 'dirty' for the ICC measure and 'race' in place of 'welfare', 'crime', and 'dirty' in the AR measure. This is due in part to the way I computed the original indices and in part due to correcting the geo-coding issue. In the original study, I computed the index of variables that maximized alpha and ICC by hand because the epiCalc::alphaBest function (now epiDisplay::alphaBest) does not return results (nor an error message) for these data. For reanalysis, I wrote a function that computed variables to include in the index via successive removal of items. The overall alpha is actually slightly lower in new AR measure, while the new ICC measure has a slightly higher correlation coefficient.&lt;/p>
&lt;p>For the sake of transparency, I first report results based on the original items included in the index as reported in Messing et al. 2015 using the updated data, then report the new ICC and AR measures.&lt;/p>
&lt;p>Using the original indices with the errantly remove cases included, instead of a 36% increase in stereotype-consistent word completions using the ICC measure, this meant a revised estimate of a 20% increase in stereotype activation (M_Light = 0.33, M_Dark = 0.41, T(859.0) = 2.08, P = 0.038, two-sided). For the AR measure, instead of a 13% increase (M_Light = 0.97, M_Dark = 1.11, T(626.72) = 1.77, P = 0.078, two-sided), this meant an 8% increase (M_Light = 0.98, M_Dark = 1.06, T(850.9) = 1.12, P = 0.265, two-sided).&lt;/p>
&lt;p>Re-estimating the indices when including all U.S. cases translates to less conclusive findings---a revised estimate of an 8% increase in stereotype activation in the original study (M_Light = 0.79, M_Dark = 0.86, T(850.7) = 1.27, P = 0.203, two-sided) using the ICC measure, and an 8% increase (M_Light = 0.87, M_Dark = 0.91, T(839.1) = 0.77, P = 0.439, two-sided) using the AR measure.&lt;/p>
&lt;p>A slightly smaller effect was also observed when examining differences between conservatives and other participants. Correcting the geo-coding error and updating the indices reduced the estimate of stereotype activation for conservatives. Instead of a 53% increase, the original ICC measure yields a 29% increase (M_Other = 0.35, M_Conservative = 0.49, T(205.9) = 2.49, P = 0.013, two-sided).  The new ICC measure yields an 18% increase (M_Other = 0.80, M_Conservative = 0.98, T(207.9) = 2.41, P = 0.017, two-sided).  For the AR measure, instead of a 29% increase, this meant an 18% increase using either measure (original: M_Other = 0.99, M_Conservative = 1.18, T(210.4) = 2.11, P = 0.036, two-sided) (new: M_Other = 0.86, M_Conservative = 1.05, T(214.3) = 2.47, P = 0.014, two-sided).&lt;/p>
&lt;p>&lt;strong>The replication&lt;/strong>&lt;/p>
&lt;p>I conducted one exact replication and one very close replication with slightly different images, which I pooled for a total of 3,151 respondents, substantially more than the 630 included in the original writeup.  This gives me more statistical power and more precise estimates of the effect in question. (I provide results for each design separately - one of which appears underpowered - at the end of this post).&lt;/p>
&lt;p>To be clear, I did not pre-register this replication. However, I've tried to err on the side of exhaustive reporting when the original study did not provide exacting specificity in analyzing the new data. Due to the nature of this replication---the presentation of the same analysis conducted in the original study---the p-values provide highly informative, if not conclusive evidence regarding the nature of the effect.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/cb54a931e7c01f3bfd03caa2899518b447ab470e.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>The average reported age was 36; 52% of participants identified as female; 84% identified as White, 8% as Black; 5% as Hispanic; and 3% as Other. 52% identified as liberal, 27% as moderate, 22% as conservative.&lt;/p>
&lt;p>Recomputing the ICC index yielded the following items: black, poor, drug. Recomputing the AR index yielded: lazy, black, poor, welfare, crime, drug, which is close to the original study.&lt;/p>
&lt;p>In the replication data, the ICC yields a 5% increase in stereotype activation (M_Light = 0.90, M_Dark = 0.95, T(3142.8) = 1.56, P = 0.119, two-sided). Similarly, the alpha measure yields a 5% increase (M_Light = 1.04, M_Dark = 1.09, T(3145.8) = 1.70, P = 0.089, two-sided).&lt;/p>
&lt;p>The original study isn't completely clear on the question of whether a replication should report on the recomputed ICC and AP measures, or the exact same items as in the original study, so it's worth reporting those as well. The &lt;em>original&lt;/em> ICC measure yields a 3% increase in stereotype activation (M_Light = 0.36, M_Dark = 0.37, T(3148.9) = 0.51, P = 0.611, two-sided). Using the &lt;em>original&lt;/em> AR measure yields a 6% increase (M_Light = 1.01, M_Dark = 1.07, T(3147.9) = 1.91, P = 0.057, two-sided).&lt;/p>
&lt;p>Finally, it's worth reporting on an index that simply uses all stereotype-consistent items in the replication reveals a 5% increase in stereotype activation (M_Light = 1.30, M_Dark = 1.37, T(3147.7) = 2.05, P = 0.040, two-sided).&lt;/p>
&lt;p>A pooled analysis, after normalizing the ICC and AR measures, yields similar results:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/a0415aa41faee2db8452e14712ae9505f7a96f0a.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;pre>&lt;code>=================================================
ICC Alpha ALL
-------------------------------------------------
(Intercept) -0.041 -0.028 1.292***
(0.034) (0.034) (0.036)
cond: Dark/Light 0.067* 0.058 0.067*
(0.032) (0.032) (0.034)
study 0.006 -0.001 0.008
(0.023) (0.023) (0.025)
-------------------------------------------------
R-squared 0.0 0.0 0.0
N 4012 4012 4012
=================================================
&lt;/code>&lt;/pre>
&lt;p>I also replicated this study with a different, lesser-known Black politician (Jesse White). However, a manipulation check revealed that only 36% of respondents said the candidate was Black in the &amp;ldquo;light&amp;rdquo; condition, compared to 83% in the darker condition, suggesting that any analysis would be severely confounded by perceived race of the target politician. (I did not ask this question in the Barack Obama studies).&lt;/p>
&lt;p>&lt;strong>Additional analysis&lt;/strong>&lt;/p>
&lt;p>The superior power afforded by pooling all three studies may allow the exploration of treatment heterogeneity. &lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1643225" target="_blank" rel="noopener">Past work&lt;/a> suggests the possibility that darker images might cause people inclined toward more stereotype-consistent responses to evaluate politicians more negatively. However, this analysis would &lt;a href="http://andrewgelman.com/2017/09/12/conditioning-post-treatment-variables-can-ruin-experiment/" target="_blank" rel="noopener">condition on post-treatment variables&lt;/a>, which in this case is particularly concerning since the treatment affects stereotype activation according to the original study and replication above.  As an alternative, I consider a specification that uses conservative identification instead, which is a strong predictor of stereotype activation (as shown in the original study), but shouldn&amp;rsquo;t be affected by the treatment. It reveals evidence for the predicted interactions, suggesting that when conservatives are exposed to darker rather than lighter images of Obama, they have slightly &amp;ldquo;colder&amp;rdquo; feelings toward the former president (P = 0.039), perceive him to be less competent (P = 0.061), and less trustworthy (P = 0.083).&lt;/p>
&lt;pre>&lt;code>=================================================================
obama_therm competence trust
-----------------------------------------------------------------
(Intercept) 69.446*** 4.213*** 3.867***
(0.705) (0.029) (0.031)
cond: Dark/Light 0.451 0.024 0.045
(0.989) (0.041) (0.044)
iscons -40.497*** -1.574*** -1.625***
(1.565) (0.064) (0.069)
cond: Dark/Light x iscons -4.462* -0.167 -0.165
(2.160) (0.089) (0.095)
-----------------------------------------------------------------
R-squared 0.3 0.3 0.2
N 3932 3928 3926
=================================================================
&lt;/code>&lt;/pre>
&lt;p>A plot of the model predictions for the thermometer ratings suggests that the effect is concentrated among conservatives.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/f82207e502f7cdd9c8a6b178efa7ca801d1b8e4c.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>&lt;strong>Conclusion&lt;/strong>&lt;/p>
&lt;p>The more items one uses to form an index, the less noise we should expect, and the more likely any replication attempt should be expected to succeed.  It should also mean greater statistical precision. This could explain the remaining discrepancy between this study and the original after adjusting for the geo-coding error pointed out above. It's also possible that something about the timing or the subjects recruited in the replication studies that explain the observed differences.&lt;/p>
&lt;p>Nonetheless, this replication provides evidence that darker images of Black political figures, or at least of President Barack Obama, do in fact activate stereotypes.  This much larger sample suggests that the true effect is smaller than what I found in the original study, which as noted above, contained some errors.&lt;/p>
&lt;p>Replication materials available on &lt;a href="http://dx.doi.org/10.7910/DVN/WY7PR8" target="_blank" rel="noopener">dataverse&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Appendix&lt;/strong>&lt;/p>
&lt;p>Below I present alternate specifications estimated without pooling. These specifications suggest first that replication 2 (as well as the original study) was not well-powered. It also suggests that the outcome measures with more items yield more reliable estimates.&lt;/p>
&lt;p>Outcome measure summing all items:&lt;/p>
&lt;p>Replication 1: M_Light = 1.29, M_Dark = 1.36, T(2115.4) = -1.59, P = 0.113, two-sided&lt;/p>
&lt;p>Replication 2: M_Light = 1.30, M_Dark = 1.39, T(982.7) = -1.35, P = 0.177, two-sided&lt;/p>
&lt;p>Original Alpha outcome measure:&lt;/p>
&lt;p>Replication 1: M_Light = 0.99, M_Dark = 1.06, T(2114.9) = -1.79, P = 0.073, two-sided&lt;/p>
&lt;p>Replication 2: M_Light = 1.04, M_Dark = 1.09, T(979.8) = -0.85, P = 0.393, two-sided&lt;/p>
&lt;p>Newly estimated Alpha outcome measure:&lt;/p>
&lt;p>Replication 1: M_Light = 1.02, M_Dark = 1.09, T(2109.4) = -1.77, P = 0.077, two-sided&lt;/p>
&lt;p>Replication 2: M_Light = 1.07, M_Dark = 1.10, T(984.6) = -0.53, P = 0.597, two-sided&lt;/p>
&lt;p>Original ICC outcome measure:&lt;/p>
&lt;p>Replication 1: M_Light = 0.89, M_Dark = 0.94, T(2100.6) = -1.49, P = 0.137, two-sided.&lt;/p>
&lt;p>Replication 2: M_Light = 0.93, M_Dark = 0.96, T(985.9) = -0.67, P = 0.506, two-sided&lt;/p>
&lt;p>Newly estimated ICC outcome measure&lt;/p>
&lt;p>Replication 1: M_Light = 0.35, M_Dark = 0.36, T(2122.5) = -0.22, P = 0.826, two-sided&lt;/p>
&lt;p>Replication 2: M_Light = 0.38, M_Dark = 0.40, T(978.7) = -0.72, P = 0.472, two-sided
 
A replication of prior &lt;a href="http://www.ljzigerell.com/?p=3622" target="_blank" rel="noopener">critique and reanalysis&lt;/a>.  The patterns that run contrary to our original findings are not significant in the replication data.&lt;/p>
&lt;table style="height:1612px;" width="584">
&lt;tbody>
&lt;tr>
&lt;td>&lt;strong>Variable&lt;/strong>&lt;/td>
&lt;td>&lt;strong>effect size&lt;/strong>&lt;/td>
&lt;td>&lt;strong>p-value&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;span style="font-weight:400;">feeling therm&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">-0.028&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.439&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;span style="font-weight:400;">race minority welfare crime rap&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.022&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.538&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;span style="font-weight:400;">race minority welfare rap&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.017&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.629&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;span style="font-weight:400;">race minority rap&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.011&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.752&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;span style="font-weight:400;">race&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.006&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.861&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;span style="font-weight:400;">minority&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">-0.013&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.713&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;span style="font-weight:400;">rap&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.024&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.502&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;span style="font-weight:400;">welfare&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.014&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.695&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;span style="font-weight:400;">comp&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">-0.013&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.72&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;span style="font-weight:400;">crime&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.016&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.659&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;span style="font-weight:400;">trust&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.002&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.946&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;span style="font-weight:400;">brother&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.042&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.236&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;span style="font-weight:400;">drug&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.008&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.822&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;span style="font-weight:400;">lazy&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.026&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.474&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;span style="font-weight:400;">black&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.084&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.019&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;span style="font-weight:400;">dirty&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.028&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.438&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;span style="font-weight:400;">poor&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">-0.002&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.957&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;span style="font-weight:400;">allwcs&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.073&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.04&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;span style="font-weight:400;">original&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.018&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.611&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;span style="font-weight:400;">alpha&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.068&lt;/span>&lt;/td>
&lt;td>&lt;span style="font-weight:400;">0.057&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>Estimating heterogeneous treatment effects and the effects of heterogeneous treatments with ensemble methods</title><link>https://solmessing.netlify.app/publication/grimmer-2017-estimating/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/grimmer-2017-estimating/</guid><description>&lt;pre>&lt;code>- [Replication materials](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/BQMLQW).
- [Software implementation](https://github.com/SolomonMg/HetSL) (under development)
&lt;/code>&lt;/pre></description></item><item><title>Partisan Conflict and Congressional Outreach</title><link>https://solmessing.netlify.app/publication/solomon-2017-partisan/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/solomon-2017-partisan/</guid><description/></item><item><title>Bias in the flesh: Skin complexion and stereotype consistency in political campaigns</title><link>https://solmessing.netlify.app/publication/messing-2016-bias/</link><pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/messing-2016-bias/</guid><description/></item><item><title>Ideologically diverse news, an agenda for future research</title><link>https://solmessing.netlify.app/post/exposure-to-ideologically-diverse-response/</link><pubDate>Fri, 24 Apr 2015 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/post/exposure-to-ideologically-diverse-response/</guid><description>&lt;p>Earlier this month, we published an early access version of our paper in ScienceExpress (&lt;a href="https://solmessing.netlify.app/pdf/Science-2015-Bakshy-1130-2.pdf">Bakshy et al. 2015&lt;/a>), &amp;ldquo;Exposure to ideologically diverse news and opinion on Facebook.&amp;rdquo; The paper constitutes the first attempt to quantify the extent to which ideologically cross-cutting hard news and opinion is shared by friends, appears in algorithmically ranked News Feeds, and is actually consumed (i.e., click through to read).&lt;/p>
&lt;p>We are grateful for the widespread interest this paper, which grew out of two threads of related research that we began nearly five years ago: Eytan and Lada's work on the role of social networks in information diffusion (&lt;a href="http://arxiv.org/pdf/1201.4145v2.pdf" target="_blank" rel="noopener">Bakshy et al. 2012&lt;/a>) and Sean and Solomon's work on selective exposure in social media (&lt;a href="http://crx.sagepub.com/content/41/8/1042" target="_blank" rel="noopener">Messing and Westwood 2012&lt;/a>).&lt;/p>
&lt;p>While &lt;em>Science&lt;/em> papers are explicitly prohibited from suggesting future directions for research, we would like to shed additional light on our study and raise a few questions that we would be excited to see addressed in future work.&lt;/p>
&lt;p>&lt;strong>Tradeoffs when Selecting a Population&lt;/strong>&lt;/p>
&lt;p>There were tradeoffs when deciding on who to include in this study. While we could have examined all U.S. adults on Facebook, we focused on people who identify as liberals or conservatives and encounter hard news, opinion, and other political content in social media regularly. We did so because many important questions around &amp;ldquo;echo chambers&amp;rdquo; and &amp;ldquo;filter bubbles&amp;quot;on Facebook relate to this subpopulation, and we used self-reported ideological preferences to define it.&lt;/p>
&lt;p>Using self-reported ideological preferences in online profiles is not the only a way to measure ideology or define the population of interest. Yet, people who publicly identify as liberals or conservatives in their Facebook profiles are an interesting and important subpopulation worthy of study for many reasons. As &lt;a href="http://gking.harvard.edu/files/gking/files/words.pdf" target="_blank" rel="noopener">Hopkins and King 2010&lt;/a> have pointed out, studying the expression and behavior of those who are politically engaged online is of interest to political scientists studying activists (&lt;a href="http://www.hup.harvard.edu/catalog.php?isbn=9780674942936" target="_blank" rel="noopener">Verba, Schlozman, and Brady 1995&lt;/a>), the media (&lt;a href="http://195.130.87.21:8080/dspace/handle/123456789/979" target="_blank" rel="noopener">Drezner and Farrell 2004&lt;/a>), public opinion (&lt;a href="https://scholar.google.com/scholar?cluster=15572370959137190849&amp;amp;hl=en&amp;amp;as_sdt=2005&amp;amp;sciodt=0,5" target="_blank" rel="noopener">Gamson 1992&lt;/a>), social networks (&lt;a href="http://dl.acm.org/citation.cfm?id=1134277" target="_blank" rel="noopener">Adamic and Glance 2005&lt;/a>; &lt;a href="http://www.langtoninfo.co.uk/web_content/9780521542234_frontmatter.pdf" target="_blank" rel="noopener">Huckfeldt and Sprague 1995&lt;/a>), and elite influence (&lt;a href="http://press.princeton.edu/titles/8425.html" target="_blank" rel="noopener">Grindle 2005&lt;/a>; &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.160.8347&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="noopener">Hindman, Tsioutsiouliklis, and Johnson 2003;&lt;/a> &lt;a href="https://books.google.com/books?hl=en&amp;amp;lr=&amp;amp;id=83yNzu6toisC&amp;amp;oi=fnd&amp;amp;pg=PR8&amp;amp;dq=The&amp;#43;Nature&amp;#43;and&amp;#43;Origins&amp;#43;of&amp;#43;Mass&amp;#43;Opinion&amp;amp;ots=6oEwiBZtSM&amp;amp;sig=ySnwRZUuVtOyTkC-3raaL88pYns#v=onepage&amp;amp;q=The%20Nature%20and%20Origins%20of%20Mass%20Opinion&amp;amp;f=false" target="_blank" rel="noopener">Zaller 1992&lt;/a>).&lt;/p>
&lt;p>This subpopulation has limitations and is not the only population of interest. The data are not appropriate for those who seek estimates of the entire U.S. public, people without strong opinions, or people not on Facebook (at least not without additional extrapolation, re-weighting, additional evidence, etc.). While our data &lt;em>could plausibly&lt;/em> also provide good estimates of the population of people who are ideologically active and have clear preferences, we are not claiming that's necessarily the case---that remains to be determined in future work.&lt;/p>
&lt;p>We'd like to help other researchers looking to study other populations understand more about the population we've defined. An important question in this regard is what proportion of active U.S. adults actually report an identifiable left/right/center ideology in their profile. That number is 25%, or 10.1 million people.&lt;/p>
&lt;p>It's also informative to examine the proportion of those users who provide identifiable profile affiliations conditional on demographics and Facebook usage:&lt;/p>
&lt;table style="border-collapse:collapse;width:473px;height:512px;" border="0" width="174" cellspacing="0" cellpadding="0">&lt;colgroup> &lt;col style="width:65pt;" span="2" width="87" /> &lt;/colgroup>
&lt;tbody>
&lt;tr style="height:18pt;">
&lt;td class="xl63" style="height:18pt;width:65pt;" width="87" height="24">&lt;strong>Age&lt;/strong>&lt;/td>
&lt;td class="xl63" style="width:65pt;" width="87">&lt;strong>Percent reporting ideological affiliation&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr style="height:18pt;">
&lt;td class="xl63" style="height:18pt;" height="24">18-24&lt;/td>
&lt;td class="xl64">21.60%&lt;/td>
&lt;/tr>
&lt;tr style="height:18pt;">
&lt;td class="xl63" style="height:18pt;" height="24">25-44&lt;/td>
&lt;td class="xl64">28.50%&lt;/td>
&lt;/tr>
&lt;tr style="height:18pt;">
&lt;td class="xl63" style="height:18pt;" height="24">45-64&lt;/td>
&lt;td class="xl64">24.30%&lt;/td>
&lt;/tr>
&lt;tr style="height:18pt;">
&lt;td class="xl63" style="height:18pt;" height="24">65+&lt;/td>
&lt;td class="xl64">21.40%&lt;/td>
&lt;/tr>
&lt;tr style="height:18pt;">
&lt;td class="xl63" style="height:18pt;" height="24">&lt;/td>
&lt;td class="xl63">&lt;/td>
&lt;/tr>
&lt;tr style="height:18pt;">
&lt;td class="xl63" style="height:18pt;" height="24">&lt;strong>Gender&lt;/strong>&lt;/td>
&lt;td class="xl63">&lt;strong>Percent reporting ideological affiliation&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr style="height:18pt;">
&lt;td class="xl63" style="height:18pt;" height="24">Female&lt;/td>
&lt;td class="xl64">21.90%&lt;/td>
&lt;/tr>
&lt;tr style="height:18pt;">
&lt;td class="xl63" style="height:18pt;" height="24">Male&lt;/td>
&lt;td class="xl64">30.60%&lt;/td>
&lt;/tr>
&lt;tr style="height:18pt;">
&lt;td class="xl63" style="height:18pt;" height="24">&lt;/td>
&lt;td class="xl63">&lt;/td>
&lt;/tr>
&lt;tr style="height:18pt;">
&lt;td class="xl63" style="height:18pt;" height="24">&lt;strong>Login Days&lt;/strong>&lt;/td>
&lt;td class="xl63">&lt;strong>Percent reporting ideological affiliation&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr style="height:18pt;">
&lt;td class="xl63" style="height:18pt;" height="24">105-140&lt;/td>
&lt;td class="xl64">18.90%&lt;/td>
&lt;/tr>
&lt;tr style="height:18pt;">
&lt;td class="xl63" style="height:18pt;" height="24">140-185&lt;/td>
&lt;td class="xl64">26.70%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Clearly those who report an ideology in their profile tend to be more active on Facebook. They are also more likely to be men, which is consistent with the well-documented gender gap in American politics (&lt;a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;amp;aid=245226&amp;amp;fileId=S0003055404001315" target="_blank" rel="noopener">Box-Steffensmeier 2004&lt;/a>).&lt;/p>
&lt;p>It's possible that these individuals differ from other Facebook users in other ways. It seems plausible to expect these people to have higher levels of political interest, a stronger sense of political ideology and political identity, and to be more likely to be active in politics than most others on Facebook. It's also possible that these individuals are more extroverted than the average user, especially in the somewhat taboo domain of politics. These possibilities also strike us as interesting questions for study in future work.&lt;/p>
&lt;p>&lt;strong>How to Measure Ideology&lt;/strong>&lt;/p>
&lt;p>We hope others will replicate this work using other populations and ways of measuring ideology, which will provide a broader view of exposure to political media. Data on ideology could be collected by, for example, surveying users, imputing ideology based on user behavior, or joining data to the voter file. Each of these methods have advantages and potential challenges.&lt;/p>
&lt;p>Using surveys in future work would allow researchers to collect data on ideology in a way that can facilitate comparisons with much of the extant literature in political science, and allow researchers to sample from a less politically engaged population. Of course, this could be tricky because survey response rates might be affected by the phenomenon under study. In other words, the salience of political discussion from the right or left, and/or prior choices to consume content could make people more/less likely to respond to a survey asking about ideology, or affect the way they report the strength of their ideological preferences. This could confound measurement in a way that would be difficult to detect and correct. Yet it would be fascinating to see how survey results compare to the results in this study.&lt;/p>
&lt;p>We would also encourage the application of large-scale methods that impute individuals&amp;rsquo; ideological leanings using social networks or revealed preferences. This would have the advantage of allowing researchers to estimate ideological preferences for a broader population, and could be applied to empirical contexts for which self-reported ideological affiliations are not present.&lt;/p>
&lt;p>However, these approaches present challenges. Imputing ideology based on social networks would make it difficult to estimate what proportion of people&amp;rsquo;s networks contain individuals from the other side. &lt;a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;amp;aid=9586211&amp;amp;fileId=S0003055414000525" target="_blank" rel="noopener">Bond and Messing, 2015&lt;/a> and &lt;a href="http://pan.oxfordjournals.org/content/23/1/76.full.pdf?keytype=ref&amp;amp;ijkey=uMFPw4dsMHM7608" target="_blank" rel="noopener">Barberá 2014&lt;/a> discuss some of the challenges related to estimating ideology based on revealed preferences. Another challenge specific to the quantities estimated in our paper is that because behavior may be caused by the composition of individuals&amp;rsquo; social networks, what their friends share, and how they engage with Facebook, using revealed preferences to select the population could introduce endogenous selection bias (&lt;a href="http://www.annualreviews.org/doi/abs/10.1146/annurev-soc-071913-043455" target="_blank" rel="noopener">Elwert and Winship 2014&lt;/a>). A study that negotiates these issues would be a tremendously valuable contribution. Similar methods could also be used to obtain measures of ideological alignment of content.&lt;/p>
&lt;p>Lastly, researchers could use party registration from the voter file. This approach would yield millions of records, but have different selection problems&amp;mdash;match rates may differ by region, state, age, gender, etc. Again, the advantage of approaches like this are that these studies compliment each other and provide a fuller picture of how exposure to viewpoints from the other side occur in social media.&lt;/p>
&lt;p>Future work should also examine how exposure varies in different subpopulations. For example, one hypothesis to test is whether those with weaker or less consistent ideological preferences have more cross cutting content shared by friends, rendered in social media streams, and selected for reading. Some preliminary analysis suggests that indeed, among the individuals in our study, those with a weaker stated ideological affiliation have on average more cross-cutting content at each stage in the exposure process.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/5a469dbbfb5b2916a87554ca4a247579da11e2b2.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>&lt;strong>Other Data Sources&lt;/strong>&lt;/p>
&lt;p>There are many other important questions related to this paper that necessitate new data sources: Does encountering cross-cutting content increase or decrease attitude polarization? What about attitudes toward members of the other side? Does it change specific policy preferences? Are liberals and conservatives more or less likely to see content in News Feed &lt;em>because&lt;/em> it was cross-cutting? Do they actively avoid cross-cutting political content &lt;em>because&lt;/em> of expressions in the title or because of the fact that the media source is suggestive of a cross-cutting article? How do changes to ranking algorithms and user interfaces affect selective exposure? And how can we better understand actual discourse about politics in social media, rather than merely shared media content?&lt;/p>
&lt;p>Answering these questions necessitates collecting innovative data sets via online experimentation (&lt;a href="http://pan.oxfordjournals.org/content/20/3/351.short" target="_blank" rel="noopener">Berinsky et al. 2012&lt;/a>), social media (&lt;a href="http://scholar.harvard.edu/dtingley/files/fall2012.pdf" target="_blank" rel="noopener">Ryan and Broockman 2012&lt;/a>), crowdsourcing (&lt;a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2526461" target="_blank" rel="noopener">Budak et al. 2014&lt;/a>), large scale field experimentation (&lt;a href="http://gking.harvard.edu/publications/randomized-Experimental-Study-Censorship-China" target="_blank" rel="noopener">King et al. 2014&lt;/a>), observational social media data, clever ways to collect data about individual differences in ranking (&lt;a href="http://www.lazerlab.net/sites/default/files/publications/Measuring%20Personalization%20of%20Web%20Search.pdf" target="_blank" rel="noopener">Hannak et al. 2013&lt;/a>), smart ways to combine behavioral and survey data (&lt;a href="http://arxiv.org/pdf/1304.1837v3.pdf" target="_blank" rel="noopener">Chen et al. 2014&lt;/a>), and panel data (&lt;a href="http://faculty-gsb.stanford.edu/athey/documents/localnews.pdf" target="_blank" rel="noopener">Athey and Mobius 2012&lt;/a>, &lt;a href="https://5harad.com/papers/bubbles.pdf" target="_blank" rel="noopener">Flaxman et al. 2014&lt;/a>).&lt;/p>
&lt;p>Many of these are causal questions necessitating experimental and/or quasi experimental designs. For example, the extent to which people select content because it is cross-cutting could be investigated using experiments like this one (e.g., &lt;a href="http://crx.sagepub.com/content/41/8/1042" target="_blank" rel="noopener">Messing and Westwood 2012&lt;/a>) or through identifying sources of natural exogenous variation. And while Diana Mutz and others have done ground-breaking research on the effects of encountering cross-cutting arguments on political attitudes (&lt;a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;amp;aid=208463&amp;amp;fileId=S0003055402004264" target="_blank" rel="noopener">Mutz 2002&lt;/a>b) and behavior (&lt;a href="http://www.jstor.org/stable/3088437" target="_blank" rel="noopener">Mutz 2002&lt;/a>a), more research into how these effects play out in the long term (using approaches like &lt;a href="http://faculty.wcas.northwestern.edu/~jnd260/pub/Druckman%20Fein%20Leeper%20APSR.pdf" target="_blank" rel="noopener">Druckman et al 2012&lt;/a>) would be of tremendous benefit to the literature. It is difficult to expose people to any sort of argument for a long period of time (say over the course of a U.S. national political campaign cycle), in a way that is not confounded with people's existing preferences and the social environment, though creative quasi-experimental work (&lt;a href="http://web.stanford.edu/~ayurukog/cable_news.pdf" target="_blank" rel="noopener">Martin and Yurukoglu 2014&lt;/a>) is emerging in this area.&lt;/p>
&lt;p>Many of these questions necessitate that researchers identify the effects of cross-cutting arguments both on and off Facebook. To get a full picture of how cross-cutting arguments affect politics requires understanding the myriad of ways individuals get information, both on the Internet (&lt;a href="https://5harad.com/papers/bubbles.pdf" target="_blank" rel="noopener">Flaxman et al. 2014&lt;/a>) and offline (&lt;a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;amp;aid=208463&amp;amp;fileId=S0003055402004264" target="_blank" rel="noopener">Mutz 2002&lt;/a>a), what kinds of information people discuss in offline contexts (&lt;a href="http://dx.doi.org/10.1017/S0003055402004264" target="_blank" rel="noopener">Mutz 2002&lt;/a>b), and the relative influence of all of these factors on opinions.&lt;/p>
&lt;p>Finally, if individuals' online networks and choices do substantially impact the diversity of news in individuals' overall &amp;ldquo;information diets,&amp;rdquo; future research could examine the effects of connecting those with more disparate views (&lt;a href="http://ajps.org/2015/03/11/partisanship-in-social-settings-when-democrats-and-republicans-meet/" target="_blank" rel="noopener">Klar 2014&lt;/a>), encouraging consumption of cross-cutting content (&lt;a href="http://www.smunson.com/portfolio/projects/socnews_icwsm15.pdf" target="_blank" rel="noopener">Agapie and Munson 2015&lt;/a>), or simply encouraging individuals to read more diverse news by making individuals more aware of the balance of news they consume (&lt;a href="http://www.smunson.com/portfolio/projects/aggdiversity/balancer-icwsm.pdf" target="_blank" rel="noopener">Munson et al. 2013&lt;/a>).&lt;/p>
&lt;p>These questions are especially important in light of the fact that there are substantial opportunities for people to read more news on Facebook. The plots below illustrate the average proportion of stories shared by friends, those that are seen in News Feed, and those clicked on for liberals and conservatives in the study. Clearly there is an opportunity to read more news from either side.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/ddb611f4a223213b3a653208e5bdd048d6b9cd94.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>&lt;strong>Dataverse&lt;/strong>&lt;/p>
&lt;p>Finally, we believe that reproducing, replicating, and conducting additional analyses on extant data sets is extremely important and helps generate ideas for future work (&lt;a href="http://gking.harvard.edu/files/abs/replication-Abs.shtml" target="_blank" rel="noopener">King 1995&lt;/a>, &lt;a href="http://thomasleeper.com/2015/05/open-science-language/" target="_blank" rel="noopener">Leeper 2015&lt;/a>). In that spirit, we have created a &lt;a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/AAI7VA" target="_blank" rel="noopener">Dataverse archive&lt;/a>. The repository includes replication data, scripts, as well as some additional supplementary data and code for extending our work.&lt;/p>
&lt;p>&lt;strong>References&lt;/strong>&lt;/p>
&lt;p>E. Bakshy, S. Messing, L.A. Adamic. 2015. Exposure to ideologically diverse news and opinion on Facebook. &lt;em>Science&lt;/em>.&lt;/p>
&lt;p>E. Bakshy, I. Rosenn, C.A. Marlow, L.A. Adamic. 2012. The Role of Social Networks in Information Diffusion. &lt;em>ACM WWW 2012.&lt;/em>&lt;/p>
&lt;p>S. Messing and S.J. Westwood. 2012. Selective Exposure in the Age of Social Media: Endorsements Trump Partisan Source Affiliation When Selecting News Online. &lt;em>Communication Research&lt;/em>.&lt;/p>
&lt;p>P. Barberá (2015). Birds of the same feather tweet together: Bayesian ideal point estimation using Twitter data. &lt;em>Political Analysis&lt;/em>, &lt;em>23&lt;/em>(1), 76-91.&lt;/p>
&lt;p>R. Bond, S. Messing, Quantifying Social Media&amp;rsquo;s Political Space: Estimating Ideology from Publicly Revealed Preferences on Facebook. &lt;em>American Political Science Review&lt;/em>&lt;/p>
&lt;p>F. Elwert and C. Winship. 2014. Endogenous Selection Bias: The Problem of Conditioning on a Collider Variable. &lt;em>Annual Review of Sociology.&lt;/em>&lt;/p>
&lt;p>G. King, J. Pan, and M. E. Roberts. 2014. Reverse-Engineering Censorship in China: Randomized Experimentation and Participant Observation. &lt;em>Science&lt;/em>.&lt;/p>
&lt;p>C. Budak, S. Goel, &amp;amp; J. M. Rao. (2014). Fair and Balanced? Quantifying Media Bias Through Crowdsourced Content Analysis. &lt;em>Quantifying Media Bias Through Crowdsourced Content Analysis (November 17, 2014)&lt;/em>.&lt;/p>
&lt;p>S. Athey, M. Mobius. The Impact of News Aggregators on Internet News Consumption: The Case of Localization. Working paper. &lt;a href="http://faculty-gsb.stanford.edu/athey/documents/localnews.pdf" target="_blank" rel="noopener">http://faculty-gsb.stanford.edu/athey/documents/localnews.pdf&lt;/a>&lt;/p>
&lt;p>A. Hannak, P. Sapiezynski, A. Molavi Kakhki, B. Krishnamurthy, D. Lazer, A. Mislove, C. Wilson. 2013. Measuring personalization of web search. &lt;em>ACM WWW 2013&lt;/em>.&lt;/p>
&lt;p>A. Chen and A. Owen and M. Shi. Data Enriched Linear Regression. Working paper. &lt;a href="http://arxiv.org/pdf/1304.1837v3.pdf" target="_blank" rel="noopener">http://arxiv.org/pdf/1304.1837v3.pdf&lt;/a>&lt;/p>
&lt;p>G.J. Martin, A. Yurukoglu. Working paper. Bias in Cable News: Real Effects and Polarization. Working paper. &lt;a href="http://web.stanford.edu/~ayurukog/cable_news.pdf" target="_blank" rel="noopener">http://web.stanford.edu/~ayurukog/cable_news.pdf&lt;/a>&lt;/p>
&lt;p>S.R. Flaxman, S. Goel, J.M. Rao. Filter Bubbles, Echo Chambers, and Online News Consumption. Working paper. &lt;a href="https://5harad.com/papers/bubbles.pdf" target="_blank" rel="noopener">https://5harad.com/papers/bubbles.pdf&lt;/a>&lt;/p>
&lt;p>D.C. Mutz. 2002. The Consequences of Cross-Cutting Networks for Political Participation. &lt;em>American Journal of Political Science&lt;/em>.&lt;/p>
&lt;p>D.C. Mutz. 2002. Cross-cutting Social Networks: Testing Democratic Theory in Practice. &lt;em>American Political Science Review&lt;/em>.&lt;/p>
&lt;p>J. N. Druckman, J. Fein, &amp;amp; T. Leeper. 2012. A source of bias in public opinion stability. &lt;em>American Political Science Review&lt;/em>.&lt;/p>
&lt;p>E. Agapie, S.A. Munson. 2015. &amp;ldquo;&lt;a href="http://smunson.com/portfolio/projects/socnews_icwsm15.pdf" target="_blank" rel="noopener">Social Cues and Interest in Reading Political News Stories&lt;/a>.&amp;rdquo; &lt;em>AAAI ICWSM 2015&lt;/em>.&lt;/p>
&lt;p>S. Klar. 2014. Partisanship in a Social Setting. &lt;em>American Journal of Political Science&lt;/em>.&lt;/p>
&lt;p>S.A. Munson, S.Y. Lee, P. Resnick. 2013. Encouraging Reading of Diverse Political Viewpoints with a Browser Widget. &lt;em>AAAI ICWSM 2013&lt;/em>.&lt;/p>
&lt;p>G. King. 1995. &amp;ldquo;Replication, Replication.&amp;rdquo; &lt;em>Political Science and Politics&lt;/em>. &lt;a href="http://j.mp/1wP9Vqn" target="_blank" rel="noopener">http://j.mp/1wP9Vqn&lt;/a>&lt;/p>
&lt;p>T. Leeper. 2015. What's in a Name? The Concepts and Language of Replication and Reproducibility. Blog post. &lt;a href="http://thomasleeper.com/2015/05/open-science-language/" target="_blank" rel="noopener">http://thomasleeper.com/2015/05/open-science-language/&lt;/a>&lt;/p></description></item><item><title>Exposure to ideologically diverse news and opinion on Facebook</title><link>https://solmessing.netlify.app/publication/bakshy-2015-exposure/</link><pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/bakshy-2015-exposure/</guid><description>&lt;ul>
&lt;li>&lt;a href="http://science.sciencemag.org/content/348/6239/1090" target="_blank" rel="noopener">Review by David Lazer&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://science.sciencemag.org/highwire/filestream/630053/field_highwire_adjunct_files/1/Bakshy-SM.revision.1.pdf" target="_blank" rel="noopener">Supplementary materials&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://dx.doi.org/10.7910/DVN/LDJ7MS" target="_blank" rel="noopener">Replication materials&lt;/a>&lt;/li>
&lt;li>Media: &lt;a href="http://www.nytimes.com/2015/05/08/technology/facebook-study-disputes-theory-of-political-polarization-among-users.html?_r=0" target="_blank" rel="noopener">New York Times&lt;/a>,
the &lt;a href="http://www.washingtonpost.com/news/energy-environment/wp/2015/05/07/facebook-study-says-its-mainly-your-fault-not-theirs-that-you-read-things-you-already-agree-with/" target="_blank" rel="noopener">Washington Post&lt;/a>, the &lt;a href="http://www.bbc.com/news/science-environment-32606724" target="_blank" rel="noopener">BBC&lt;/a>, &lt;a href="http://www.cbsnews.com/news/facebooks-news-feed-limits-your-world-view/" target="_blank" rel="noopener">CBS&lt;/a>, &lt;a href="http://www.huffingtonpost.com/2015/05/12/facebook-study-polarization_n_7245192.html" target="_blank" rel="noopener">Huffington Post&lt;/a>, &lt;a href="http://arstechnica.com/science/2015/05/dont-just-blame-facebook-we-build-our-own-bubbles/" target="_blank" rel="noopener">Ars Technica&lt;/a>, &lt;a href="http://www.wired.co.uk/news/archive/2015-05/08/facebook-echo-chamber-study" target="_blank" rel="noopener">Wired
UK&lt;/a>,
&lt;a href="http://www.theverge.com/2015/5/7/8564795/facebook-online-opinion-filter-bubble-news-feed-study" target="_blank" rel="noopener">The Verge&lt;/a>.&lt;/li>
&lt;/ul></description></item><item><title>Quantifying social media's political space: Estimating ideology from publicly revealed preferences on Facebook</title><link>https://solmessing.netlify.app/publication/bond-2015-quantifying/</link><pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/bond-2015-quantifying/</guid><description/></item><item><title>When to Use Stacked Barcharts?</title><link>https://solmessing.netlify.app/post/when-to-use-stacked-barcharts/</link><pubDate>Sat, 11 Oct 2014 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/post/when-to-use-stacked-barcharts/</guid><description>&lt;p>Yesterday a few of us on Facebook&amp;rsquo;s Data Science Team released a &lt;a href="https://www.facebook.com/notes/10152581594083859/" target="_blank" rel="noopener">blogpost showing how candidates are campaigning on Facebook in the 2014 U.S. midterm elections&lt;/a>. It was &lt;a href="http://www.washingtonpost.com/blogs/govbeat/wp/2014/10/10/how-candidates-use-facebook-motivation-more-than-persuasion/" target="_blank" rel="noopener">picked up in the Washington Post&lt;/a>, in which &lt;a href="http://www.washingtonpost.com/people/reid-wilson" target="_blank" rel="noopener">Reid Wilson&lt;/a> calls us &amp;quot;data wizards.&amp;quot; Outstanding.&lt;/p>
&lt;p>I used &lt;a href="http://had.co.nz/" target="_blank" rel="noopener">Hadly Wickham's&lt;/a> ggplot2 for every visualization in the post except a map that &lt;a href="http://web.stanford.edu/~arjunw/" target="_blank" rel="noopener">Arjun Wilkins&lt;/a> produced using D3, and for the first time I used stacked bar charts.  Now as I've stated previously, &lt;a href="https://solmessing.netlify.app/post/visualization-series-insight-from-cleveland-and-tufte-on-plotting-numeric-data-by-groups">one should generally avoid bar charts, and especially stacked bar charts&lt;/a>, except in a few specific circumstances.&lt;/p>
&lt;p>But let's talk about when not to use stacked bar charts first---I had the pleasure of chatting with Kaiser Fung of &lt;a href="http://junkcharts.typepad.com/" target="_blank" rel="noopener">JunkCharts&lt;/a> fame the other day, and I think what makes his site so compelling is the mix of schadenfreude and &lt;a href="http://betterthanenglish.com/fremdscham-german/" target="_blank" rel="noopener">Fremdscham&lt;/a> that makes taking apart someone else's mistake such an effective teaching strategy and such a memorable read. I also appreciate the subtle nod to &lt;a href="https://en.wikipedia.org/wiki/Found_object" target="_blank" rel="noopener">junk art&lt;/a>.&lt;/p>
&lt;p>Here's a typical, terrible stacked bar chart, which I found on &lt;a href="http://www.storytellingwithdata.com/" target="_blank" rel="noopener">http://www.storytellingwithdata.com/&lt;/a> and originally published on a &lt;a href="http://blogs.wsj.com/digits/2012/10/22/microsoft-windows-8-forrester/" target="_blank" rel="noopener">Wall Street Journal blogpost&lt;/a>. It shows the share of the personal computing device market by operating system, over time. The problem with using a stacked bar chart is that there are only two common baselines for comparison (the top and bottom of the plotting area), but we are interested in the relative share for more than two OS brands. The post is really concerned with Microsoft, so one solution would be to plot Microsoft versus the rest, or perhaps Microsoft on top versus Apple on the bottom with &amp;quot;Other&amp;quot; in the middle. Then we'd be able to compare the over time market share for Apple and Microsoft. As the author points out, an over time trend can also be visualized with line plots.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/b5e194c5114b79478a7ffcf600b18cd205a3a1b7.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>By far the worst offender I found in my 5 minute Google search was &lt;a href="http://junkcharts.typepad.com/junk_charts/2014/08/one-guaranteed-to-make-stephen-few-cry-.html" target="_blank" rel="noopener">from junkcharts&lt;/a> and originally published on &lt;a href="http://www.vox.com/2014/7/28/5944065/electric-cars-plug-in-vehicles-rising-sales-US" target="_blank" rel="noopener">Vox&lt;/a>. These cumulative sum plots are so bad I was surprised to see them still up. The first problem is that the plots represent an attempt to convey way too much information---either plot total sales or pick a few key brands that are most interesting and plot them on a multi-line chart or set of faceted time series plots. The only brand for which you can quickly get a sense of sales over time is the Chevy Volt because it's on the baseline. I'm sure the authors wanted to also convey the proportion of sales each year, but if you want to do that just plot the relative sales. Of course, the order in which the bars appear on the plot has no organizing principle, and you need to constantly move your eyes back and forth from the legend to the plot when trying to make sense of this monstrosity.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/a1281b5624937d4bf8069706e74028aeb8f9952d.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>As Kaiser notes in his post, less is often more. Here's his redux, which uses lines and aggregates by both quarter and brand, resulting in a far superior visualization:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/df59a98319dbf363b90514243d0ded9d9aa191ce.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>So when *should* you use a stacked bar chart? Here are a two scenarios with examples, inspired by work with &lt;a href="http://eytan.github.io/" target="_blank" rel="noopener">Eytan Bakshy&lt;/a> and conversations with &lt;a href="http://ta.virot.me/" target="_blank" rel="noopener">Ta Chiraphadhanakul&lt;/a> and &lt;a href="http://www.johnmyleswhite.com/" target="_blank" rel="noopener">John Myles White&lt;/a>.&lt;/p>
&lt;p>1. You care about comparing the proportion of two things, in this case the share of posts by Democrats and Republicans, along a variety of dimensions.  In this case those dimensions consist of keyword (dictionary-based) categories (above) and LDA topics (below).  When these are sorted by relative proportion, the reader gains insight into which campaign strategies and issues are used more by Republican or Democratic candidates.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/b16c48bc6a47363ced0e685cb506df99e15e0a73.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;ol start="2">
&lt;li>You care about comparing proportions along an ordinal, additive variable such as 5-point party identification, along a set of dimensions.  I provide an example from a forthcoming paper below (I'll re-insert the axis labels once it's published).  Notice that it draws the reader toward two sets of comparisons across dimensions -- one for strong democrats and republicans, the other for the set of *all* Democrats and *all* Republicans.&lt;/li>
&lt;/ol>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/312fb516b818733414acbe395ded53b8424f37a2.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Of course, R code to produce these plots follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Uncomment these lines and install if necessary:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#install.packages(&amp;#39;ggplot2&amp;#39;)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#install.packages(&amp;#39;dplyr&amp;#39;)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#install.packages(&amp;#39;scales&amp;#39;)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ggplot2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dplyr&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">scales&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># We start with the raw number of posts for each party for&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># each candidate. Then we compute the total by party and&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># category.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">catsByParty&lt;/span> &lt;span class="o">%&amp;gt;%&lt;/span> &lt;span class="nf">group_by&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">party&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">all_cats&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">%&amp;gt;%&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">summarise&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tot&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">summ&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">posts&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Next, compute the proportion by party for each category&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># using dplyr::mutate&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">catsByParty&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="n">catsByParty&lt;/span> &lt;span class="o">%&amp;gt;%&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">group_by&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">all_cats&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">%&amp;gt;%&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">mutate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">prop&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tot&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="nf">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tot&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Now compute the difference by category and order the&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># categories by that difference:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">catsByParty&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="n">catsByParty&lt;/span> &lt;span class="o">%&amp;gt;%&lt;/span> &lt;span class="nf">group_by&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">all_cats&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">%&amp;gt;%&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">mutate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pdiff&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">diff&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">prop&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">catsByParty&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">all_cats&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">reorder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">catsByParty&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">all_cats&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="n">catsByParty&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">pdiff&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># And plot:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">ggplot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">catsByParty&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">aes&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">all_cats&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">prop&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">fill&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">party&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">scale_y_continuous&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">labels&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">percent_format&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">geom_bar&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">stat&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#39;identity&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">geom_hline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">yintercept&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">.5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">linetype&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#39;dashed&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">coord_flip&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">theme_bw&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">ylab&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;Democrat/Republican share of page posts&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">xlab&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">scale_fill_manual&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">values&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;blue&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;red&amp;#39;&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">theme&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">legend.position&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#39;none&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">ggtitle&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;Political Issues Discussed by Party\n&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Selective exposure in the age of social media: Endorsements trump partisan source affiliation when selecting news online</title><link>https://solmessing.netlify.app/publication/messing-2014-selective/</link><pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/messing-2014-selective/</guid><description/></item><item><title>The Impression of Influence: Legislator Communication, Representation, and Democratic Accountability</title><link>https://solmessing.netlify.app/publication/gri-mes-wes-14/</link><pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/gri-mes-wes-14/</guid><description>&lt;ul>
&lt;li>Media: &lt;a href="http://www.mischiefsoffaction.com/2015/01/its-frequency-not-size-compromise.html" target="_blank" rel="noopener">Mischiefs of Faction&lt;/a>.&lt;/li>
&lt;/ul></description></item><item><title>Do attitudes about immigration predict willingness to admit individual immigrants? A cross-national test of the person-positivity bias</title><link>https://solmessing.netlify.app/publication/iyengar-2013-attitudes/</link><pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/iyengar-2013-attitudes/</guid><description/></item><item><title>Evaluation of the novel USPIO GEH121333 for MR imaging of cancer immune responses</title><link>https://solmessing.netlify.app/publication/shi-2013-evaluation/</link><pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/shi-2013-evaluation/</guid><description/></item><item><title>Ferumoxytol: a new, clinically applicable label for stem-cell tracking in arthritic joints with MRI</title><link>https://solmessing.netlify.app/publication/khurana-2013-ferumoxytol/</link><pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/khurana-2013-ferumoxytol/</guid><description/></item><item><title>Friends that Matter: How Social News Shapes Political Knowledge, Attitudes, and Behavior</title><link>https://solmessing.netlify.app/publication/messing-2013-friends/</link><pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/messing-2013-friends/</guid><description>&lt;p>&lt;a href="https://www.dropbox.com/s/n0x4iepyj9pzwiw/CH7brief.pdf?raw=true" target="_blank" rel="noopener">https://www.dropbox.com/s/n0x4iepyj9pzwiw/CH7brief.pdf?raw=true&lt;/a>&lt;/p></description></item><item><title>Iron administration before stem cell harvest enables MR imaging tracking after transplantation</title><link>https://solmessing.netlify.app/publication/khurana-2013-iron/</link><pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/khurana-2013-iron/</guid><description/></item><item><title>Role of diffusion-weighted imaging in differentiating benign and malignant pediatric abdominal tumors</title><link>https://solmessing.netlify.app/publication/gawande-2013-role/</link><pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/gawande-2013-role/</guid><description/></item><item><title>Insight From Cleveland And Tufte On Plotting Numeric Data By Groups</title><link>https://solmessing.netlify.app/post/visualization-series-insight-from-cleveland-and-tufte-on-plotting-numeric-data-by-groups/</link><pubDate>Sun, 04 Mar 2012 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/post/visualization-series-insight-from-cleveland-and-tufte-on-plotting-numeric-data-by-groups/</guid><description>&lt;p>After my post on making &lt;a href="http://solomonmessing.wordpress.com/2011/11/26/putting-it-all-together-concise-code-to-make-dotplots-with-weighted-bootstrapped-standard-errors/" target="_blank" rel="noopener">dotplots with concise code using plyr and ggplot&lt;/a>, I got an email from my dad who practices immigration law and runs a &lt;a href="http://www.messinglawoffices.com/default.aspx" target="_blank" rel="noopener">website with a variety of immigration resources and tools&lt;/a>.  He pointed out that the post was written for folks who already know that they want to make dot plots, and who already know about bootstrapped standard errors.  That&amp;rsquo;s not many people.&lt;/p>
&lt;p>In an attempt to appeal to a broader audience, I&amp;rsquo;m starting a series in which I&amp;rsquo;ll outline the key principles I use when developing a visualization.  In this post, I&amp;rsquo;ll articulate these principles, which combine some of Tuft&amp;rsquo;s aesthetic guidelines with Cleveland&amp;rsquo;s scientific approach to visualization, which is based on the psychological processes involved in making sense of visualizations, and has been rigorously tested via randomized controlled experiments.  Based on these principles, I&amp;rsquo;ll argue that dotplots and scatterplots are better than other types of plots (especially pie charts) in most situations.  In later posts, I&amp;rsquo;ll demonstrate another innovation whose widespread use I&amp;rsquo;ll credit to Cleveland and Tufte: the use of multiple panels (aka small multiples, trellis graphics, facets, generalized draftsman&amp;rsquo;s displays, multivar charts) to clearly convey the same information embedded in more complex and difficult to read visualizations, including multiple line plots and mosaic plots. In future posts I&amp;rsquo;ll also emphasize why it is important to provide some indication of the noise present in the underlying data using error bars or bands.  Along the way, I&amp;rsquo;ll put you to the test&amp;ndash;I&amp;rsquo;ll present some visualizations of the same data using different visualization techniques and ask you to try to get as much information as you can in 2 seconds from each type of visualization.&lt;/p>
&lt;p>A good visualization conveys key information to those who may have trouble interpreting numbers and/or statistics, which can make your findings accessible to a wider audience (more on this below).  Visualizations also give your audience a break from lexical processing, which is especially useful when you are presenting your findings&amp;ndash;people can listen to you and process the findings from a well-designed visual at the same time, but most people have trouble listening while reading your PowerPoint bullet points.  Visualizations also convey key information embedded in massive amounts of data, which can aid your own exploratory analysis of data, no matter how massive.&lt;/p>
&lt;p>Yet most visualizations are flawed, drawn using elements that make it unnecessarily difficult for the human visual system to make sense of things.  I see a lot of these visualizations attending research presentations, screening incoming draft manuscripts as the assistant editor for &lt;a href="http://www.tandf.co.uk/journals/upcp" target="_blank" rel="noopener">Political Communication&lt;/a>, and as a consumer of media info-graphics (CNN is especially bad, have a look at &lt;a href="http://tech.fortune.cnn.com/tag/pie-chart/" target="_blank" rel="noopener">this monstrosity&lt;/a>).  Kevin Fox has an &lt;a href="http://fury.com/2010/03/why-3d-pie-charts-are-bad/" target="_blank" rel="noopener">especially compelling visual speaking to this here&lt;/a>. A big part of the problem is that Microsoft makes it easy to draw flashy but ultimately confusing visualizations in Excel.  If you are too busy to read this post in full, follow this short list of guidelines and you&amp;rsquo;ll be on your way to producing elegant visualizations that impose a minimal cognitive burden on your audience:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Never represent something in 2 or &lt;a href="http://www.psdgraphics.com/wp-content/uploads/2009/02/3d_pie_chart.jpg" target="_blank" rel="noopener">worse yet 3 dimensions&lt;/a> if it can be represented in one&amp;mdash;NEVER use pie charts, 3-D pie charts, stacked bar charts, or 3-D bar charts.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Remove as much chart junk as possible&amp;ndash;unnecessary gridlines, shading, borders, etc.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Give your audience a sense of the noise present in your data&amp;ndash;draw error bars or confidence bands if you are plotting estimates.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If you want to plot multiple types of groups on a single outcome (the visual analog of cross-tabulations/marginals), use &lt;a href="http://solomonmessing.files.wordpress.com/2011/11/trtbypid2.png?w=640" target="_blank" rel="noopener">multi-paneled plots&lt;/a>. These can also help if &lt;a href="http://www.bo.astro.it/~eps/buz10503/ff08.jpg" target="_blank" rel="noopener">overploting looks too cluttered&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Avoid mosaic plots. Instead use &lt;a href="http://wiki.stdout.org/rcookbook/Graphs/Facets%20%28ggplot2%29?action=AttachFile&amp;amp;do=get&amp;amp;target=hp_sex_smoker_free_free.png" target="_blank" rel="noopener">paneled histograms&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ditch the legend if you can (you almost always can).&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>The rest of the content in this series emphasizes why it makes sense to follow these guidelines. In this post I&amp;rsquo;ll look at the first point in detail and touch on the sixth. These two guidelines are most relevant when you want to look at a quantitative variable (e.g., earnings, vote-share, temperature, etc.) across different qualitative groupings (e.g., industry segment, candidate, party, racial group, season, etc.).  This is one of the most common visualization tasks in business, media, and social science, and for this task people often use pie charts and/or bar charts, and occasionally dot plots.&lt;/p>
&lt;p>&lt;strong>The science of graphical perception&lt;/strong>&lt;/p>
&lt;p>When most people think about visualization, they think first of &lt;a href="http://www.edwardtufte.com/tufte/" target="_blank" rel="noopener">Edward Tufte&lt;/a>.  Tufte emphasizes integrity to the data, showing relationships between phenomena, and above all else aesthetic minimalism.  I appreciate his ruthless crusade against &lt;a href="http://chartjun%20k.karmanaut.com/" target="_blank" rel="noopener">chart junk&lt;/a> and &lt;a href="http://jakeporway.com/2011/08/data-without-borders-logo-contest/" target="_blank" rel="noopener">pie charts (nice quote from Data without Borders)&lt;/a>. We share an affinity for multipanel plotting approaches, which he calls &amp;ldquo;small multiples,&amp;rdquo; (thanks to &lt;a href="http://www.stanford.edu/~rjweiss/" target="_blank" rel="noopener">Rebecca Weiss&lt;/a> for pointing this out) though I think people give Tufte too much credit for their invention&amp;mdash;both &lt;a href="http://www.juiceanalytics.com/writing/better-know-visualization-small-multiples/" target="_blank" rel="noopener">juiceanalytics&lt;/a> and &lt;a href="http://www.infovis-wiki.net/index.php/Small_Multiples" target="_blank" rel="noopener">infovis-wiki&lt;/a> write that Cleveland introduced the concept/principle. However, both Cleveland and Tufte published books in 1983 discussing the use of multipanel displays; &lt;a href="http://blog.revolutionanalytics.com/2011/11/small-multiples-of-the-sky.html" target="_blank" rel="noopener">David Smith over at Revolutions&lt;/a> writes that &amp;ldquo;the &amp;ldquo;small-multiples&amp;rdquo; principle of data visualization [was] pioneered by Cleveland and popularized in Tufte&amp;rsquo;s first book&amp;rdquo;; and the earliest reference to a work containing multipanel displays I could find was published *long* before Tufte&amp;rsquo;s 1983 work&amp;ndash;Seder, Leonard (1950), &amp;ldquo;Diagnosis with Diagrams—Part I&amp;rdquo;, Industrial Quality Control (New York, New York: American Society for Quality Control) 7 (1): 11–19.&lt;/p>
&lt;iframe src="https://giphy.com/embed/11JbaLzOXsg6Fq" width="200" frameBorder="0" class="giphy-embed" align="right">&lt;/iframe>
I'm less sure about Tufte's advice to always show axes starting at zero, which can make comparison between two groups difficult, and to "show causality," which can end up misleading your readers.  Of course, the visualizations on display in the glossy pages of Tufte's books are beautiful.  But while his books are full of general advice that we should all keep in mind when creating plots, he does not put forth a theory of what works and what doesn't when trying to visualize data.
&lt;p>Cleveland (with Robert McGill) develops such a theory and subjects it to rigorous scientific testing. In my last post I linked to one of Cleveland&amp;rsquo;s studies showing that &lt;a href="https://www.cs.ubc.ca/~tmm/courses/cpsc533c-04-spr/readings/cleveland.pdf" target="_blank" rel="noopener">dots (or bars) aligned on the same scale are indeed the best visualization to convey a series of numerical estimates&lt;/a>.  In this work, Cleveland examined how accurately our visual system can process visual elements or &amp;ldquo;perceptual units&amp;rdquo; representing underlying data.  These elements include markers aligned on the same scale (e.g., dot plots, scatterplots, ordinary bar charts), the length of lines that are not aligned on the same scale (e.g., stacked bar plots), area (pie charts and mosaic plots), angles (also pie charts), shading/color, volume, curvature, and direction.&lt;/p>
&lt;p>
&lt;figure id="figure-graphical-elements-cleveland">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/graphicalelementscleveland.png" alt="Graphical Elements (Cleveland)" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Graphical Elements (Cleveland)
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>He runs two experiments: the first compares judgements about relative position (grouped bar charts) to judgements based only on length (stacked bar charts); the second compares judgements about relative position (ordinary bar charts) to judgements about angles/area (pie charts).  Here are the materials he uses, courtesy of the &lt;a href="http://graphics.stanford.edu/" target="_blank" rel="noopener">Stanford Computer Graphics Lab&lt;/a>:&lt;/p>
&lt;p>
&lt;figure id="figure-graphical-perception-experiments">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/slide012.png" alt="Graphical perception experiments" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Graphical perception experiments
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>
&lt;figure id="figure-graphical-perception-experiments">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/slide013.png" alt="Graphical perception experiments" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Graphical perception experiments
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>The results are resoundingly clear&amp;mdash;judgements about position relative to a baseline are dramatically more accurate than judgements about angles, area, or length (with no baseline).  Hence, he suggests that we replace pie charts with bar charts or dot plots and that we substitute stacked bar charts for grouped bar charts.&lt;/p>
&lt;p>A striking and often overlooked finding in this work is the fact that the group of participants without technical training, &amp;ldquo;mostly ordinary housewives&amp;rdquo; as Cleveland describes them, performed &lt;em>just as well&lt;/em> as the group of mostly men with substantial technical training and experience.   This finding provides evidence for something that I&amp;rsquo;ve long suspected: that visualizations make it easier for people lacking quantitative experience to understand your results, serving to level the playing field.  If you want your findings to be broadly accessible, it&amp;rsquo;s probably better to present a visualization rather than a bunch of numbers.  It also suggests that if someone is having trouble interpreting your visualizations, it&amp;rsquo;s probably your fault.&lt;/p>
&lt;p>&lt;strong>Dotplots versus pie charts and stacked barplots&lt;/strong>&lt;/p>
&lt;p>Now let&amp;rsquo;s put this to the test.  Take a look at each visualization below for two seconds, looking for the percent of the vote that Mitt Romney, Ron Paul, and Jon Huntsman got.&lt;/p>
&lt;p>&lt;a href="https://solmessing.netlify.app/img/primarydot2.png">
&lt;figure id="figure-primarydot">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/primarydot2.png" alt="primaryDot" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
primaryDot
&lt;/figcaption>&lt;/figure>
&lt;/a>&lt;a href="https://solmessing.netlify.app/img/primarypie1.png">
&lt;figure id="figure-primarypie">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/primarypie1.png" alt="primaryPie" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
primaryPie
&lt;/figcaption>&lt;/figure>
&lt;/a>&lt;a href="https://solmessing.netlify.app/img/primarystacked.png">
&lt;figure id="figure-primarystacked">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/primarystacked.png" alt="primaryStacked" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
primaryStacked
&lt;/figcaption>&lt;/figure>
&lt;/a>&lt;/p>
&lt;p>Which is easiest to read? Which conveys information most accurately? Let&amp;rsquo;s first take a look at the most critical information&amp;ndash;the order in which the candidates placed.  In all plots, the candidates are arrayed in order from highest to least vote share, and it&amp;rsquo;s easy to see that Mitt won.  But once we start looking at who came in second, third, and so on, differences emerge.  It&amp;rsquo;s slightly harder to process order in the pie chart because your eye has to go around the plot rather than up and down in a straight line.  In the stacked bar chart, we need to look up which color corresponds to which candidate&amp;rsquo;s in the legend (as Tufte told us not to use), adding a layer of cognitive processing.&lt;/p>
&lt;p>Second, which conveys estimates most accurately? The dot plot is the clear winner here.  We can quickly see that Romney got about 37%, Paul got about 24%, and Huntsman got about 16%, just by looking at dots relative to the axis.  When we look at the pie chart, it&amp;rsquo;s really tough to estimate the exact percent each candidate got.  Same with the stacked bar chart. We could add numbers to the pie and bar charts, which would even things out to some extent, but then why not just display a table with exact percents?&lt;/p>
&lt;p>One argument I used to hear all the time when I worked in industry is that pie charts &amp;ldquo;convey a sense of proportion.&amp;rdquo;  Well, sure, I guess I can kind of guestimate that Ron Paul&amp;rsquo;s vote share is about 1/4.  What about Jon Huntsman? Hmm, it looks like about 15 percent, which is 3/20.  But wait, why do I want to convert things into fractions anyway? I don&amp;rsquo;t think in terms of fractions, I think in terms of percents.  And if I really care about proportion, I suppose I could extend the axis from 0 to 100.&lt;/p>
&lt;p>Suppose I want to plot results for the top 15 candidates, not just the top 6?  Here&amp;rsquo;s what happens:&lt;/p>
&lt;p>&lt;a href="https://solmessing.netlify.app/img/primarydot15.png">
&lt;figure id="figure-primarydot15">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/primarydot15.png" alt="primaryDot15" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
primaryDot15
&lt;/figcaption>&lt;/figure>
&lt;/a>&lt;a href="https://solmessing.netlify.app/img/primarypie15.png">
&lt;figure id="figure-primarypie15">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/primarypie15.png" alt="primaryPie15" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
primaryPie15
&lt;/figcaption>&lt;/figure>
&lt;/a>&lt;/p>
&lt;p>No contest, the pie chart fails completely.  We&amp;rsquo;d need to add a legend with colors for each candidate, which adds another layer of cognitive processing&amp;ndash;we&amp;rsquo;d need to look up each color in the lengend as we go.  And even after adding the legend, you wouldn&amp;rsquo;t be able to distinguish the lower performing candidates from say write-in votes because the pie slices would be too small.  The stacked bar chart will fail for the same reasons, so I&amp;rsquo;ve excluded it in the interest of brevity.  Note that we don&amp;rsquo;t need to add colors to the dotplot to convey the same information, which saves an extra plotting element that we can use to represent something else (say candidate&amp;rsquo;s campaign funds or total assets).  And, on top of it all, the dot plot takes up less screen/page real estate!&lt;/p>
&lt;p>Why do I use dot plots instead of ordinary bar charts? A &lt;a href="http://www.perceptualedge.com/articles/b-eye/encoding_values_in_graph.pdf" target="_blank" rel="noopener">nice visualization guide from perceptualedge.com&lt;/a> points out that often we want to only visualize differences between groups in a narrow range (they use an example wherein monthly expenses vary from $4,250-$5,500). But the length of a bar is supposed to facilitate accurate comparisons between values, so when you use a bar plot starting from $4,250, the length between bars dramatically exaggerates the actual differences. Dot plots do not have this problem because dot encode values using only location, so one must reference the axis to interpret the value.&lt;/p>
&lt;p>A related points is that bars are often used to convey counts&amp;ndash;we use them in histograms to represent frequency and track say counts of dollars earned/raised in bar charts.  In fact, a team of doctors I work with at the med school recently sent in a manuscript to Radiology containing a bar chart plotting mean values between groups; they got back the following comment from the statistical reviewer: &amp;ldquo;the y-axis is quantitative but the data are represented using bars as if the data were counts.&amp;rdquo;  People often use bar plots to convey estimates of means (and &lt;a href="http://www.stanford.edu/~messing/APSAPoster.pdf" target="_blank" rel="noopener">I&amp;rsquo;ve certainly done this&lt;/a>), which can serve to exaggerate differences in means and hence effect sizes if you do not plot the bars from zero.&lt;/p>
&lt;p>In addition, dot plots have aesthetic advantages.  They convey the numerical estimate in question with a single one-dimensional point, rather than a two dimensional bar.  There&amp;rsquo;s simply less that the eye needs to process.  Accordingly, if a pattern across qualitative groupings exists, it&amp;rsquo;s often easier to see with a dot plot.  For example, below I plot the average user ratings for each article to which &lt;a href="http://www.stanford.edu/~seanjw/" target="_blank" rel="noopener">Sean Westwood&lt;/a> and I exposed subjects in a news reading experiment.  The pattern that emerges is an &amp;ldquo;S&amp;rdquo; curve in which one or two stories dominate the ratings, most are sort of average, and a few are uniformly terrible. Note that you&amp;rsquo;d probably want to use something like this more for yourself than to communicate your results to others as it might overload your audience with too much information&amp;ndash;you&amp;rsquo;d do better to select a subset of these articles or remove some of the ones in the middle (thanks to Yph Lelkes for making this point).&lt;/p>
&lt;p>&lt;a href="https://solmessing.netlify.app/img/dotplot-story-rating.png">
&lt;figure id="figure-dotplot-story-rating">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/dotplot-story-rating.png" alt="dotplot-story-rating" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
dotplot-story-rating
&lt;/figcaption>&lt;/figure>
&lt;/a>&lt;/p>
&lt;p>One question that remains is if pie charts are so bad, why are they so common? Perhaps we like them because we find them comforting just as we find pies and pizza? Well if so we&amp;rsquo;d expect pie charts to be less common in places like Japan and China where people grow up eating different food.  Consider info-graphics in newspapers: I haven&amp;rsquo;t yet done a systematic content analysis, but I was unable to find a single pie chart in Japan&amp;rsquo;s Yomimuri Shimbun nor the Asahi Shimbun; nor in China&amp;rsquo;s Beijing Daily nor Sing Tao Daily.  I did see plenty of maps, however, which I suppose one could argue are reminiscent of noodles.&lt;/p>
&lt;p>&lt;strong>Implementation&lt;/strong>&lt;/p>
&lt;p>The most efficient way to produce solid visualizations with the ability to implement multiple panels, proper standard error estimates, and dot plots is probably in R using the ggplot2 package.  If you do not have time to learn R and remain tied to MS-Excel stick to ordinary barplots to visualize quantitative variables among multiple groups (not recommended).&lt;/p>
&lt;p>Otherwise, if you don&amp;rsquo;t already use it, &lt;a href="http://cran.r-project.org/" target="_blank" rel="noopener">download R&lt;/a> and a decent editor like &lt;a href="http://rstudio.org/" target="_blank" rel="noopener">Rstudio&lt;/a>.  Then get started with ggplot2 and dot plots by running the following code chunk which will replicate the election figure above:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">pres&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">read.csv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;https://SolomonMg.github.io/img/primaryres.csv&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">as.is&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">T&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># sort data in order of percent of vote:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">pres&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="n">pres&lt;/span>&lt;span class="nf">[order&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pres&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">Percentage&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">decreasing&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">T&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># only show top 15 candidates:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">pres&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="n">pres[1&lt;/span>&lt;span class="o">:&lt;/span>&lt;span class="m">15&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># create a precentage variable&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">pres&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">Percentage&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="n">pres&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">Percentage&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="m">100&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># reorder the Candidate factor by percentage for plotting purposes:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">pres&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">Candidate&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">reorder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pres&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">Candidate&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pres&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">Percentage&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># To install ggplot2, run the following line after deleting the #&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#install.packages(&amp;#34;ggplot2&amp;#34;)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ggplot2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">ggplot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pres&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">aes&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Percentage&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">factor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Candidate&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">geom_point&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">theme_bw&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nf">xlab&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;Percent of Vote&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nf">ylab&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;Candidate&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">ggtitle&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;New Hampshire Primary 2012&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>After loading our data and running a few preliminary data processing operations, we pass ggplot our data set, &amp;ldquo;pres,&amp;rdquo; then we tell it what aesthetic elements we want to use, in this case that x is going to be our &amp;ldquo;Percentage&amp;rdquo; variable and y is going to be our &amp;ldquo;Candidate&amp;rdquo; variable. We tell ggplot that we want to display points for every xy pair. We also tell it to use the black and white theme, and pass some obscure axis options that ensures the axis plot correctly. Then we tell it what to label the x and y axis, and give it a title.&lt;/p>
&lt;p>We can also reproduce the article ratings by story plot above using ggplot2 (even though I originally produced the plot using the lattice package).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># To install ggplot2, run the following line after deleting the #&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#install.packages(&amp;#34;ggplot2&amp;#34;)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ggplot2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">load&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">file&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;https://SolomonMg.github.io/img/db.Rda&amp;#34;&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># if you haven&amp;#39;t installed dplyr, delete the # and run this line:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># install.packages(&amp;#34;dplyr&amp;#34;)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dplyr&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">table&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">db&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">story&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># first we use plyr to calculate the mean rating and SE for each story&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ratingdat&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="n">db&lt;/span> &lt;span class="o">%&amp;gt;%&lt;/span> &lt;span class="nf">group_by&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">story&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">%&amp;gt;%&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">summarise&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">M&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">mean&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">rating&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">na.rm&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">T&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">SE&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">sd&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">rating&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">na.rm&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">T&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="nf">sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">length&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">na.omit&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">rating&lt;/span>&lt;span class="p">))),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">N&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">length&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">na.omit&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">rating&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># make story into an ordered factor, ordering by mean rating:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ratingdat&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">story&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">factor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ratingdat&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">story&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ratingdat&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">story&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">reorder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ratingdat&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">story&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ratingdat&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">M&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># take a look at our handiwork:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">ggplot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ratingdat&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">aes&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">M&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">xmin&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">M&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">SE&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">xmax&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">M&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">SE&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">story&lt;/span> &lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">geom_point&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nf">geom_segment&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="nf">aes&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">M&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">SE&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">xend&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">M&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">SE&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">story&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">yend&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">story&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">theme_bw&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nf">xlab&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;Mean rating&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nf">ylab&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;Story&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">ggtitle&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;Rating article by Story, with SE&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Now save&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">ggsave&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">file&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#34;plots/dotplot-story-rating.pdf&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">height&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">14&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">width&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">8.5&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Working with Bipartite/Affiliation Network Data in R</title><link>https://solmessing.netlify.app/post/working-with-bipartite-affiliation-network-data-in-r/</link><pubDate>Sun, 04 Mar 2012 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/post/working-with-bipartite-affiliation-network-data-in-r/</guid><description>&lt;p>Data can often be usefully conceptualized in terms affiliations between people (or other key data entities). It might be useful analyze common group membership, common purchasing decisions, or common patterns of behavior. This post introduces bipartite/affiliation network data and provides R code to help you process and visualize this kind of data. I recently updated this for use with larger data sets, though I put it together a while back.&lt;/p>
&lt;h2 id="preliminaries">Preliminaries&lt;/h2>
&lt;p>Much of the material here is covered in the more comprehensive &lt;a href="http://sna.stanford.edu/rlabs.php" target="_blank" rel="noopener">&amp;ldquo;Social Network Analysis Labs in R and SoNIA,&amp;rdquo;&lt;/a> on which I collaborated with Dan McFarland, Sean Westwood and Mike Nowak.
For a great online introduction to social network analysis see the online book &lt;a href="http://www.faculty.ucr.edu/~hanneman/nettext/" target="_blank" rel="noopener">Introduction to Social Network Methods&lt;/a> by Robert Hanneman and Mark Riddle.&lt;/p>
&lt;h2 id="bipartiteaffiliation-network-data">Bipartite/Affiliation Network Data&lt;/h2>
&lt;p>A network can consist of different &amp;lsquo;classes&amp;rsquo; of nodes. For example, a two-mode network might consist of people (the first mode) and groups in which they are members (the second mode). Another very common example of two-mode network data consists of users on a particular website who communicate in the same forum thread.
Here&amp;rsquo;s a short example of this kind of data. Run this in R for yourself - just copy an paste into the command line or into a script and it will generate a dataframe that we can use for illustrative purposes:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">df&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">data.frame&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">person&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;Sam&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;Sam&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;Sam&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;Greg&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;Tom&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;Tom&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;Tom&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;Mary&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;Mary&amp;#39;&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">group&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;a&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;b&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;c&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;a&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;b&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;c&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;d&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;b&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;d&amp;#39;&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stringsAsFactors&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">F&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">df&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">person&lt;/span> &lt;span class="n">group&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">1&lt;/span>    &lt;span class="n">Sam&lt;/span>     &lt;span class="n">a&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">2&lt;/span>    &lt;span class="n">Sam&lt;/span>     &lt;span class="n">b&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">3&lt;/span>    &lt;span class="n">Sam&lt;/span>     &lt;span class="n">c&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">4&lt;/span>   &lt;span class="n">Greg&lt;/span>     &lt;span class="n">a&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">5&lt;/span>    &lt;span class="n">Tom&lt;/span>     &lt;span class="n">b&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">6&lt;/span>    &lt;span class="n">Tom&lt;/span>     &lt;span class="n">c&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">7&lt;/span>    &lt;span class="n">Tom&lt;/span>     &lt;span class="n">d&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">8&lt;/span>   &lt;span class="n">Mary&lt;/span>     &lt;span class="n">b&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">9&lt;/span>   &lt;span class="n">Mary&lt;/span>     &lt;span class="n">d&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="c1">Fast, efficient two-mode to one-mode conversion in R&lt;/h2>
&lt;p>Suppose we wish to analyze or visualize how the people are connected directly - that is, what if we want the network of people where a tie between two people is present if they are both members of the same group? We need to perform a two-mode to one-mode conversion.&lt;/p>
&lt;p>To convert a two-mode incidence matrix to a one-mode adjacency matrix, one can simply multiply an incidence matrix by its transpose, which sum the common 1&amp;rsquo;s between rows. Recall that matrix multiplication entails multiplying the k-th entry of a row in the first matrix by the k-th entry of a column in the second matrix, then summing, such that the ij-th row-column entry in resulting matrix represents the dot-product of the i-th row of the first matrix and the j-th column of the second. In mathematical notation:&lt;/p>
&lt;p>$$
AB = \left [
\begin{array}{cc}
a &amp;amp; b \\\
c &amp;amp; d
\end{array} \right ]
\left [ \begin{array}{cc}
e &amp;amp; f \\\
g &amp;amp; h
\end{array} \right ] =
\left [
\begin{array}{cc}
ae+bg &amp;amp; af+bh \\\
ce+dg &amp;amp; cf+dh
\end{array}
\right ]
$$&lt;/p>
&lt;p>Notice further that multiplying a matrix by its transpose yields the following:&lt;/p>
&lt;p>$$
\begin{align}
AA&amp;rsquo; =
\left[
\begin{array}{cc}
a &amp;amp; b \\\
c &amp;amp; d
\end{array}
\right]
\left[
\begin{array}{cc}
a &amp;amp; c \\\
b &amp;amp; d
\end{array}
\right] =
\left[
\begin{array}{cc}
aa+bb &amp;amp; ac+bd \\\
ca+db &amp;amp; cc+dd
\end{array}
\right]
\end{align}
$$&lt;/p>
&lt;p>Because our incidence matrix consists of 0&amp;rsquo;s and 1&amp;rsquo;s, the off-diagonal entries represent the total number of common columns, which is exactly what we wanted. We&amp;rsquo;ll use the &lt;code>%*%&lt;/code> operator to tell R to do exactly this. Let&amp;rsquo;s take a look at a small example using toy data of people and groups to which they belong. We&amp;rsquo;ll coerce the data to an incidence matrix, then multiply the incidence matrix by its transpose to get the number of common groups between people.&lt;/p>
&lt;p>This is easy to do using the matrix algebra functions included in R. But first, you need to restructure your (edgelist) network data as an incidence matrix. An incidence will record a 1 for row-column combinations where a tie is present and 0 otherwise. One easy way to do this in R is to use the table function and then coerce the table object to a matrix object:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">m&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">table&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="n">df&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">M&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">as.matrix&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you are using the network or sna packages, a network object be coerced via &lt;code>as.matrix(your-network)&lt;/code>; with the igraph package use &lt;code>get.adjacency(your-network)&lt;/code>.&lt;/p>
&lt;p>This is great, but what about if we are working with a really large data set? Network data is almost always sparse&amp;mdash;there are far more pairwise combinations of potential connections than actual observed connections. Hence, we&amp;rsquo;d actually prefer to keep the underlying data structured in edgelist format, but we&amp;rsquo;d also like access to R&amp;rsquo;s matrix algebra functionality.&lt;/p>
&lt;p>We can get the best of both worlds using the Matrix library to construct a sparse triplet representation of a matrix. But we&amp;rsquo;d also like to avoid building the entire incidence matrix and just feed Matrix our edgelist directly, a point that came up in a recent conversation I had with &lt;a href="http://seanjtaylor.com/" target="_blank" rel="noopener">Sean Taylor&lt;/a>. We feed &lt;code>Matrix&lt;/code> our &amp;lsquo;person&amp;rsquo; column to index &amp;lsquo;i&amp;rsquo; (rows in the new incidence matrix), our &amp;lsquo;group&amp;rsquo; column to index j (columns in the new incidence matrix), and we repeat &amp;lsquo;1&amp;rsquo; for the length of the edgelist to denote an incidence.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;Matrix&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">A&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">spMatrix&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nrow&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nf">length&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">unique&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">df&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">person&lt;/span>&lt;span class="p">)),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ncol&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nf">length&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">unique&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">df&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">group&lt;/span>&lt;span class="p">)),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">as.numeric&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">factor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">df&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">person&lt;/span>&lt;span class="p">)),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">as.numeric&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">factor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">df&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">group&lt;/span>&lt;span class="p">)),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">rep&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">length&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">as.numeric&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">df&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">person&lt;/span>&lt;span class="p">)))&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">row.names&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">levels&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">factor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">df&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">person&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">colnames&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">levels&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">factor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">df&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">group&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">A&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We will either convert to the &amp;lsquo;mode&amp;rsquo; represented by the columns or by the rows.
To get the one-mode representation of ties between rows (people in our example), multiply the matrix by its transpose. Note that you must use the matrix-multiplication operator &lt;code>%*%&lt;/code> rather than a simple astrisk. The R code is:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">Arow&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="n">A&lt;/span> &lt;span class="o">%*%&lt;/span> &lt;span class="nf">t&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>But we can still do better! The function tcrossprod is faster and more efficient for this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">Arow&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">tcrossprod&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Arow will now represent the one-mode matrix formed by the row entities&amp;mdash;people will have ties to each other if they are in the same group, in our example. Here&amp;rsquo;s what it looks like:&lt;/p>
&lt;pre>&lt;code>Arow
4 x 4 sparse Matrix of class &amp;quot;dgCMatrix&amp;quot;
Greg Mary Sam Tom
Greg 1 . 1 .
Mary . 2 1 2
Sam 1 1 3 2
Tom . 2 2 3
&lt;/code>&lt;/pre>
&lt;p>To get the one-mode matrix formed by the column entities (i.e. the number of people) enter the following command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">Acol&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">t&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">%*%&lt;/span> &lt;span class="n">A&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Again, we can use tcrossprod to make this even more efficient:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">Acol&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">tcrossprod&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">t&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And the resulting co-membership matrix is as follows:&lt;/p>
&lt;pre>&lt;code>Mcol
group
group a b c d
a 2 1 1 0
b 1 3 2 2
c 1 2 2 1
d 0 2 1 2
&lt;/code>&lt;/pre>
&lt;p>Although we&amp;rsquo;ve used a very small network for our example, this code is highly extensible to the analysis of larger networks with R.&lt;/p>
&lt;h2 id="c1">Analysis of Two Mode Data and Mobility&lt;/h2>
&lt;p>Let&amp;rsquo;s work with some actual affiliation data, collected by Dan McFarland on student extracurricular affiliations. It&amp;rsquo;s a longitudinal data set, with 3 waves - 1996, 1997, 1998.  It consists of students (anonymized) and the student organizations in which they are members (e.g. National Honor Society, wrestling team, cheerleading squad, etc.).
What we&amp;rsquo;ll do is to read in the data, explore it, make a few two-to-one mode conversions, and visualize it.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Load the &amp;#39;igraph&amp;#39; library&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;igraph&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># (1) Read in the data files, NA data objects coded as &amp;#39;na&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">magact96&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">read.delim&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;https://solomonmg.github.io/assets/img/mag_act96.txt&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">na.strings&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#39;na&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">magact97&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">read.delim&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;https://solomonmg.github.io/assets/img/mag_act97.txt&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">na.strings&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#39;na&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">magact98&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">read.delim&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;https://solomonmg.github.io/assets/img/mag_act98.txt&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">na.strings&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#39;na&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Missing data is coded as &amp;ldquo;na&amp;rdquo; in this data, which is why we gave R the command na.strings = &amp;ldquo;na&amp;rdquo;.&lt;/p>
&lt;p>These files consist of four columns of individual-level attributes (ID, gender, grade, race), then a bunch of group membership dummy variables (coded &amp;ldquo;1&amp;rdquo; for membership, &amp;ldquo;0&amp;rdquo; for no membership).  We need to set aside the first four columns (which do not change from year to year).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">magattrib&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">magact96[&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="o">:&lt;/span>&lt;span class="m">4&lt;/span>&lt;span class="n">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">g96&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">as.matrix&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magact96[&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="o">:&lt;/span>&lt;span class="m">4&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="n">]&lt;/span>&lt;span class="p">);&lt;/span> &lt;span class="nf">row.names&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g96&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">magact96&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">ID.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">g97&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">as.matrix&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magact97[&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="o">:&lt;/span>&lt;span class="m">4&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="n">]&lt;/span>&lt;span class="p">);&lt;/span> &lt;span class="nf">row.names&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g97&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">magact97&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">ID.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">g98&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">as.matrix&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magact98[&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="o">:&lt;/span>&lt;span class="m">4&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="n">]&lt;/span>&lt;span class="p">);&lt;/span> &lt;span class="nf">row.names&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g98&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">magact98&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">ID.&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>By using the &lt;code>[,-(1:4)]&lt;/code> index, we drop those columns so that we have a square incidence matrix for each year, and then tell R to set the row names of the matrix to the student&amp;rsquo;s ID. Note that we need to keep the &amp;ldquo;.&amp;rdquo; after ID in this dataset (because it&amp;rsquo;s in the name of the variable).
Now we load these two-mode matrices into igraph:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">i96&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">graph.incidence&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g96&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mode&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;all&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">i97&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">graph.incidence&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g97&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mode&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;all&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">i98&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">graph.incidence&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g98&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mode&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;all&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="plotting-two-mode-networks">Plotting two-mode networks&lt;/h3>
&lt;p>Now, let&amp;rsquo;s plot these graphs. The igraph package has excellent plotting functionality that allows you to assign visual attributes to igraph objects before you plot. The alternative is to pass 20 or so arguments to the &lt;code>plot.igraph()&lt;/code> function, which gets really messy.&lt;/p>
&lt;p>Let&amp;rsquo;s assign some attributes to our graph. First we set vertex attributes, making sure to make them slightly transparent by altering the gamma, using the &lt;code>rgb(r,g,b,gamma)&lt;/code> function to set the color. This makes it much easier to look at a really crowded graph, which might look like a giant hairball otherwise.
You can read up on the RGB color model &lt;a href="http://en.wikipedia.org/wiki/RGB_color_model" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>Each node (or &amp;ldquo;vertex&amp;rdquo;) object is accessible by calling &lt;code>V(g)&lt;/code>, and you can call (or create) a node attribute by using the &lt;code>$&lt;/code> operator so that you call &lt;code>V(g)$attribute&lt;/code>. Here&amp;rsquo;s how to set the color attribute for a set of nodes in a graph object:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">color[1&lt;/span>&lt;span class="o">:&lt;/span>&lt;span class="m">1295&lt;/span>&lt;span class="n">]&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">rgb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">.5&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">color[1296&lt;/span>&lt;span class="o">:&lt;/span>&lt;span class="m">1386&lt;/span>&lt;span class="n">]&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">rgb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">.5&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Notice that we index the &lt;code>V(g)$color&lt;/code> object by a seemingly arbitrary value, 1295.  This marks the end of the student nodes, and 1296 is the first group node. You can view which nodes are which by typing V(i96). R prints out a list of all the nodes in the graph, and those with a number are obviously different from those that consist of a group name.&lt;/p>
&lt;p>Now we&amp;rsquo;ll set some other graph attributes:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">label&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">name&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">label.color&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">rgb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">.2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">.5&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">label.cex&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="m">.4&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">size&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="m">6&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">frame.color&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="kc">NA&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can also set edge attributes. Here we&amp;rsquo;ll make the edges nearly transparent and slightly yellow because there will be so many edges in this graph:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">E&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">color&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">rgb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">.5&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">.5&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">.2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now, we&amp;rsquo;ll open a pdf &amp;ldquo;device&amp;rdquo; on which to plot. This is just a connection to a pdf file. Note that the code below will take a minute or two to execute (or longer if you have a pre- Intel dual-core processor).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">pdf&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;i96.pdf&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">plot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layout&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">layout.fruchterman.reingold&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">dev.off&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that we&amp;rsquo;ve used the Fruchterman-Reingold force-directed layout algorithm here.  Generally speaking, the when you have a ton of edges, the Kamada-Kawai layout algorithm works well but, it can get really slow for networks with a lot of nodes. Also, for larger networks, layout.fruchterman.reingold.grid is faster, but can fail to produce a plot with any meaninful pattern if you have too many isolates, as is the case here. Experiment for yourself.
Here&amp;rsquo;s what we get:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/i96.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>It&amp;rsquo;s oddly reminiscent of a cresent and star, but impossible to read. Now, if you open the &lt;a href="https://solmessing.netlify.app/img/i96.pdf">pdf output&lt;/a>, you&amp;rsquo;ll notice that you can zoom in on any part of the graph ad infinitum without losing any resolution. How is that possible in such a small file? It&amp;rsquo;s possible because the pdf device output consists of data based on vectors: lines, polygons, circles, elipses, etc., each specified by a mathematical formula that your pdf program renders when you view it. Regular bitmap or jpeg picture output, on the other hand, consists of a pixel-coordinate mapping of the image in question, which is why you lose resolution when you zoom in on a digital photograph or a plot produced with most other programs.&lt;/p>
&lt;p>Let&amp;rsquo;s remove all of the isolates (the cresent), change a few aesthetic features, and replot. First, we&amp;rsquo;ll remove isloates, by deleting all nodes with a degree of 0, meaning that they have zero edges. Then, we&amp;rsquo;ll suppress labels for students and make their nodes smaller and more transparent. Then we&amp;rsquo;ll make the edges more narrow more transparent. Then, we&amp;rsquo;ll replot using various layout algorithms:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">i96&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">delete.vertices&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="n">[&lt;/span> &lt;span class="nf">degree&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">==&lt;/span>&lt;span class="m">0&lt;/span> &lt;span class="n">]&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">label[1&lt;/span>&lt;span class="o">:&lt;/span>&lt;span class="m">857&lt;/span>&lt;span class="n">]&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="kc">NA&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">color[1&lt;/span>&lt;span class="o">:&lt;/span>&lt;span class="m">857&lt;/span>&lt;span class="n">]&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span>  &lt;span class="nf">rgb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">.1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">size[1&lt;/span>&lt;span class="o">:&lt;/span>&lt;span class="m">857&lt;/span>&lt;span class="n">]&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="m">2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">E&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">width&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="m">.3&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">E&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">color&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">rgb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">.5&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">.5&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">.1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">pdf&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;i96.2.pdf&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">plot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layout&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">layout.kamada.kawai&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">dev.off&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">pdf&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;i96.3.pdf&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">plot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layout&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">layout.fruchterman.reingold.grid&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">dev.off&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">pdf&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;i96.4.pdf&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">plot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layout&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">layout.fruchterman.reingold&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">dev.off&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>I personally prefer the Fruchterman-Reingold layout in this case. The nice thing about this layout is that it really emphasizes centrality&amp;ndash;the nodes that are most central are nearly always placed in the middle of the plot. Here&amp;rsquo;s what it looks like:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/i962.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Very pretty, but you can&amp;rsquo;t see which groups are which at this resolution. Zoom assets/in on the &lt;a href="https://solmessing.netlify.app/img/samplepdf.pdf">pdf output&lt;/a>, and you can see things pretty clearly.&lt;/p>
&lt;h3 id="two-mode-to-one-mode-data-transformation">Two mode to one mode data transformation&lt;/h3>
&lt;p>We&amp;rsquo;ve emphasized groups in this visualization so much, that we might want to just create a network consisting of group co-membership. First we need to create a new network object. We&amp;rsquo;ll do that the same way for this network as for our example at the top of this page:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">g96e&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">t&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g96&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">%*%&lt;/span> &lt;span class="n">g96&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">g97e&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">t&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g97&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">%*%&lt;/span> &lt;span class="n">g97&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">g98e&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">t&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g98&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">%*%&lt;/span> &lt;span class="n">g98&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">i96e&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">graph.adjacency&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g96e&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mode&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#39;undirected&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now we need to tansform the graph so that multiple edges become an attribute ( &lt;code>E(g)$weight&lt;/code> ) of each unique edge:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">E&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96e&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">weight&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">count.multiple&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96e&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">i96e&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">simplify&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96e&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now we&amp;rsquo;ll set the other plotting parameters as we did above:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Set vertex attributes&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96e&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">label&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96e&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">name&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96e&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">label.color&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">rgb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">.2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">.8&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96e&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">label.cex&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="m">.6&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96e&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">size&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="m">6&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96e&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">frame.color&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="kc">NA&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96e&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">color&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">rgb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">.5&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Set edge gamma according to edge weight&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">egam&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="nf">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">E&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96e&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">weight&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="m">+.3&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="nf">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">E&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96e&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">weight&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="m">+.3&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">E&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96e&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">color&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">rgb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">.5&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">.5&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">egam&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We set edge gamma as a function of how many edges exist between two nodes, or in this case, how many students each group has in common. For illustrative purposes, let&amp;rsquo;s compare how the Kamada-Kawai and Fruchterman-Reingold algorithms render this graph:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">pdf&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;i96e.pdf&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">plot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96e&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">main&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#39;layout.kamada.kawai&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layout&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">layout.kamada.kawai&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">plot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i96e&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">main&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#39;layout.fruchterman.reingold&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layout&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">layout.fruchterman.reingold&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">dev.off&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>I like the Kamada-Kawai layout for this graph, because the center of the graph is too busy otherwise. And here&amp;rsquo;s what the resulting plot looks like:
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/i96e-kk.jpeg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>You can check out the difference between each layout yourself. Here&amp;rsquo;s what the &lt;a href="https://solmessing.netlify.app/img/i96e.pdf">assets/pdf output looks like&lt;/a>.  Page 1 shows the Kamada-Kawai layout and page 2 shows the Fruchterman Reingold layout.&lt;/p>
&lt;h3 id="group-overlap-networks-and-plots">Group overlap networks and plots&lt;/h3>
&lt;p>Now we might also be interested in the percent overlap between groups. Note that this will be a directed graph, because the percent overlap will not be symmetric across groups&amp;ndash;for example, it may be that 3/4 of Spanish NHS members are in NHS, but only 1/8 of NHS members are in the Spanish NHS. We&amp;rsquo;ll create this graph for all years in our data (though we could do it for one year only).
First we&amp;rsquo;ll need to create a percent overlap graph. We start by dividing each row by the diagonal (this is really easy in R):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">ol96&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="n">g96e&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="nf">diag&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g96e&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ol97&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="n">g97e&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="nf">diag&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g97e&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ol98&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="n">g98e&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="nf">diag&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g98e&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, sum the matricies and set any NA cells (caused by dividing by zero in the step above) to zero:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">magall&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="n">ol96&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">ol97&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">ol98&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">magall&lt;/span>&lt;span class="nf">[is.na&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magall&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="n">]&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="m">0&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that magall now consists of a percent overlap matrix, but because we&amp;rsquo;ve summed over 3 years, the maximun is now 3 instead of 1.
Let&amp;rsquo;s compute average club size, by taking the mean across each value in each diagonal:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">magdiag&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">apply&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">cbind&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">diag&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g96e&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="nf">diag&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g97e&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="nf">diag&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g98e&lt;/span>&lt;span class="p">)),&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mean&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, we&amp;rsquo;ll generate centrality measures for magall. When we create the igraph object from our matrix, we need to set weighted=T because otherwise igraph dichotomizes edges at 1. This can distort our centrality measures because now edges represent  more than binary connections&amp;ndash;they represent the percent of membership overlap.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">magallg&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">graph.adjacency&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magall&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">weighted&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">T&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Degree&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallg&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">degree&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">degree&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallg&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Betweenness centrality&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallg&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">btwcnt&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">betweenness&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallg&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Before we plot this, we should probably filter some of the edges, otherwise our graph will probably be too busy to make sense of visually.  Take a look at the distribution of connection strength by plotting the density of the magall matrix:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">plot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">density&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magall&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/densitymagall.jpeg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Nearly all of the edge weights are below 1&amp;ndash;or in other words, the percent overlap for most clubs is less than 1/3. Let&amp;rsquo;s filter at 1, so that an edge will consists of group overlap of more than 1/3 of the group&amp;rsquo;s members in question.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">magallgt1&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="n">magall&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">magallgt1[magallgt1&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="n">]&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="m">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">magallggt1&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">graph.adjacency&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallgt1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">weighted&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">T&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Removes loops:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">magallggt1&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">simplify&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallggt1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">remove.multiple&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">FALSE&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">remove.loops&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">TRUE&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Before we do anything else, we&amp;rsquo;ll create a custom layout based on Fruchterman.-Ringold wherein we adjust the coordates by hand using the tkplot gui tool to make sure all of the labels are visible. This is very useful if you want to create a really sharp-looking network visualization for publication.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">magallggt1&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">layout&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">layout.fruchterman.reingold&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallggt1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallggt1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">label&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallggt1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">name&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">tkplot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallggt1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let the plot load, then maximize the window, and select to View -&amp;gt; Fit to Screen so that you get maximum resolution for this large graph. Now hand-place the nodes, making sure no labels overlap:
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/tkplotscreenshot.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Pay special attention to whether the labels overlap (or might overlap if the font was bigger) along the vertical. Save the layout coordinates to the graph object:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">magallggt1&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">layout&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">tkplot.getcoords&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We use &amp;ldquo;1&amp;rdquo; here because only if this was the first tkplot object you called. If you called tkplot a few times, use the last plot object. You can tell which object is visible because at the top of the tkplot interface, you&amp;rsquo;ll see something like &amp;ldquo;Graph plot 1&amp;rdquo; or in the case of my screenshot above &amp;ldquo;Graph plot 7&amp;rdquo; (it was the seventh time I called tkplot).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Set vertex attributes&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallggt1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">label&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallggt1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">name&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallggt1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">label.color&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">rgb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">.2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">.6&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallggt1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">size&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="m">6&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallggt1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">frame.color&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="kc">NA&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallggt1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">color&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">rgb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">.5&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Set edge attributes&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">E&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallggt1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">arrow.size&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="m">.3&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Set edge gamma according to edge weight&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">egam&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="nf">E&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallggt1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">weight&lt;/span>&lt;span class="m">+.1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="nf">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">E&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallggt1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">weight&lt;/span>&lt;span class="m">+.1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">E&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallggt1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">color&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">rgb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">.5&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">.5&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">egam&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>One thing that we can do with this graph is to set label size as a function of degree, which adds a &amp;ldquo;tag-cloud&amp;rdquo;-like element to the visualization:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallggt1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">label.cex&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallggt1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">degree&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallggt1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">degree&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">+&lt;/span> &lt;span class="m">.3&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#note, unfortunately one must play with the formula above to get the&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#ratio just right&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let&amp;rsquo;s plot the results:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">pdf&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;magallggt1customlayout.pdf&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">plot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">magallggt1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">dev.off&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that we used the custom layout, which because we made part of the igraph object magallggt1, we did not need to specify in plot command.
assets/Here&amp;rsquo;s the &lt;a href="https://solmessing.netlify.app/img/magallggt1custom.pdf">pdf output&lt;/a>, and here&amp;rsquo;s what it looks like:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://solmessing.netlify.app/img/magallggt1custom.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>This visualization reveals much more information about our network than our cresent-star visualization.&lt;/p>
&lt;h3 id="mobility-markov-and-transition-probabilities">Mobility, Markov, and Transition Probabilities&lt;/h3>
&lt;p>In order to shed light on how people flow through these groups, we&amp;rsquo;ll compute transition probabilities. These transition probabilities are more generally referred to as Markov chains.&lt;/p>
&lt;p>First we&amp;rsquo;ll create a new matrix that multiplies 1996 magnet with 1997 magnet so you see the number of students moving from 1996 membership to 1997 memberships.&lt;/p>
&lt;p>Before we actually do this, we need to do some data munging to make sure that the rows and columns for g96 and g97 are the same. We&amp;rsquo;ll use the match() function for this.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># First, let&amp;#39;s get an idea of how many column-names (activities) and row&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># names (student ids) are in common between the two years:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">(&lt;/span>&lt;span class="n">cnames&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">intersect&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="nf">colnames&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g96&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="nf">colnames&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g97&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">)&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">(&lt;/span>&lt;span class="n">rnames&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">intersect&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="nf">row.names&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g96&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="nf">row.names&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g97&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">)&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Great, there are a lot of names in common. Now we&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># need to make sure we are only using the rows&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># and columns of each matrix that contain entries used in&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># both years. We also need to make sure that the columns and&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># rows are in the same order.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># In order to accomplish this we are going to exploit R&amp;#39;s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># indexing capabilities. We are going to have R &amp;#34;rebuild&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># each matrix according to the order of rnames and cnames.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># We&amp;#39;ll use the match() function to accomplish this.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">g96matched&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">g96[&lt;/span> &lt;span class="nf">match&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">rnames&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">row.names&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g96&lt;/span>&lt;span class="p">)),&lt;/span> &lt;span class="nf">match&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cnames&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">colnames&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g96&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="n">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">g97matched&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">g97[&lt;/span> &lt;span class="nf">match&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">rnames&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">row.names&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g97&lt;/span>&lt;span class="p">)),&lt;/span> &lt;span class="nf">match&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cnames&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">colnames&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g97&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="n">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># We need to do the same thing for the diagonal of the matrix g96e, which is&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># our co-membership/affiliation matrix computed above:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">mag96diagmatched&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">diag&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="n">g96e[&lt;/span> &lt;span class="nf">match&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cnames&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">colnames&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g96e&lt;/span>&lt;span class="p">)),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nf">match&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cnames&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">colnames&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g96e&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="n">]&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Now let&amp;#39;s check to make sure things worked correctly:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">which&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">row.names&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g96matched&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="nf">row.names&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g97matched&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">which&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">colnames&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g96matched&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="nf">colnames&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g97matched&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now that these are effectively matricies, we can multiply to get the transition probability matrix:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">mag96_97&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">t&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g96matched&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">%*%&lt;/span> &lt;span class="n">g97matched&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let&amp;rsquo;s munge the 97 and 98 data and repeat:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">cnames&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">intersect&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="nf">colnames&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g97&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="nf">colnames&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g98&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">rnames&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">intersect&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="nf">row.names&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g97&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="nf">row.names&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g98&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">g97matched&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">g97[&lt;/span> &lt;span class="nf">match&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">rnames&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">row.names&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g97&lt;/span>&lt;span class="p">)),&lt;/span> &lt;span class="nf">match&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cnames&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">colnames&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g97&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="n">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">g98matched&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">g98[&lt;/span> &lt;span class="nf">match&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">rnames&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">row.names&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g98&lt;/span>&lt;span class="p">)),&lt;/span> &lt;span class="nf">match&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cnames&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">colnames&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g98&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="n">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And again for the 97-98 transition:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">mag97_98&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">t&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g97matched&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">%*%&lt;/span> &lt;span class="n">g98matched&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now we need to get the group-level membership matrix diagonal, ordered by the current set of columns.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">mag96diagmatched&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">diag&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="n">g96e[&lt;/span> &lt;span class="nf">match&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cnames&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">colnames&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g96e&lt;/span>&lt;span class="p">)),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nf">match&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cnames&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">colnames&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g96e&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="n">]&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">mag97diagmatched&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">diag&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="n">g97e[&lt;/span> &lt;span class="nf">match&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cnames&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">colnames&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g97e&lt;/span>&lt;span class="p">)),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nf">match&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cnames&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">colnames&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g97e&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="n">]&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">mag98diagmatched&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">diag&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="n">g98e[&lt;/span> &lt;span class="nf">match&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cnames&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">colnames&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g98e&lt;/span>&lt;span class="p">)),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nf">match&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cnames&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">colnames&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g98e&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="n">]&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And finally we can create the transition probability matrix! Divide magmob96_97 by mag96diagmatched in to get the transition probability matrix (Markov chain):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">magmob96_97&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">mag96_97&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">mag96diagmatched&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">magmob97_98&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">mag97_98&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">mag97diagmatched&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now add the matrices and divide by 2:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">mobility_all&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">magmob96_97&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">magmob97_98&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="m">2&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now plot as with the event-overlap graphs!&lt;/p></description></item><item><title>Differentiation of Normal Thymus from Anterior Mediastinal Lymphoma and Lymphoma Recurrence at Pediatric PET/CT</title><link>https://solmessing.netlify.app/publication/gawande-2012-differentiation/</link><pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/gawande-2012-differentiation/</guid><description/></item><item><title>FDG PET/CT for the Evaluation of Normal Thymus, Lymphoma Recurrence, and Mediastinal Lymphoma in Pediatric Patients Response</title><link>https://solmessing.netlify.app/publication/gawande-2012-fdg/</link><pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/gawande-2012-fdg/</guid><description/></item><item><title>How Words and Money Cultivate a Personal Vote: The Effect of Legislator Credit Claiming on Constituent Credit Allocation</title><link>https://solmessing.netlify.app/publication/grimmer-how-2012/</link><pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/grimmer-how-2012/</guid><description>&lt;ul>
&lt;li>&lt;a href="https://solmessing.netlify.app/pdf/ccsup.pdf">Supplementary Information&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Intravenous ferumoxytol allows noninvasive MR imaging monitoring of macrophage migration into stem cell transplants</title><link>https://solmessing.netlify.app/publication/khurana-2012-intravenous/</link><pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/khurana-2012-intravenous/</guid><description/></item><item><title>The 2012 Election Day Through the Facebook Lens</title><link>https://solmessing.netlify.app/publication/messing-2012-ts/</link><pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/messing-2012-ts/</guid><description/></item><item><title>Who is a `deserving'immigrant? An experimental study of Norwegian attitudes</title><link>https://solmessing.netlify.app/publication/aalberg-2012-deserving/</link><pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/aalberg-2012-deserving/</guid><description/></item><item><title>Measuring issue salience: Using supervised machine learning to generate data from free responses to the 'most important problem' question</title><link>https://solmessing.netlify.app/publication/messing-2011-measuring/</link><pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/messing-2011-measuring/</guid><description/></item><item><title>Of course I wouldn't do that in real life: advancing the arguments for increasing realism in HCI experiments</title><link>https://solmessing.netlify.app/publication/lew-2011-course/</link><pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/lew-2011-course/</guid><description/></item><item><title>Do explicit racial cues influence candidate preference? The case of skin complexion in the 2008 campaign</title><link>https://solmessing.netlify.app/publication/iyengar-2008-explicit/</link><pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/publication/iyengar-2008-explicit/</guid><description/></item><item><title/><link>https://solmessing.netlify.app/admin/config.yml</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/admin/config.yml</guid><description/></item><item><title>Tech Stack</title><link>https://solmessing.netlify.app/home/techsetup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/home/techsetup/</guid><description>&lt;ul>
&lt;li>&lt;a href="https://gist.github.com/kevin-smets/8568070" target="_blank" rel="noopener">Best terminal (iterm 2) setup I&amp;rsquo;ve found yet&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://stackoverflow.com/a/42536153/34935" target="_blank" rel="noopener">Fix annoying error messages in RStudio&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.walware.de/goto/statet" target="_blank" rel="noopener">StatET&lt;/a>, an R plugin for Eclipse. Why not just RStudio? StatET has the best object browser available if you&amp;rsquo;re doing anything with JSON or lists of lists, and will not crash when R crashes, unlike RStudio. It also plays well with cloud instances. Though, yes, I find myself using RStudio on new machines because setting up StatET can be a challenge.&lt;/li>
&lt;li>Get started using &lt;a href="https://sourcethemes.com/academic/docs/" target="_blank" rel="noopener">Hugo - Academic&lt;/a>.&lt;/li>
&lt;li>Quickly snap windows side-by-side with shortcut keys on Mac using &lt;a href="https://github.com/rxhanson/Rectangle" target="_blank" rel="noopener">Rectangle&lt;/a>.&lt;/li>
&lt;li>How to setup &lt;a href="https://www.techrepublic.com/article/how-to-enable-custom-keyboard-shortcuts-in-gmail/" target="_blank" rel="noopener">shotcut keys&lt;/a> in gmail.&lt;/li>
&lt;/ul></description></item><item><title>Tech Stack</title><link>https://solmessing.netlify.app/techsetup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://solmessing.netlify.app/techsetup/</guid><description>&lt;ul>
&lt;li>&lt;a href="https://gist.github.com/kevin-smets/8568070" target="_blank" rel="noopener">Best terminal (iterm 2) setup I&amp;rsquo;ve found yet&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://stackoverflow.com/a/42536153/34935" target="_blank" rel="noopener">Fix annoying error messages in RStudio&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.walware.de/goto/statet" target="_blank" rel="noopener">StatET&lt;/a>, an R plugin for Eclipse. Why not just RStudio? StatET has the best object browser available if you&amp;rsquo;re doing anything with JSON or lists of lists, and will not crash when R crashes, unlike RStudio. It also plays well with cloud instances. Though, yes, I find myself using RStudio on new machines because setting up StatET can be a challenge.&lt;/li>
&lt;li>Get started using &lt;a href="https://sourcethemes.com/academic/docs/" target="_blank" rel="noopener">Hugo - Academic&lt;/a>.&lt;/li>
&lt;li>Quickly snap windows side-by-side with shortcut keys on Mac using &lt;a href="https://github.com/rxhanson/Rectangle" target="_blank" rel="noopener">Rectangle&lt;/a>.&lt;/li>
&lt;li>How to setup &lt;a href="https://www.techrepublic.com/article/how-to-enable-custom-keyboard-shortcuts-in-gmail/" target="_blank" rel="noopener">shotcut keys&lt;/a> in gmail.&lt;/li>
&lt;/ul></description></item></channel></rss>